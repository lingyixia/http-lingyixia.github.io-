<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    

    

    



    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    
    
    
    <title>Bert | 灵翼俠的个人博客 | 不做搬运工</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="论文">
    <meta name="description" content="本文不讲Bert原理,拟进行Bert源码分析和应用  Bert源码分析组织结构  这是Bert Github地址,打开后会看到这样的结构:                                                                                                          下面我将逐个分析上诉图片中加框的文件,其他的文件">
<meta name="keywords" content="论文">
<meta property="og:type" content="article">
<meta property="og:title" content="Bert">
<meta property="og:url" content="https://lingyixia.github.io/2019/07/22/Bert/index.html">
<meta property="og:site_name" content="灵翼俠的个人博客">
<meta property="og:description" content="本文不讲Bert原理,拟进行Bert源码分析和应用  Bert源码分析组织结构  这是Bert Github地址,打开后会看到这样的结构:                                                                                                          下面我将逐个分析上诉图片中加框的文件,其他的文件">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://lingyixia.github.io/img/Bert/bert.jpeg">
<meta property="og:updated_time" content="2021-09-19T18:01:25.943Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bert">
<meta name="twitter:description" content="本文不讲Bert原理,拟进行Bert源码分析和应用  Bert源码分析组织结构  这是Bert Github地址,打开后会看到这样的结构:                                                                                                          下面我将逐个分析上诉图片中加框的文件,其他的文件">
<meta name="twitter:image" content="https://lingyixia.github.io/img/Bert/bert.jpeg">
    
        <link rel="alternate" type="application/atom+xml" title="灵翼俠的个人博客" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/Favicon.png">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</head>

<script type="text/javascript" src="/js/clicklove.js"></script>
<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand_background.jpeg)">
      <div class="brand">
        <a href="/about/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.png">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">陈飞宇</h5>
          <a href="mailto:chinachenfeiyu@outlook.com" title="chinachenfeiyu@outlook.com" class="mail">chinachenfeiyu@outlook.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/books"  >
                <i class="icon icon-lg icon-book"></i>
                读书
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/movies"  >
                <i class="icon icon-lg icon-film"></i>
                影视
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/games"  >
                <i class="icon icon-lg icon-gamepad"></i>
                游戏
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/meditations"  >
                <i class="icon icon-lg icon-leaf"></i>
                随笔
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/links"  >
                <i class="icon icon-lg icon-link"></i>
                友情链接
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/lingyixia" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Bert</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale" align="center">
        <h1 class="title">Bert</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-07-22T13:55:40.000Z" itemprop="datePublished" class="page-time">
  2019-07-22
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/NLP/">NLP</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Bert源码分析"><span class="post-toc-number">1.</span> <span class="post-toc-text">Bert源码分析</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#组织结构"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">组织结构</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#modeling-py"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">modeling.py</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#BertConfig"><span class="post-toc-number">1.2.1.</span> <span class="post-toc-text">BertConfig</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#BertModel"><span class="post-toc-number">1.2.2.</span> <span class="post-toc-text">BertModel</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#embedding-lookup"><span class="post-toc-number">1.2.3.</span> <span class="post-toc-text">embedding_lookup</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#embedding-postprocessor"><span class="post-toc-number">1.2.4.</span> <span class="post-toc-text">embedding_postprocessor</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#create-attention-mask-from-input-mask"><span class="post-toc-number">1.2.5.</span> <span class="post-toc-text">create_attention_mask_from_input_mask</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#transformer-model"><span class="post-toc-number">1.2.6.</span> <span class="post-toc-text">transformer_model</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#attention-layer"><span class="post-toc-number">1.2.7.</span> <span class="post-toc-text">attention_layer</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#激活函数"><span class="post-toc-number">1.2.8.</span> <span class="post-toc-text">激活函数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#总结一"><span class="post-toc-number">1.2.9.</span> <span class="post-toc-text">总结一:</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#create-pretraining-data-py"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">create_pretraining_data.py</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#def-main"><span class="post-toc-number">1.3.1.</span> <span class="post-toc-text">def main</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#class-TrainingInstance"><span class="post-toc-number">1.3.2.</span> <span class="post-toc-text">class TrainingInstance</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#def-create-training-instances"><span class="post-toc-number">1.3.3.</span> <span class="post-toc-text">def create_training_instances</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#def-create-instances-from-document"><span class="post-toc-number">1.3.4.</span> <span class="post-toc-text">def create_instances_from_document</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#def-create-masked-lm-predictions"><span class="post-toc-number">1.3.5.</span> <span class="post-toc-text">def create_masked_lm_predictions</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tokenization-py"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">tokenization.py</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#FullTokenizer"><span class="post-toc-number">1.4.1.</span> <span class="post-toc-text">FullTokenizer</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#class-BasicTokenizer"><span class="post-toc-number">1.4.2.</span> <span class="post-toc-text">class BasicTokenizer</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#class-WordpieceTokenizer"><span class="post-toc-number">1.4.3.</span> <span class="post-toc-text">class WordpieceTokenizer</span></a></li></ol></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-Bert"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Bert</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-07-22 21:55:40" datetime="2019-07-22T13:55:40.000Z"  itemprop="datePublished">2019-07-22</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/NLP/">NLP</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <blockquote>
<p>本文不讲Bert原理,拟进行Bert源码分析和应用</p>
</blockquote>
<h1 id="Bert源码分析"><a href="#Bert源码分析" class="headerlink" title="Bert源码分析"></a>Bert源码分析</h1><h2 id="组织结构"><a href="#组织结构" class="headerlink" title="组织结构"></a>组织结构</h2><blockquote>
<blockquote>
<p><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">这是Bert Github地址</a>,打开后会看到这样的结构:<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/img/Bert/bert.jpeg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>下面我将逐个分析上诉图片中加框的文件,其他的文件不是源码,不用分析.</p>
</blockquote>
</blockquote>
<h2 id="modeling-py"><a href="#modeling-py" class="headerlink" title="modeling.py"></a>modeling.py</h2><blockquote>
<blockquote>
<p>该文件是整个Bert模型源码,包含两个类:</p>
<ul>
<li>BertConfig:Bert配置类</li>
<li>BertModel:Bert模型类</li>
<li>embedding_lookup:用来返回函数token embedding词向量</li>
<li>embedding_postprocessor:得到token embedding+segment embedding+position embedding</li>
<li>create_attention_mask_from_input_mask得到mask,用来attention该attention的部分</li>
<li>transformer_model和attention_layer:Transform的ender部分,也就是self-attention,不解释了，看太多遍了.</li>
</ul>
</blockquote>
</blockquote>
<p><strong>注意上面的顺序,不是乱写的,是按照BertModel调用顺序组织的.</strong></p>
<h3 id="BertConfig"><a href="#BertConfig" class="headerlink" title="BertConfig"></a>BertConfig</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">class BertConfig(object):</span><br><span class="line">    def __init__(self,</span><br><span class="line">                 vocab_size,#词表大小</span><br><span class="line">                 hidden_size=768,#即是词向量维度又是Transform的隐藏层维度</span><br><span class="line">                 num_hidden_layers=12,#Transformer encoder中的隐藏层数,普通Transform中是6个</span><br><span class="line">                 num_attention_heads=12,multi-head attention 的head的数量,普通Transform中是8个</span><br><span class="line">                 intermediate_size=3072,encoder的“中间”隐层神经元数,普通Transform中是一个feed-forward</span><br><span class="line">                 hidden_act=&quot;gelu&quot;,#隐藏层激活函数</span><br><span class="line">                 hidden_dropout_prob=0.1,#隐层dropout率</span><br><span class="line">                 attention_probs_dropout_prob=0.1,#注意力部分的dropout</span><br><span class="line">                 max_position_embeddings=512,#最大位置编码长度,也就是序列的最大长度</span><br><span class="line">                 type_vocab_size=16,#token_type_ids的大小,所谓的token_type_ids在Bert中是0或1，也就是上句标记为0，下句标记为1，鬼知道默认为16是啥意思。。。</span><br><span class="line">                 initializer_range=0.02):随机初始化的参数</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_hidden_layers = num_hidden_layers</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        self.hidden_act = hidden_act</span><br><span class="line">        self.intermediate_size = intermediate_size</span><br><span class="line">        self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">        self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">        self.max_position_embeddings = max_position_embeddings</span><br><span class="line">        self.type_vocab_size = type_vocab_size</span><br><span class="line">        self.initializer_range = initializer_range</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_dict(cls, json_object):</span><br><span class="line">        config = BertConfig(vocab_size=None)</span><br><span class="line">        for (key, value) in six.iteritems(json_object):</span><br><span class="line">            config.__dict__[key] = value</span><br><span class="line">        return config</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_json_file(cls, json_file):</span><br><span class="line">        with tf.gfile.GFile(json_file, &quot;r&quot;) as reader:</span><br><span class="line">            text = reader.read()</span><br><span class="line">        return cls.from_dict(json.loads(text))</span><br><span class="line"></span><br><span class="line">    def to_dict(self):</span><br><span class="line">        output = copy.deepcopy(self.__dict__)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">    def to_json_string(self):</span><br><span class="line">        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + &quot;\n&quot;</span><br></pre></td></tr></table></figure>
<h3 id="BertModel"><a href="#BertModel" class="headerlink" title="BertModel"></a>BertModel</h3><blockquote>
<blockquote>
<p>现在进入正题,开始分析Bert模型源码</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">class BertModel(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None,use_one_hot_embeddings=False, scope=None):</span><br><span class="line">        config = copy.deepcopy(config)</span><br><span class="line">        if not is_training:</span><br><span class="line">            config.hidden_dropout_prob = 0.0</span><br><span class="line">            config.attention_probs_dropout_prob = 0.0</span><br><span class="line">        input_shape = get_shape_list(input_ids, expected_rank=2)</span><br><span class="line">        batch_size = input_shape[0]</span><br><span class="line">        seq_length = input_shape[1]</span><br><span class="line">        if input_mask is None:</span><br><span class="line">            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">        if token_type_ids is None:</span><br><span class="line">            token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">        with tf.variable_scope(scope, default_name=&quot;bert&quot;):</span><br><span class="line">            with tf.variable_scope(&quot;embeddings&quot;):</span><br><span class="line">                (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">                    input_ids=input_ids,</span><br><span class="line">                    vocab_size=config.vocab_size,</span><br><span class="line">                    embedding_size=config.hidden_size,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">                    use_one_hot_embeddings=use_one_hot_embeddings)#调用embedding_lookup得到初始词向量</span><br><span class="line">                self.embedding_output = embedding_postprocessor(</span><br><span class="line">                    input_tensor=self.embedding_output,</span><br><span class="line">                    use_token_type=True,</span><br><span class="line">                    token_type_ids=token_type_ids,</span><br><span class="line">                    token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">                    token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">                    use_position_embeddings=True,</span><br><span class="line">                    position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">                    dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">            with tf.variable_scope(&quot;encoder&quot;):</span><br><span class="line">                attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask)</span><br><span class="line">                self.all_encoder_layers = transformer_model(</span><br><span class="line">                    input_tensor=self.embedding_output,</span><br><span class="line">                    attention_mask=attention_mask,</span><br><span class="line">                    hidden_size=config.hidden_size,</span><br><span class="line">                    num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">                    num_attention_heads=config.num_attention_heads,</span><br><span class="line">                    intermediate_size=config.intermediate_size,</span><br><span class="line">                    intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">                    hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    do_return_all_layers=True)</span><br><span class="line"></span><br><span class="line">            self.sequence_output = self.all_encoder_layers[-1]</span><br><span class="line">            with tf.variable_scope(&quot;pooler&quot;):</span><br><span class="line">                first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)</span><br><span class="line">                self.pooled_output = tf.layers.dense(</span><br><span class="line">                    first_token_tensor,</span><br><span class="line">                    config.hidden_size,</span><br><span class="line">                    activation=tf.tanh,</span><br><span class="line">                    kernel_initializer=create_initializer(config.initializer_range))</span><br><span class="line">    def get_pooled_output(self):</span><br><span class="line">        return self.pooled_output</span><br><span class="line">    def get_sequence_output(self):</span><br><span class="line">        return self.sequence_output</span><br><span class="line"></span><br><span class="line">    def get_all_encoder_layers(self):</span><br><span class="line">        return self.all_encoder_layers</span><br><span class="line"></span><br><span class="line">    def get_embedding_output(self):</span><br><span class="line">        return self.embedding_output</span><br><span class="line"></span><br><span class="line">    def get_embedding_table(self):</span><br><span class="line">        return self.embedding_table</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<p>参数说明:</p>
<ul>
<li><code>config</code>:一个<code>BertConfig</code>实例</li>
<li><code>is_training</code>:<code>bool</code>类型,是否是训练流程,用类控制是否dropout</li>
<li><code>input_ids</code>:输入<code>Tensor</code>, <code>shape</code>是<code>[batch_size, seq_length]</code>.</li>
<li><code>input_mask</code>:<code>shape</code>是<code>[batch_size, seq_length]</code>无需细讲</li>
<li><code>token_type_ids</code>:<code>shape</code>是<code>[batch_size, seq_length]</code>,<code>bert</code>中就是0或1</li>
<li><code>use_one_hot_embeddings</code>:在<code>embedding_lookup</code>返回词向量的时候使用,详细见<code>embedding_lookup</code>函数</li>
</ul>
</blockquote>
</blockquote>
<h3 id="embedding-lookup"><a href="#embedding-lookup" class="headerlink" title="embedding_lookup"></a>embedding_lookup</h3><blockquote>
<blockquote>
<p>为了得到进入模型的词向量(token embedding)</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def embedding_lookup(input_ids,vocab_size,embedding_size=128,initializer_range=0.02,word_embedding_name=&quot;word_embeddings&quot;,use_one_hot_embeddings=False):</span><br><span class="line">  if input_ids.shape.ndims == 2:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[-1])</span><br><span class="line">  embedding_table = tf.get_variable(name=word_embedding_name,shape=[vocab_size, embedding_size],initializer=create_initializer(initializer_range))</span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [-1])</span><br><span class="line">  if use_one_hot_embeddings:</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  else:</span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line">  output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size])</span><br><span class="line">  return (output, embedding_table)</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<p>参数说明：</p>
<ul>
<li>input_ids：[batch_size, seq_length]</li>
<li>vocab_size:词典大小</li>
<li>initializer_range：初始化参数</li>
<li>word_embedding_name:不解释</li>
<li>use_one_hot_embeddings:是否使用one_hot方式初始化(为啥我感觉这里是True还是False结果得到的结果是一样的？？？？？)如下代码.</li>
</ul>
</blockquote>
</blockquote>
<p>return:token embedding:[batch_size, seq_length, embedding_size].和embedding_table(不解释)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line">flat_input_ids = [2, 4, 5]</span><br><span class="line">embedding_table = tf.constant(value=[[1, 2, 3, 4],</span><br><span class="line">                                     [5, 6, 7, 8],</span><br><span class="line">                                     [9, 1, 2, 3],</span><br><span class="line">                                     [5, 6, 7, 8],</span><br><span class="line">                                     [6, 4, 78, 9],</span><br><span class="line">                                     [6, 8, 9, 3]],dtype=tf.float32)</span><br><span class="line">one_hot_input_ids = tf.one_hot(flat_input_ids, depth=6)</span><br><span class="line">output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">print(output)</span><br><span class="line">print(100*&apos;*&apos;)</span><br><span class="line">output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
<h3 id="embedding-postprocessor"><a href="#embedding-postprocessor" class="headerlink" title="embedding_postprocessor"></a>embedding_postprocessor</h3><blockquote>
<blockquote>
<p>bert模型的输入向量有三个,embedding_lookup得到的是token embedding 我们还需要segment embedding和position embedding,这三者的维度是完全相同的(废话不相同怎么加啊。。。)本部分代码会将这三个embeddig加起来并dropout</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def embedding_postprocessor(input_tensor,</span><br><span class="line">                            use_token_type=False,</span><br><span class="line">                            token_type_ids=None,</span><br><span class="line">                            token_type_vocab_size=16,</span><br><span class="line">                            token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">                            use_position_embeddings=True,</span><br><span class="line">                            position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">                            initializer_range=0.02,</span><br><span class="line">                            max_position_embeddings=512,</span><br><span class="line">                            dropout_prob=0.1):</span><br><span class="line">    input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">    batch_size = input_shape[0]</span><br><span class="line">    seq_length = input_shape[1]</span><br><span class="line">    width = input_shape[2]</span><br><span class="line">    output = input_tensor</span><br><span class="line">    if use_token_type:</span><br><span class="line">        if token_type_ids is None:</span><br><span class="line">            raise ValueError(&quot;`token_type_ids` must be specified if&quot;&quot;`use_token_type` is True.&quot;)</span><br><span class="line">        token_type_table = tf.get_variable(name=token_type_embedding_name,</span><br><span class="line">                                           shape=[token_type_vocab_size, width],</span><br><span class="line">                                           initializer=create_initializer(initializer_range))</span><br><span class="line">        flat_token_type_ids = tf.reshape(token_type_ids, [-1])</span><br><span class="line">        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">        token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width])</span><br><span class="line">        output += token_type_embeddings</span><br><span class="line">    if use_position_embeddings:</span><br><span class="line">        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">        with tf.control_dependencies([assert_op]):</span><br><span class="line">            full_position_embeddings = tf.get_variable(name=position_embedding_name,</span><br><span class="line">                                                       shape=[max_position_embeddings, width],</span><br><span class="line">                                                       initializer=create_initializer(initializer_range))</span><br><span class="line">            position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])</span><br><span class="line">            num_dims = len(output.shape.as_list())</span><br><span class="line">            position_broadcast_shape = []</span><br><span class="line">            for _ in range(num_dims - 2):</span><br><span class="line">                position_broadcast_shape.append(1)</span><br><span class="line">            position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">            position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)</span><br><span class="line">            output += position_embeddings</span><br><span class="line">    output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">    return output</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<p>参数说明:</p>
<ul>
<li>input_tensor:token embedding[batch_size, seq_length, embedding_size]</li>
<li>use_token_type是否使用segment embedding</li>
<li>token_type_ids:[batch_size, seq_length],这两个参数其实就是控制生成segment embedding的,上诉代码中的<code>output += token_type_embeddings</code>就是得到token embedding+segment embedding</li>
<li>use_position_embeddings:是否使用位置信息</li>
<li>max_position_embeddings:序列最大长度<br>注:</li>
<li>本部分代码中的<code>width</code>其实就是词向量维度(换个<code>embedding_size</code>能死啊。。。)</li>
<li>可以看出位置信息跟Transform的固定方式不一样，它是训练出来的.</li>
<li><code>output += position_embeddings</code>就得到了三者的想加结果<br>return :token embedding+segment embedding+position_embeddings</li>
</ul>
</blockquote>
</blockquote>
<h3 id="create-attention-mask-from-input-mask"><a href="#create-attention-mask-from-input-mask" class="headerlink" title="create_attention_mask_from_input_mask"></a>create_attention_mask_from_input_mask</h3><blockquote>
<blockquote>
<p>目的是将本来shape为[batch_size, seq_length]转为[batch_size, seq_length,seq_length],为什么要这样的维度呢?因为…..算了麻烦不写了，去我的另一篇<a href="https://lingyixia.github.io/2019/04/05/transformer/">Transform</a>中看吧</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def create_attention_mask_from_input_mask(from_tensor, to_mask):</span><br><span class="line">    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])</span><br><span class="line">    batch_size = from_shape[0]</span><br><span class="line">    from_seq_length = from_shape[1]</span><br><span class="line">    to_shape = get_shape_list(to_mask, expected_rank=2)</span><br><span class="line">    to_seq_length = to_shape[1]</span><br><span class="line">    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)</span><br><span class="line">    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)</span><br><span class="line">    mask = broadcast_ones * to_mask</span><br><span class="line">    return mask</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<p>参数说明:</p>
<ul>
<li>from_tensor:[batch_size, seq_length].</li>
<li>to_mask:[batch_size, seq_length]<br>注:<code>Transform</code>中的<code>mask</code>和平常用的不太一样,这里的<code>mask</code>是为了在计算<code>attention</code>的时候”看不到不应该看到的内容”,计算方式为该看到的<code>mask</code>为0，不该看到的<code>mask</code>为一个负的很大的数字,然后两者相加(平常使用<code>mask</code>是看到的为1，看不到的为0，然后两者做点乘)，这样在计算<code>softmax</code>的时候那些负数的<code>attention</code>会非常非常小,也就基本看不到了.</li>
</ul>
</blockquote>
</blockquote>
<h3 id="transformer-model"><a href="#transformer-model" class="headerlink" title="transformer_model"></a>transformer_model</h3><blockquote>
<blockquote>
<p>这一部分是<code>Transform</code>部分,但是只有<code>encoder</code>部分,从<code>BertModel</code>中的<code>with tf.variable_scope(&quot;encoder&quot;):</code>这一部分也可以看出来</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">def transformer_model(input_tensor,</span><br><span class="line">                      attention_mask=None,</span><br><span class="line">                      hidden_size=768,</span><br><span class="line">                      num_hidden_layers=12,</span><br><span class="line">                      num_attention_heads=12,</span><br><span class="line">                      intermediate_size=3072,</span><br><span class="line">                      intermediate_act_fn=gelu,</span><br><span class="line">                      hidden_dropout_prob=0.1,</span><br><span class="line">                      attention_probs_dropout_prob=0.1,</span><br><span class="line">                      initializer_range=0.02,</span><br><span class="line">                      do_return_all_layers=False):</span><br><span class="line">    if hidden_size % num_attention_heads != 0:</span><br><span class="line">        raise ValueError(</span><br><span class="line">            &quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span><br><span class="line">            &quot;heads (%d)&quot; % (hidden_size, num_attention_heads))</span><br><span class="line">    attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">    input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">    batch_size = input_shape[0]</span><br><span class="line">    seq_length = input_shape[1]</span><br><span class="line">    input_width = input_shape[2]</span><br><span class="line">    if input_width != hidden_size:</span><br><span class="line">        raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; %</span><br><span class="line">                         (input_width, hidden_size))</span><br><span class="line">    prev_output = reshape_to_matrix(input_tensor)#这个不单独写了,就是将[batch_size, seq_length, embedding_size]的input 给reahpe为[batch_size*seq_length,embedding_size]</span><br><span class="line">    all_layer_outputs = []</span><br><span class="line">    for layer_idx in range(num_hidden_layers):</span><br><span class="line">        with tf.variable_scope(&quot;layer_%d&quot; % layer_idx):</span><br><span class="line">            layer_input = prev_output</span><br><span class="line">            with tf.variable_scope(&quot;attention&quot;):</span><br><span class="line">                attention_heads = []</span><br><span class="line">                with tf.variable_scope(&quot;self&quot;):</span><br><span class="line">                    attention_head = attention_layer(from_tensor=layer_input,</span><br><span class="line">                                                     to_tensor=layer_input,</span><br><span class="line">                                                     attention_mask=attention_mask,</span><br><span class="line">                                                     num_attention_heads=num_attention_heads,</span><br><span class="line">                                                     size_per_head=attention_head_size,</span><br><span class="line">                                                     attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">                                                     initializer_range=initializer_range,</span><br><span class="line">                                                     do_return_2d_tensor=True,</span><br><span class="line">                                                     batch_size=batch_size,</span><br><span class="line">                                                     from_seq_length=seq_length,</span><br><span class="line">                                                     to_seq_length=seq_length)</span><br><span class="line">                    attention_heads.append(attention_head)</span><br><span class="line">                attention_output = None</span><br><span class="line">                if len(attention_heads) == 1:</span><br><span class="line">                    attention_output = attention_heads[0]</span><br><span class="line">                else:</span><br><span class="line">                    attention_output = tf.concat(attention_heads, axis=-1)</span><br><span class="line">                with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">                    attention_output = tf.layers.dense(attention_output,</span><br><span class="line">                                                       hidden_size,</span><br><span class="line">                                                       kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">                    attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">                    attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line">            with tf.variable_scope(&quot;intermediate&quot;):#feed-forword部分</span><br><span class="line">                intermediate_output = tf.layers.dense(attention_output,</span><br><span class="line">                                                      intermediate_size,</span><br><span class="line">                                                      activation=intermediate_act_fn,</span><br><span class="line">                                                      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">            with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">                layer_output = tf.layers.dense(intermediate_output,</span><br><span class="line">                                               hidden_size,</span><br><span class="line">                                               kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">                layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">                layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">                prev_output = layer_output</span><br><span class="line">                all_layer_outputs.append(layer_output)</span><br><span class="line">    if do_return_all_layers:</span><br><span class="line">        final_outputs = []</span><br><span class="line">        for layer_output in all_layer_outputs:</span><br><span class="line">            final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">            final_outputs.append(final_output)</span><br><span class="line">        return final_outputs</span><br><span class="line">    else:</span><br><span class="line">        final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">        return final_output</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<p>参数说明:</p>
<ul>
<li>input_tensor:token embedding+segment embedding+position embedding [batch_size, seq_length, embedding_size]</li>
<li>attention_mask:[batch_size, seq_length,seq_length]</li>
<li>hidden_size:不解释</li>
<li>num_hidden_layers:多少个<code>ecncoder block</code></li>
<li>num_attention_heads:多少个<code>head</code></li>
<li>intermediate_size:<code>feed forward</code>隐藏层维度</li>
<li>intermediate_act_fn:<code>feed forward</code>激活函数<br>其他的不解释了<br>return [batch_size, seq_length, hidden_size],</li>
</ul>
</blockquote>
</blockquote>
<h3 id="attention-layer"><a href="#attention-layer" class="headerlink" title="attention_layer"></a>attention_layer</h3><blockquote>
<blockquote>
<p>其实就是<code>self-attention</code>,但是在计算的时候全都转换为了二维矩阵，按注释的意思是避免反复reshape,因为reshape在CPU/GPU上易于实现，但是在TPU上不易实现,这样可以加速训练.</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">def attention_layer(from_tensor,</span><br><span class="line">                    to_tensor,</span><br><span class="line">                    attention_mask=None,</span><br><span class="line">                    num_attention_heads=1,</span><br><span class="line">                    size_per_head=512,</span><br><span class="line">                    query_act=None,</span><br><span class="line">                    key_act=None,</span><br><span class="line">                    value_act=None,</span><br><span class="line">                    attention_probs_dropout_prob=0.0,</span><br><span class="line">                    initializer_range=0.02,</span><br><span class="line">                    do_return_2d_tensor=False,</span><br><span class="line">                    batch_size=None,</span><br><span class="line">                    from_seq_length=None,</span><br><span class="line">                    to_seq_length=None):</span><br><span class="line">    def transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, width):</span><br><span class="line">        output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line">        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])</span><br><span class="line">        return output_tensor</span><br><span class="line"></span><br><span class="line">    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])</span><br><span class="line">    to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])</span><br><span class="line">    if len(from_shape) != len(to_shape):</span><br><span class="line">        raise ValueError(&quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;)</span><br><span class="line">    if len(from_shape) == 3:</span><br><span class="line">        batch_size = from_shape[0]</span><br><span class="line">        from_seq_length = from_shape[1]</span><br><span class="line">        to_seq_length = to_shape[1]</span><br><span class="line">    elif len(from_shape) == 2:</span><br><span class="line">        if (batch_size is None or from_seq_length is None or to_seq_length is None):</span><br><span class="line">            raise ValueError(</span><br><span class="line">                &quot;When passing in rank 2 tensors to attention_layer, the values &quot;</span><br><span class="line">                &quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot;</span><br><span class="line">                &quot;must all be specified.&quot;)</span><br><span class="line">    from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">    to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line">    query_layer = tf.layers.dense(from_tensor_2d,</span><br><span class="line">                                  num_attention_heads * size_per_head,</span><br><span class="line">                                  activation=query_act,</span><br><span class="line">                                  name=&quot;query&quot;,</span><br><span class="line">                                  kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    key_layer = tf.layers.dense(to_tensor_2d,</span><br><span class="line">                                num_attention_heads * size_per_head,</span><br><span class="line">                                activation=key_act,</span><br><span class="line">                                name=&quot;key&quot;,</span><br><span class="line">                                kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    value_layer = tf.layers.dense(to_tensor_2d,</span><br><span class="line">                                  num_attention_heads * size_per_head,</span><br><span class="line">                                  activation=value_act,</span><br><span class="line">                                  name=&quot;value&quot;,</span><br><span class="line">                                  kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads, from_seq_length, size_per_head)</span><br><span class="line">    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, to_seq_length, size_per_head)</span><br><span class="line">    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)</span><br><span class="line">    attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head)))</span><br><span class="line">    if attention_mask is not None:</span><br><span class="line">        attention_mask = tf.expand_dims(attention_mask, axis=[1])</span><br><span class="line">        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0</span><br><span class="line">        attention_scores += adder#这里就是使用mask来attention该attention的部分</span><br><span class="line">    attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line">    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line">    value_layer = tf.reshape(value_layer, [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line">    value_layer = tf.transpose(value_layer, [0, 2, 1, 3])</span><br><span class="line">    context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line">    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])</span><br><span class="line">    if do_return_2d_tensor:</span><br><span class="line">        context_layer = tf.reshape(context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">    else:</span><br><span class="line">        context_layer = tf.reshape(context_layer, [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">    return context_layer</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<p>参数说明:</p>
<ul>
<li>from_tensor在Transform中被转为二维[batch_size*seq_length, embedding_size]</li>
<li>to_shape:传过来的参数跟from_tensor一毛一样,在这里没什么卵用其实,因为q和k的length是一样的</li>
<li>attention_mask:[batch_size, seq_length,seq_length]</li>
<li>num_attention_heads:head数量</li>
<li>size_per_head:每一个head维度,代码中是用总维度除以head数量得到的:attention_head_size = int(hidden_size / num_attention_heads)<br>return: return :[batch_size, from_seq_length,num_attention_heads * size_per_head].</li>
</ul>
</blockquote>
</blockquote>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def gelu(x):</span><br><span class="line">  cdf = 0.5 * (1.0 + tf.tanh(</span><br><span class="line">      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))</span><br><span class="line">  return x * cdf</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<p>这个激活函数很有特色，其实这个公式就是$x \times \Phi(x)$,后一项是正态函数,也就是说,gelu中后面那一大堆其实近似等于$\int_{-\infty}^{x}\frac{1}{\sqrt(2\pi)}e^{-\frac{x^2}{2}}dx$,至于咋来的这个近似值，还不清楚。<br>测试函数:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from scipy import stats</span><br><span class="line">import math</span><br><span class="line">a = stats.norm.cdf(2, 0, 1)</span><br><span class="line"></span><br><span class="line">def gelu(x):</span><br><span class="line">    return 0.5 * (1.0 + math.tanh((math.sqrt(2 / math.pi) * (x + 0.044715 * math.pow(x, 3)))))</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line">print(gelu(2))</span><br><span class="line">#结果:</span><br><span class="line">#0.9772498680518208</span><br><span class="line">#0.9772988470438875</span><br></pre></td></tr></table></figure></p>
</blockquote>
</blockquote>
<h3 id="总结一"><a href="#总结一" class="headerlink" title="总结一:"></a>总结一:</h3><p>看完模型感觉真特么简单这模型,似乎除了self-attention就啥都没有了,但是先别着急,一般情况下模型是重点，但是对于Bert而言，模型却仅仅是开始，真正的创新点还在下面.</p>
<h2 id="create-pretraining-data-py"><a href="#create-pretraining-data-py" class="headerlink" title="create_pretraining_data.py"></a>create_pretraining_data.py</h2><blockquote>
<blockquote>
<p>这部分代码用来生成训练样本,我们从<code>main</code>函数开始看起,首先进入<code>tokenization.py</code></p>
</blockquote>
</blockquote>
<h3 id="def-main"><a href="#def-main" class="headerlink" title="def main"></a>def main</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def main(_):</span><br><span class="line">    tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line">    tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)</span><br><span class="line">    input_files = []</span><br><span class="line">    for input_pattern in FLAGS.input_file.split(&quot;,&quot;):</span><br><span class="line">        input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line">    tf.logging.info(&quot;*** Reading from input files ***&quot;)</span><br><span class="line">    for input_file in input_files:</span><br><span class="line">        tf.logging.info(&quot;  %s&quot;, input_file)</span><br><span class="line">    rng = random.Random(FLAGS.random_seed)</span><br><span class="line">    instances = create_training_instances(input_files,</span><br><span class="line">                                          tokenizer,</span><br><span class="line">                                          FLAGS.max_seq_length,</span><br><span class="line">                                          FLAGS.dupe_factor,</span><br><span class="line">                                          FLAGS.short_seq_prob,</span><br><span class="line">                                          FLAGS.masked_lm_prob,</span><br><span class="line">                                          FLAGS.max_predictions_per_seq,</span><br><span class="line">                                          rng)</span><br><span class="line"></span><br><span class="line">    output_files = FLAGS.output_file.split(&quot;,&quot;)</span><br><span class="line">    tf.logging.info(&quot;*** Writing to output files ***&quot;)</span><br><span class="line">    for output_file in output_files:</span><br><span class="line">        tf.logging.info(&quot;  %s&quot;, output_file)</span><br><span class="line">    write_instance_to_example_files(instances,</span><br><span class="line">                                    tokenizer, </span><br><span class="line">                                    FLAGS.max_seq_length,</span><br><span class="line">                                    FLAGS.max_predictions_per_seq, </span><br><span class="line">                                    output_files)</span><br></pre></td></tr></table></figure>
<h3 id="class-TrainingInstance"><a href="#class-TrainingInstance" class="headerlink" title="class TrainingInstance"></a>class TrainingInstance</h3><blockquote>
<blockquote>
<p>单个训练样本类,看<code>__init__</code>就能看出来，没什么其他东西</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class TrainingInstance(object):</span><br><span class="line">    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,</span><br><span class="line">                 is_random_next):</span><br><span class="line">        self.tokens = tokens</span><br><span class="line">        self.segment_ids = segment_ids</span><br><span class="line">        self.is_random_next = is_random_next</span><br><span class="line">        self.masked_lm_positions = masked_lm_positions</span><br><span class="line">        self.masked_lm_labels = masked_lm_labels</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        s = &quot;&quot;</span><br><span class="line">        s += &quot;tokens: %s\n&quot; % (&quot; &quot;.join(</span><br><span class="line">            [tokenization.printable_text(x) for x in self.tokens]))</span><br><span class="line">        s += &quot;segment_ids: %s\n&quot; % (&quot; &quot;.join([str(x) for x in self.segment_ids]))</span><br><span class="line">        s += &quot;is_random_next: %s\n&quot; % self.is_random_next</span><br><span class="line">        s += &quot;masked_lm_positions: %s\n&quot; % (&quot; &quot;.join(</span><br><span class="line">            [str(x) for x in self.masked_lm_positions]))</span><br><span class="line">        s += &quot;masked_lm_labels: %s\n&quot; % (&quot; &quot;.join(</span><br><span class="line">            [tokenization.printable_text(x) for x in self.masked_lm_labels]))</span><br><span class="line">        s += &quot;\n&quot;</span><br><span class="line">        return s</span><br></pre></td></tr></table></figure>
<h3 id="def-create-training-instances"><a href="#def-create-training-instances" class="headerlink" title="def create_training_instances"></a>def create_training_instances</h3><blockquote>
<blockquote>
<p>这个函数是重中之重，用来生成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def create_training_instances(input_files, tokenizer, max_seq_length,</span><br><span class="line">                              dupe_factor, short_seq_prob, masked_lm_prob,</span><br><span class="line">                              max_predictions_per_seq, rng):</span><br><span class="line">    all_documents = [[]]#外层是文档，内层是文档中的每个句子</span><br><span class="line">    for input_file in input_files:</span><br><span class="line">        with tf.gfile.GFile(input_file, &quot;r&quot;) as reader:</span><br><span class="line">            while True:</span><br><span class="line">                line = tokenization.convert_to_unicode(reader.readline())</span><br><span class="line">                if not line:</span><br><span class="line">                    break</span><br><span class="line">                line = line.strip()</span><br><span class="line">                if not line:# 空行表示文档分割</span><br><span class="line">                    all_documents.append([])</span><br><span class="line">                tokens = tokenizer.tokenize(line)</span><br><span class="line">                if tokens:</span><br><span class="line">                    all_documents[-1].append(tokens)</span><br><span class="line">    all_documents = [x for x in all_documents if x]</span><br><span class="line">    rng.shuffle(all_documents)</span><br><span class="line">    vocab_words = list(tokenizer.vocab.keys())</span><br><span class="line">    instances = []</span><br><span class="line">    for _ in range(dupe_factor):</span><br><span class="line">        for document_index in range(len(all_documents)):</span><br><span class="line">            instances.extend(</span><br><span class="line">                create_instances_from_document(all_documents,</span><br><span class="line">                                               document_index,</span><br><span class="line">                                               max_seq_length,</span><br><span class="line">                                               short_seq_prob,</span><br><span class="line">                                               masked_lm_prob,</span><br><span class="line">                                               max_predictions_per_seq,</span><br><span class="line">                                               vocab_words,</span><br><span class="line">                                               rng))</span><br><span class="line">    rng.shuffle(instances)</span><br><span class="line">    return instances</span><br></pre></td></tr></table></figure></p>
<p>参数说明:<br>dupe_factor:每一个句子用几次:因为如果一个句子只用一次的话那么mask的位置就是固定的，这样我们把每个句子在训练中都多用几次,而且没次的mask位置都不相同,就可以防止某些词永远看不到<br>short_seq_prob:长度小于“max_seq_length”的样本比例。因为在fine-tune过程里面输入的target_seq_length是可变的（小于等于max_seq_length），那么为了防止过拟合也需要在pre-train的过程当中构造一些短的样本<br>max_predictions_per_seq:一个句子里最多有多少个[MASK]标记<br>masked_lm_prob:多少比例的Token被MASK掉<br>rng:随机率</p>
</blockquote>
</blockquote>
<h3 id="def-create-instances-from-document"><a href="#def-create-instances-from-document" class="headerlink" title="def create_instances_from_document"></a>def create_instances_from_document</h3><blockquote>
<blockquote>
<p>一个文档中抽取训练样本,重中之重</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">def create_instances_from_document(all_documents, document_index, max_seq_length, short_seq_prob,</span><br><span class="line">                                   masked_lm_prob, max_predictions_per_seq, vocab_words, rng):</span><br><span class="line">    document = all_documents[document_index]</span><br><span class="line">    # 为[CLS], [SEP], [SEP]预留三个空位</span><br><span class="line">    max_num_tokens = max_seq_length - 3</span><br><span class="line">    target_seq_length = max_num_tokens  # 以short_seq_prob的概率随机生成（2~max_num_tokens）的长度</span><br><span class="line">    if rng.random() &lt; short_seq_prob:</span><br><span class="line">        target_seq_length = rng.randint(2, max_num_tokens)</span><br><span class="line">    instances = []</span><br><span class="line">    current_chunk = []</span><br><span class="line">    current_length = 0</span><br><span class="line">    i = 0</span><br><span class="line">    while i &lt; len(document):</span><br><span class="line">        segment = document[i]</span><br><span class="line">        current_chunk.append(segment)</span><br><span class="line">        current_length += len(segment)</span><br><span class="line">        # 将句子依次加入current_chunk中，直到加完或者达到限制的最大长度</span><br><span class="line">        if i == len(document) - 1 or current_length &gt;= target_seq_length:</span><br><span class="line">            if current_chunk:</span><br><span class="line">                # `a_end`是第一个句子A结束的下标</span><br><span class="line">                a_end = 1</span><br><span class="line">                if len(current_chunk) &gt;= 2:</span><br><span class="line">                    a_end = rng.randint(1, len(current_chunk) - 1)</span><br><span class="line">                tokens_a = []</span><br><span class="line">                for j in range(a_end):</span><br><span class="line">                    tokens_a.extend(current_chunk[j])</span><br><span class="line">                tokens_b = []</span><br><span class="line">                is_random_next = False</span><br><span class="line">                # `a_end`是第一个句子A结束的下标</span><br><span class="line">                if len(current_chunk) == 1 or rng.random() &lt; 0.5:</span><br><span class="line">                    is_random_next = True</span><br><span class="line">                    target_b_length = target_seq_length - len(tokens_a)</span><br><span class="line">                    # 随机的挑选另外一篇文档的随机开始的句子</span><br><span class="line">                    # 但是理论上有可能随机到的文档就是当前文档，因此需要一个while循环</span><br><span class="line">                    # 这里只while循环10次，理论上还是有重复的可能性，但是我们忽略</span><br><span class="line">                    for _ in range(10):</span><br><span class="line">                        random_document_index = rng.randint(0, len(all_documents) - 1)</span><br><span class="line">                        if random_document_index != document_index:</span><br><span class="line">                            break</span><br><span class="line">                    random_document = all_documents[random_document_index]</span><br><span class="line">                    random_start = rng.randint(0, len(random_document) - 1)</span><br><span class="line">                    for j in range(random_start, len(random_document)):</span><br><span class="line">                        tokens_b.extend(random_document[j])</span><br><span class="line">                        if len(tokens_b) &gt;= target_b_length:</span><br><span class="line">                            break</span><br><span class="line">                    # 对于上述构建的随机下一句，我们并没有真正地使用它们</span><br><span class="line">                    # 所以为了避免数据浪费，我们将其“放回”</span><br><span class="line">                    num_unused_segments = len(current_chunk) - a_end</span><br><span class="line">                    i -= num_unused_segments</span><br><span class="line">                else:</span><br><span class="line">                    is_random_next = False</span><br><span class="line">                    for j in range(a_end, len(current_chunk)):</span><br><span class="line">                        tokens_b.extend(current_chunk[j])</span><br><span class="line">                # 如果太多了，随机去掉一些</span><br><span class="line">                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)</span><br><span class="line">                assert len(tokens_a) &gt;= 1</span><br><span class="line">                assert len(tokens_b) &gt;= 1</span><br><span class="line">                tokens = []</span><br><span class="line">                segment_ids = []</span><br><span class="line">                # 处理句子A</span><br><span class="line">                tokens.append(&quot;[CLS]&quot;)</span><br><span class="line">                segment_ids.append(0)</span><br><span class="line">                for token in tokens_a:</span><br><span class="line">                    tokens.append(token)</span><br><span class="line">                    segment_ids.append(0)</span><br><span class="line">                # 句子A结束，加上【SEP】</span><br><span class="line">                tokens.append(&quot;[SEP]&quot;)</span><br><span class="line">                segment_ids.append(0)</span><br><span class="line">                # 处理句子B</span><br><span class="line">                for token in tokens_b:</span><br><span class="line">                    tokens.append(token)</span><br><span class="line">                    segment_ids.append(1)</span><br><span class="line">                # 句子B结束，加上【SEP】</span><br><span class="line">                tokens.append(&quot;[SEP]&quot;)</span><br><span class="line">                segment_ids.append(1)</span><br><span class="line">                # 调用 create_masked_lm_predictions来随机对某些Token进行mask</span><br><span class="line">                (tokens, masked_lm_positions,</span><br><span class="line">                 masked_lm_labels) = create_masked_lm_predictions(tokens,</span><br><span class="line">                                                                  masked_lm_prob,</span><br><span class="line">                                                                  max_predictions_per_seq,</span><br><span class="line">                                                                  vocab_words, rng)</span><br><span class="line">                instance = TrainingInstance(tokens=tokens,</span><br><span class="line">                                            segment_ids=segment_ids,</span><br><span class="line">                                            is_random_next=is_random_next,</span><br><span class="line">                                            masked_lm_positions=masked_lm_positions,</span><br><span class="line">                                            masked_lm_labels=masked_lm_labels)</span><br><span class="line">                instances.append(instance)</span><br><span class="line">            current_chunk = []</span><br><span class="line">            current_length = 0</span><br><span class="line">        i += 1</span><br><span class="line">    return instances</span><br></pre></td></tr></table></figure>
<h3 id="def-create-masked-lm-predictions"><a href="#def-create-masked-lm-predictions" class="headerlink" title="def create_masked_lm_predictions"></a>def create_masked_lm_predictions</h3><blockquote>
<blockquote>
<p>真正的mask在这里实现</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng):</span><br><span class="line">    cand_indexes = [] # [CLS]和[SEP]不能用于MASK</span><br><span class="line">    for (i, token) in enumerate(tokens):</span><br><span class="line">        if token == &quot;[CLS]&quot; or token == &quot;[SEP]&quot;:</span><br><span class="line">            continue</span><br><span class="line">        if (FLAGS.do_whole_word_mask and len(cand_indexes) &gt;= 1 and</span><br><span class="line">                token.startswith(&quot;##&quot;)):</span><br><span class="line">            cand_indexes[-1].append(i)</span><br><span class="line">        else:</span><br><span class="line">            cand_indexes.append([i])</span><br><span class="line">    rng.shuffle(cand_indexes)</span><br><span class="line">    output_tokens = list(tokens)</span><br><span class="line">    num_to_predict = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob))))</span><br><span class="line">    masked_lms = []</span><br><span class="line">    covered_indexes = set()</span><br><span class="line">    for index_set in cand_indexes:</span><br><span class="line">        if len(masked_lms) &gt;= num_to_predict:</span><br><span class="line">            break</span><br><span class="line">        if len(masked_lms) + len(index_set) &gt; num_to_predict:</span><br><span class="line">            continue</span><br><span class="line">        is_any_index_covered = False</span><br><span class="line">        for index in index_set:</span><br><span class="line">            if index in covered_indexes:</span><br><span class="line">                is_any_index_covered = True</span><br><span class="line">                break</span><br><span class="line">        if is_any_index_covered:</span><br><span class="line">            continue</span><br><span class="line">        for index in index_set:</span><br><span class="line">            covered_indexes.add(index)</span><br><span class="line">            masked_token = None</span><br><span class="line">            # 80% of the time, replace with [MASK]</span><br><span class="line">            if rng.random() &lt; 0.8:</span><br><span class="line">                masked_token = &quot;[MASK]&quot;</span><br><span class="line">            else:</span><br><span class="line">            # 10% of the time, keep original</span><br><span class="line">                if rng.random() &lt; 0.5:</span><br><span class="line">                    masked_token = tokens[index]</span><br><span class="line">            # 10% of the time, replace with random word</span><br><span class="line">                else:</span><br><span class="line">                    masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]</span><br><span class="line">            output_tokens[index] = masked_token</span><br><span class="line">            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))</span><br><span class="line">    assert len(masked_lms) &lt;= num_to_predict</span><br><span class="line">     # 按照下标重排，保证是原来句子中出现的顺序</span><br><span class="line">    masked_lms = sorted(masked_lms, key=lambda x: x.index)</span><br><span class="line">    masked_lm_positions = []</span><br><span class="line">    masked_lm_labels = []</span><br><span class="line">    for p in masked_lms:</span><br><span class="line">        masked_lm_positions.append(p.index)</span><br><span class="line">        masked_lm_labels.append(p.label)</span><br><span class="line">    return (output_tokens, masked_lm_positions, masked_lm_labels)</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<p>代码流程是这样的:首先嫁给你一个句子随机打乱,并确定一个句子的15%是多少个token，设num_to_predict,然后对于[0,是多少个token，设num_to_predict]token，以80%的概率替换为[mask],10%的概率替换，10%的概率保持,这样就做到了对于15%的toke80% [mask],10%替换,10%保持。而预测的不是那15%的80（标注问题），而是全部15%。为什么要mask呢？你想啊，我们的目的是得到这样一个模型:输入一个句子，输出一个能够尽可能表示该句子的向量(用最容易理解的语言就是我们不知道输入的是什么玩意，但是我们需要知道输出的向量是什么),如果不mask直接训练那不就相当于用1来推导1？而如果我们mask一部分就意味着并不知道输入(至少不知道全部),至于为什么要把15不全部mask，我觉得这个解释很不错，但是过于专业化:</p>
<ul>
<li>如果把 100% 的输入替换为 [MASK]：模型会偏向为 [MASK] 输入建模，而不会学习到 non-masked 输入的表征。</li>
<li>如果把 90% 的输入替换为 [MASK]、10% 的输入替换为随机 token：模型会偏向认为 non-masked 输入是错的。</li>
<li>如果把 90% 的输入替换为 [MASK]、维持 10% 的输入不变：模型会偏向直接复制 non-masked 输入的上下文无关表征。<br>所以，为了使模型可以学习到相对有效的上下文相关表征，需要以 1:1 的比例使用两种策略处理 non-masked 输入。论文提及，随机替换的输入只占整体的 1.5%，似乎不会对最终效果有影响（模型有足够的容错余量）。<br>通俗点说就是全部mask的话就意味着用mask来预测真正的单词,学习的仅仅是mask(而且mask的每个词都不一样，学到的mask表示也不一样，很显然不合理)，加入10%的替换就意味着用错的词预测对的词，而10%保持不变意味着用1来推导1，因此后两个10%的作用其实是为了学到没有mask的部分。<br>或者还有一种解释方式: 因为每次都是要学习这15%的token，其他的学不到(认识到这一点很重要)倘若某一个词在训练模型的时候被mask了，而微调的时候出现了咋办？因此不管怎样，都必须让模型好歹”认识一下”这个词.<h2 id="tokenization-py"><a href="#tokenization-py" class="headerlink" title="tokenization.py"></a>tokenization.py</h2>按照<code>create_pretraining_data.py</code>中<code>main</code>的调用顺序，先看<code>FullTokenizer</code>类</li>
</ul>
</blockquote>
</blockquote>
<h3 id="FullTokenizer"><a href="#FullTokenizer" class="headerlink" title="FullTokenizer"></a>FullTokenizer</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class FullTokenizer(object):</span><br><span class="line">    def __init__(self, vocab_file, do_lower_case=True):</span><br><span class="line">        self.vocab = load_vocab(vocab_file)</span><br><span class="line">        self.inv_vocab = &#123;v: k for k, v in self.vocab.items()&#125;</span><br><span class="line">        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)</span><br><span class="line">        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)</span><br><span class="line"></span><br><span class="line">    def tokenize(self, text):</span><br><span class="line">        split_tokens = []</span><br><span class="line">        for token in self.basic_tokenizer.tokenize(text):</span><br><span class="line">            for sub_token in self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">                split_tokens.append(sub_token)</span><br><span class="line">        return split_tokens</span><br><span class="line"></span><br><span class="line">    def convert_tokens_to_ids(self, tokens):</span><br><span class="line">        return convert_by_vocab(self.vocab, tokens)</span><br><span class="line"></span><br><span class="line">    def convert_ids_to_tokens(self, ids):</span><br><span class="line">        return convert_by_vocab(self.inv_vocab, ids)</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<p>在<code>__init__</code>中可以看到，又得先分析<code>BasicTokenizer</code>类和<code>WordpieceTokenizer</code>类(哎呀真烦，最后在回来做超链接吧),除此之外就是调用了几个小函数,<code>load_vocab</code>它的输入参数是bert模型的词典,返回的是一个<code>OrdereDict</code>:{词:词号}.其他的不说了，没啥意思。</p>
</blockquote>
</blockquote>
<h3 id="class-BasicTokenizer"><a href="#class-BasicTokenizer" class="headerlink" title="class BasicTokenizer"></a>class BasicTokenizer</h3><blockquote>
<blockquote>
<p>目的是根据空格，标点进行普通的分词，最后返回的是关于词的列表，对于中文而言是关于字的列表。</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">class BasicTokenizer(object):</span><br><span class="line">  def __init__(self, do_lower_case=True):</span><br><span class="line">    self.do_lower_case = do_lower_case</span><br><span class="line"></span><br><span class="line">  def tokenize(self, text):</span><br><span class="line">  ##其实就是把字符串转为了list，分英文单词和中文单词处理</span><br><span class="line">  ##eg:Mr. Cassius crossed the highway, and stopped suddenly.转为[&apos;mr&apos;, &apos;.&apos;, &apos;cassius&apos;, &apos;crossed&apos;, &apos;the&apos;, &apos;highway&apos;, &apos;,&apos;, &apos;and&apos;, &apos;stopped&apos;, &apos;suddenly&apos;, &apos;.&apos;]</span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    text = self._clean_text(text)</span><br><span class="line">    text = self._tokenize_chinese_chars(text)</span><br><span class="line">    orig_tokens = whitespace_tokenize(text)#无需细说，就是把string按照空格切分为list</span><br><span class="line">    split_tokens = []</span><br><span class="line">    for token in orig_tokens:</span><br><span class="line">      if self.do_lower_case:</span><br><span class="line">        token = token.lower()</span><br><span class="line">        token = self._run_strip_accents(token)#这个函数干了什么我也没看明白,但是对正题流程不重要,略过吧</span><br><span class="line">      split_tokens.extend(self._run_split_on_punc(token))</span><br><span class="line">    output_tokens = whitespace_tokenize(&quot; &quot;.join(split_tokens))</span><br><span class="line">    return output_tokens</span><br><span class="line"></span><br><span class="line">  def _run_strip_accents(self, text):</span><br><span class="line">    text = unicodedata.normalize(&quot;NFD&quot;, text)</span><br><span class="line">    output = []</span><br><span class="line">    for char in text:</span><br><span class="line">      cat = unicodedata.category(char)</span><br><span class="line">      if cat == &quot;Mn&quot;:</span><br><span class="line">        continue</span><br><span class="line">      output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br><span class="line"></span><br><span class="line">  def _run_split_on_punc(self, text):</span><br><span class="line">    chars = list(text)</span><br><span class="line">    i = 0</span><br><span class="line">    start_new_word = True</span><br><span class="line">    output = []</span><br><span class="line">    while i &lt; len(chars):</span><br><span class="line">      char = chars[i]</span><br><span class="line">      if _is_punctuation(char):</span><br><span class="line">        output.append([char])</span><br><span class="line">        start_new_word = True</span><br><span class="line">      else:</span><br><span class="line">        if start_new_word:</span><br><span class="line">          output.append([])</span><br><span class="line">        start_new_word = False</span><br><span class="line">        output[-1].append(char)</span><br><span class="line">      i += 1</span><br><span class="line">    return [&quot;&quot;.join(x) for x in output]</span><br><span class="line"></span><br><span class="line">  def _tokenize_chinese_chars(self, text):</span><br><span class="line">    # 按字切分中文，其实就是英文单词不变,中文在字两侧添加空格</span><br><span class="line">    output = []</span><br><span class="line">    for char in text:</span><br><span class="line">      cp = ord(char)</span><br><span class="line">      if self._is_chinese_char(cp):</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">        output.append(char)</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">      else:</span><br><span class="line">        output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br><span class="line"></span><br><span class="line">  def _is_chinese_char(self, cp):</span><br><span class="line">    # 判断是否是汉字,这个函数很有意义，值得借鉴</span><br><span class="line">    # refer：https://www.cnblogs.com/straybirds/p/6392306.html</span><br><span class="line">    if ((cp &gt;= 0x4E00 and cp &lt;= 0x9FFF) or  #</span><br><span class="line">        (cp &gt;= 0x3400 and cp &lt;= 0x4DBF) or  #</span><br><span class="line">        (cp &gt;= 0x20000 and cp &lt;= 0x2A6DF) or  #</span><br><span class="line">        (cp &gt;= 0x2A700 and cp &lt;= 0x2B73F) or  #</span><br><span class="line">        (cp &gt;= 0x2B740 and cp &lt;= 0x2B81F) or  #</span><br><span class="line">        (cp &gt;= 0x2B820 and cp &lt;= 0x2CEAF) or</span><br><span class="line">        (cp &gt;= 0xF900 and cp &lt;= 0xFAFF) or  #</span><br><span class="line">        (cp &gt;= 0x2F800 and cp &lt;= 0x2FA1F)):  #</span><br><span class="line">      return True</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">  def _clean_text(self, text): # 去除无意义字符以及空格</span><br><span class="line">    output = []</span><br><span class="line">    for char in text:</span><br><span class="line">      cp = ord(char)</span><br><span class="line">      if cp == 0 or cp == 0xfffd or _is_control(char):</span><br><span class="line">        continue</span><br><span class="line">      if _is_whitespace(char):</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">      else:</span><br><span class="line">        output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br></pre></td></tr></table></figure>
<h3 id="class-WordpieceTokenizer"><a href="#class-WordpieceTokenizer" class="headerlink" title="class WordpieceTokenizer"></a>class WordpieceTokenizer</h3><blockquote>
<blockquote>
<p>这个才是重点,跑test的时候出现的那些##都是从这里拿来的，其实就是把未登录词在词表中匹配相应的前缀.</p>
</blockquote>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">class WordpieceTokenizer(object):</span><br><span class="line">  def __init__(self, vocab, unk_token=&quot;[UNK]&quot;, max_input_chars_per_word=200):</span><br><span class="line">    self.vocab = vocab</span><br><span class="line">    self.unk_token = unk_token</span><br><span class="line">    self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">  def tokenize(self, text):</span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    output_tokens = []</span><br><span class="line">    for token in whitespace_tokenize(text):</span><br><span class="line">      chars = list(token)</span><br><span class="line">      if len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">        continue</span><br><span class="line">      is_bad = False</span><br><span class="line">      start = 0</span><br><span class="line">      sub_tokens = []</span><br><span class="line">      while start &lt; len(chars):</span><br><span class="line">        end = len(chars)</span><br><span class="line">        cur_substr = None</span><br><span class="line">        while start &lt; end:</span><br><span class="line">          substr = &quot;&quot;.join(chars[start:end])</span><br><span class="line">          if start &gt; 0:</span><br><span class="line">            substr = &quot;##&quot; + substr</span><br><span class="line">          if substr in self.vocab:</span><br><span class="line">            cur_substr = substr</span><br><span class="line">            break</span><br><span class="line">          end -= 1</span><br><span class="line">        if cur_substr is None:</span><br><span class="line">          is_bad = True</span><br><span class="line">          break</span><br><span class="line">        sub_tokens.append(cur_substr)</span><br><span class="line">        start = end</span><br><span class="line">      if is_bad:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">      else:</span><br><span class="line">        output_tokens.extend(sub_tokens)</span><br><span class="line">    return output_tokens</span><br></pre></td></tr></table></figure>
<blockquote>
<blockquote>
<p>tokenize说明: 使用贪心的最大正向匹配算法<br>  eg:input = “unaffable” output = [“un”, “##aff”, “##able”],首先看”unaffable”在不在词表中，在的话就当做一个词，也就是WordPiece，不在的话在看”unaffabl”在不在，也就是<code>while</code>中的<code>end-=1</code>,最终发现”un”在词表中,算是一个WordPiece,然后start=2,也就是代码中的<code>start=end</code>,看”##affable”在不在词表中,在看”##affabl”(##表示接着前面)，最终返回[“un”, “##aff”, “##able”].注意，这样切分是可逆的，也就是可以根据词表重载”攒回”原词，以此便解决了oov问题.</p>
</blockquote>
</blockquote>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2021-09-19T18:01:25.943Z" itemprop="dateUpdated">2021-09-20 02:01:25</time>
</span><br>


        
        转载请标注:<a href="/2019/07/22/Bert/" target="_blank" rel="external">https://lingyixia.github.io/2019/07/22/Bert/</a>
        
    </div>
    
    <footer>
        <a href="https://lingyixia.github.io">
            <img src="/img/avatar.png" alt="陈飞宇">
            陈飞宇
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/论文/">论文</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://lingyixia.github.io/2019/07/22/Bert/&title=《Bert》 — 灵翼俠的个人博客&pic=https://lingyixia.github.io/img/avatar.png" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://lingyixia.github.io/2019/07/22/Bert/&title=《Bert》 — 灵翼俠的个人博客&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://lingyixia.github.io/2019/07/22/Bert/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Bert》 — 灵翼俠的个人博客&url=https://lingyixia.github.io/2019/07/22/Bert/&via=https://lingyixia.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://lingyixia.github.io/2019/07/22/Bert/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/07/27/greedy/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">贪心</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/07/16/dataAugmentation/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">dataAugmentation</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'true' == 'true',
            verify: 'false' == 'true',
            appId: "Jh0zRO9WoRQ3PcY9WjRbmeXT-gzGzoHsz",
            appKey: "tOx3IvcUkfWzD24UccR4A41Y",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->







</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢~~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="bottom">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            <span>
            PLM's Notes &nbsp; &copy; &nbsp
            </span>
            2016 - 2021
        </p>
    </div>
</footer>
    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://lingyixia.github.io/2019/07/22/Bert/&title=《Bert》 — 灵翼俠的个人博客&pic=https://lingyixia.github.io/img/avatar.png" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://lingyixia.github.io/2019/07/22/Bert/&title=《Bert》 — 灵翼俠的个人博客&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://lingyixia.github.io/2019/07/22/Bert/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Bert》 — 灵翼俠的个人博客&url=https://lingyixia.github.io/2019/07/22/Bert/&via=https://lingyixia.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://lingyixia.github.io/2019/07/22/Bert/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACKElEQVR42u3aQW7DMAwEwP7/0+61l8hLMi5gaXQqktTStABDkfz5idf1Z01eX38m+a3RwsDAeC3jWq7kuOsnr3f59G5+NgwMjHMYySEmB0qes9735swYGBgYy+C7TuluAmW8LwYGBsY84K5DZzUNxcDAwJg8uleSq8IevItjYGC8kJFX3f//50f6GxgYGK9iXMU1v9xWr7vRqTAwMLZmzNO+6rUzTxPL58HAwNiaUR3Vqgbf+eFGySIGBsYWjOp4VjVcVsttvTYqBgbG3oxek7IalJPg+7VvDAwMjO0Yk7JauSgWh87CmAUGBsbWjGSz0XBD/GeqXqQxMDDOYeQJWR40e03QvBj38RUMDIxjGHmCmBfdqgMWzRCMgYGxNaNaiO81I3tthvKYBQYGxnaM5OKatwSqAxZ5GG3mthgYGFswkuSv1zzoheDq0MZNhouBgbEdoxc6n0gf8yYEBgbGCYy8fJ9sPL8A52E3+j9gYGBswUjeTspkyRW39/lqMQ4DA2NXRq80Vu45xEE8f8LHbw8MDIwDGNUBiDyN+1bD4CbPxcDA2JRRHbaYpHTJhbkc1jEwMDZlXMX1xBF7JTwMDIxzGPmaB8pk5KL3BYCBgXECIwmy1QDaG7+YtDwxMDBOYBRK8HEg7rVFm0gMDAyMVhlu3qostDwxMDAwitfOPLmslvYeCbgYGBgvYVSblL2S2bcKcxgYGKcxqhvkrcc8yE4SUwwMjK0ZvzGFGQawjzkpAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '哎哎哎，网呢？？？';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>