<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    

    

    



    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    
    
    
    <title>双层自注意力Transformer | 灵翼俠的个人博客 | 不做搬运工</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content>
    <meta name="description" content="立题依据研究意义Transformer[1]从2017年被提出至今，一直是NLP领域重点研究对象，各种SOTA模型的诞生几乎都离不开它的加持，如今甚至在NLP领域已经成为标配，有完全取代CNN和RNN的趋势，在CV领域也有席卷的迹象。尽管Transformer[1]取得这么大成功，但是它有一个致命缺陷，其时空复杂度均为$O(n^2)$,其中$n$代表序列长度,这也是限制其在CV领域发展的一个重要原">
<meta property="og:type" content="article">
<meta property="og:title" content="双层自注意力Transformer">
<meta property="og:url" content="https://lingyixia.github.io/2021/11/07/shuffleLM/index.html">
<meta property="og:site_name" content="灵翼俠的个人博客">
<meta property="og:description" content="立题依据研究意义Transformer[1]从2017年被提出至今，一直是NLP领域重点研究对象，各种SOTA模型的诞生几乎都离不开它的加持，如今甚至在NLP领域已经成为标配，有完全取代CNN和RNN的趋势，在CV领域也有席卷的迹象。尽管Transformer[1]取得这么大成功，但是它有一个致命缺陷，其时空复杂度均为$O(n^2)$,其中$n$代表序列长度,这也是限制其在CV领域发展的一个重要原">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://lingyixia.github.io/img/double_layer_attention.png">
<meta property="og:image" content="https://lingyixia.github.io/img/stack_multi_attention.png">
<meta property="og:image" content="https://lingyixia.github.io/img/pyramid_attention.png">
<meta property="og:updated_time" content="2021-11-16T15:49:26.695Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="双层自注意力Transformer">
<meta name="twitter:description" content="立题依据研究意义Transformer[1]从2017年被提出至今，一直是NLP领域重点研究对象，各种SOTA模型的诞生几乎都离不开它的加持，如今甚至在NLP领域已经成为标配，有完全取代CNN和RNN的趋势，在CV领域也有席卷的迹象。尽管Transformer[1]取得这么大成功，但是它有一个致命缺陷，其时空复杂度均为$O(n^2)$,其中$n$代表序列长度,这也是限制其在CV领域发展的一个重要原">
<meta name="twitter:image" content="https://lingyixia.github.io/img/double_layer_attention.png">
    
        <link rel="alternate" type="application/atom+xml" title="灵翼俠的个人博客" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/Favicon.png">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</head>

<script type="text/javascript" src="/js/clicklove.js"></script>
<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand_background.jpeg)">
      <div class="brand">
        <a href="/about/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.png">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">陈飞宇</h5>
          <a href="mailto:chinachenfeiyu@outlook.com" title="chinachenfeiyu@outlook.com" class="mail">chinachenfeiyu@outlook.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/books"  >
                <i class="icon icon-lg icon-book"></i>
                读书
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/movies"  >
                <i class="icon icon-lg icon-film"></i>
                影视
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/games"  >
                <i class="icon icon-lg icon-gamepad"></i>
                游戏
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/meditations"  >
                <i class="icon icon-lg icon-leaf"></i>
                随笔
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/links"  >
                <i class="icon icon-lg icon-link"></i>
                友情链接
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/lingyixia" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">双层自注意力Transformer</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale" align="center">
        <h1 class="title">双层自注意力Transformer</h1>
        <h5 class="subtitle">
            
                <time datetime="2021-11-07T15:44:44.000Z" itemprop="datePublished" class="page-time">
  2021-11-07
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#立题依据"><span class="post-toc-number">1.</span> <span class="post-toc-text">立题依据</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#研究意义"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">研究意义</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#国内外研究现状"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">国内外研究现状</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#研究方案"><span class="post-toc-number">2.</span> <span class="post-toc-text">研究方案</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#研究目标、研究内容和关键问题"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">研究目标、研究内容和关键问题</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#研究目标"><span class="post-toc-number">2.1.1.</span> <span class="post-toc-text">研究目标</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#研究内容"><span class="post-toc-number">2.1.2.</span> <span class="post-toc-text">研究内容</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#关键问题"><span class="post-toc-number">2.1.3.</span> <span class="post-toc-text">关键问题</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#可行性分析"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">可行性分析</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#命题"><span class="post-toc-number">2.2.1.</span> <span class="post-toc-text">命题</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#解题"><span class="post-toc-number">2.2.2.</span> <span class="post-toc-text">解题</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#创新点"><span class="post-toc-number">3.</span> <span class="post-toc-text">创新点</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#研究计划"><span class="post-toc-number">4.</span> <span class="post-toc-text">研究计划</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#参考文献"><span class="post-toc-number">5.</span> <span class="post-toc-text">参考文献</span></a></li></ol>
        </nav>
    </aside>


<article id="post-shuffleLM"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">双层自注意力Transformer</h1>
        <div class="post-meta">
            <time class="post-time" title="2021-11-07 23:44:44" datetime="2021-11-07T15:44:44.000Z"  itemprop="datePublished">2021-11-07</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="立题依据"><a href="#立题依据" class="headerlink" title="立题依据"></a>立题依据</h1><h2 id="研究意义"><a href="#研究意义" class="headerlink" title="研究意义"></a>研究意义</h2><p>Transformer<a href="#refer-anchor"><sup>[1]</sup></a>从2017年被提出至今，一直是NLP领域重点研究对象，各种SOTA模型的诞生几乎都离不开它的加持，如今甚至在NLP领域已经成为标配，有完全取代CNN和RNN的趋势，在CV领域也有席卷的迹象。尽管Transformer<a href="#refer-anchor"><sup>[1]</sup></a>取得这么大成功，但是它有一个致命缺陷，其时空复杂度均为$O(n^2)$,其中$n$代表序列长度,这也是限制其在CV领域发展的一个重要原因，正因如此，不少工作被迫妥协，经典模型Bert<a href="#refer-anchor"><sup>[2]</sup></a>最长长度只有512，对于短文本而言还可以接受，但对长文本例如一篇文章一般几千个token，这种长度的序列直接训练会导致显存不足因此只能被迫截断，这样必然损失精度，因此研究如何降低复杂度至关重要。</p>
<h2 id="国内外研究现状"><a href="#国内外研究现状" class="headerlink" title="国内外研究现状"></a>国内外研究现状</h2><p>针对Transformer<a href="#refer-anchor"><sup>[1]</sup></a>时空复杂度过大的问题，业内已经有研究人员提出提出一些解决方案，Qipeng Guo等人提出Star-Transformer<a href="#refer-anchor"><sup>[3]</sup></a>，将密集Attention结构改为星形结构，基本思想是将计算两两token之间注意力分数改为计算各个token和中心token的注意力分数，从而降低复杂度。Rewon Child等人提出Sparse Transformer<a href="#refer-anchor"><sup>[4]</sup></a>，基本思路是对于某个token而言，只需要计算对其贡献最大的若干个注意力分数即可，通过将Transformer模型中的Attention矩阵稀疏化来达到减少内存消耗、降低计算力的方法，基本做法是在Softmax阶段只保留topK个结果，将计算复杂度从$O(n^2)$降低到$O(n^\frac{2}{3})$.Zihao Ye等人提出BP-Transformer<a href="#refer-anchor"><sup>[5]</sup></a>，采用分治法利用多层二叉树，逐步计算Attention最终归并，将计算复杂度降低到$O(n\log n)$。 Iz Beltagy等人提出Longformer<a href="#refer-anchor"><sup>[6]</sup></a>，对于每一个token只对固定窗口大小附近的token进行Local Attention，同时附近token采用了类似空洞卷积的方式增强用来捕获更长的序列长度的信息，针对特定任务的特定token做Global Attention，但值得注意的是，在原始代码中所谓的Local Attention窗口大小也达到了512,即内存量和Bert<a href="#refer-anchor"><sup>[2]</sup></a>相比并没有减少。Zihang Dai等人提出Transformer-XL<a href="#refer-anchor"><sup>[7]</sup></a>,将长文档切块，从第一个块开始做Transformer，然后以模仿RNN的方式传递到下一个块递归的训练，该方案被用在很多其他工作中，例如Xlnet<a href="#refer-anchor"><sup>[8]</sup></a>，同时XlNet<a href="#refer-anchor"><sup>[8]</sup></a>也使用了一种全排列的乱序方式，但是该乱序中加入了位置信息，不能算真正乱序。Sainbayar Sukhbaatar等人提出自适应跨度 Transformer<a href="#refer-anchor"><sup>[9]</sup></a>，主要思想是抛弃了区域Attention固定窗口的做法，使用一种能自适应选择长下文窗口大小的方案。Nikita Kitaev等人提出Reformer<a href="#refer-anchor"><sup>[10]</sup></a>，选择通过局部敏感哈希技术将每个词例的注意力范围变窄，Aurko Roy 等人提出Routing Transformer<a href="#refer-anchor"><sup>[11]</sup></a>尝试将问题建模为了一个路由问题，从而让模型学会选择词例的稀疏聚类。Siyu Ding等人在ERNIE-DOC<a href="#refer-anchor"><sup>[12]</sup></a>中提出回溯式feed机制和增强的循环机制，其基本原理是对于一篇长文，采用先略读后精读的方式，其中回溯式feed机制参考了xlnet<a href="#refer-anchor"><sup>[8]</sup></a>通过粗细两种粒度从乱序恢复正常语序，增强的循环机制借鉴了Transformer-xl<a href="#refer-anchor"><sup>[7]</sup></a>，但是将上一时刻上一层的输出改为上一时刻下一层的输出，相当于扩大感受野。</p>
<h1 id="研究方案"><a href="#研究方案" class="headerlink" title="研究方案"></a>研究方案</h1><h2 id="研究目标、研究内容和关键问题"><a href="#研究目标、研究内容和关键问题" class="headerlink" title="研究目标、研究内容和关键问题"></a>研究目标、研究内容和关键问题</h2><h3 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h3><p>本论文的研究目标是提出并实现一种双层Self-Attention结构的Transformer，用于降低普通Transformer时空复杂度。</p>
<h3 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h3><p>普通的Transformer模型时间和空间复杂度都是$O(n^2)$,需要注意的是这里的复杂度指的是乘/除法运算的复杂度,证明如下：</p>
<script type="math/tex; mode=display">
Attention(Q_{n\times d_k},K_{n\times d_k},V_{n\times d_k})=\frac{softmax(Q_{n\times d_k}\times K_{n\times d_k}^T)}{\sqrt{d_k}} \times V_{n\times d_k}</script><p>第一步: $Q_{n\times d_k}\times K_{n\times d_k}^T$乘法计算量为$d_k\times n^2$.</p>
<p>第二步:$softmax(Q_{n\times d_k}\times K_{n\times d_k}^T)$乘法计算量为$n^2$.</p>
<p>第三步:$\frac{softmax(Q_{n\times d_k}\times K_{n\times d_k}^T)}{\sqrt{d_k}}$除法计算量为$d_k\times n^2$.</p>
<p>第四步:$\frac{softmax(Q_{n\times d_k}\times K_{n\times d_k}^T)}{\sqrt{d_k}} \times V_{n\times d_k}$乘法计算量为$d_k\times n^2$.</p>
<p>综上所述，去除常数$d_k$，可得复杂度为$O(n^2)$,如此大的复杂度在面对长文本时很难处理，时间上勉强可以接受，但是空间上很难解决，通常只能通过限制文本长度或减小batchsize来解决，此方式过于暴力，无疑会增加降低模型效果，本论文拟尝试一种双层self-Attention的Transformer模型，其基本结构如下：</p>
<div align="center">
<img src="/img/double_layer_attention.png">
<center>双层self-Attention</center>
</div>

<p>基本思想是对长序列进行分块，先通过块内计算Attention Score，然后通过块间计算全局Attention Score。图中$token_{ij}$中$i$表示第$i$个块,$j$表示块中第$j$个token，$SA_{ij}$表示第$i$层第$j$个块的$SA$。</p>
<h3 id="关键问题"><a href="#关键问题" class="headerlink" title="关键问题"></a>关键问题</h3><ol>
<li>如何设置各个分块大小？<br>常见方式是固定窗口大小，本论文拟尝试的方式如下：<br>固定分区后分块数量$m$，则平均每个块长度为$\frac{n}{m}$,然后每个块实际大小设置为$r_i=random[1,\frac{2n}{m}]$，且满足$\sum_ir_i=n$。之所以希望窗口大小随机而不是固定主要是考虑希望窗口在序列的位置信息不是固定位置而是一定范围的位置。</li>
<li>按照标准Transformer，backbone需要串联起来，但是上诉方式，第一层backbone之后序列长度就和之前不一样了，无法直接衔接到相同形状的下一层，在BP-Transformer中解决该问题的方案是用根节点去做类似分类任务，用叶子结点去做生成任务，但是叶子是处于第一层，理论上越深层语义信息越丰富，经典论文Transformer中做翻译任务用的就是Encoder的最后一层。本论文中该问题拟尝试的解决方案有两个:</li>
</ol>
<ul>
<li><p>方案一：基本思路是为使每一个backbone输入输出相同，可尝试将两层Attention结合，即第一层SA用于捕获块内注意力，第二层SA用于捕获全局注意力，然后两者拼接即可恢复输入形状，进而可以随意串联，同时每一个backbone会重新分块。该方式可以用于任何任务。</p>
<div align="center">
<img src="/img/stack_multi_attention.png">
<center>堆叠self-Attention</center>
</div>
</li>
<li><p>方案二：基本思路是类似多层cnn堆叠的方式，呈现金字塔形状，同时并行训练多个不同分块方式的相同模型，最后一层concat在一起，该方式可用于多对一的任务中。</p>
<div align="center">
<img src="/img/pyramid_attention.png">
<center>金字塔self-Attention</center>
</div>
</li>
</ul>
<ol>
<li>SA数量的增多也意味着FNN数量的增多，可能会增加参数：<br>可以尝试同层同形SA之间参数共享。</li>
<li>块内位置信息是否必要？<br>在Nlp领域无论什么任务都比较强调语序，因此一般都会加上位置信息，看起来很正常，但一定要如此么？有个传播已久例子：“研表究明，汉字的序顺并不定一能影阅响读，比如当你看完这句话后，才发这现里的字全是乱的。”但是如果我们把这句话改为：”影研明字，的序究顺并表不当一汉阅响，比读现你看完发这定话后，才这里能的字全是如句乱的。”正常情况下很难一眼理解句意，看起来有趣的同时也蕴含着一定的道理，语序确实很重要，但或许不是那么重要？<br>出于这种思考，本论文拟尝试一种区域内乱序的方案，意思是分块后块之间严格语序，但是块内不严格语序。基本结构可以参考双层self-Attention图，并将第一层SA改为reduce_sum/reduce_mean即可实现块内乱序。</li>
</ol>
<h2 id="可行性分析"><a href="#可行性分析" class="headerlink" title="可行性分析"></a>可行性分析</h2><p>本小节旨在证明该方法是否能够降低复杂度，以及最优复杂度是多少。</p>
<h3 id="命题"><a href="#命题" class="headerlink" title="命题"></a>命题</h3><p>假设一段文字长度为$n$，则其在self-Attention部分计算量为$n^2$，我们把它切成$m$段，为方便计算假设每段长度相同,即$\frac{n}{m}$，求此时双层self-Attention的计算量最低是多少。</p>
<h3 id="解题"><a href="#解题" class="headerlink" title="解题"></a>解题</h3><p>根据上图可得则此时的计算量为:$\frac{n^2}{m^2} \times m + m^2=\frac{n^2}{m}+m^2$(块内Attention和块间Attention相加)，令：</p>
<script type="math/tex; mode=display">
f(m)=\frac{n^2}{m}+m^2 \quad (1<m<n)</script><p>求导：</p>
<script type="math/tex; mode=display">
f'(m)=\frac{2m^3-n^2}{m^2}\quad (1<m<n)</script><p>令$f’(m)=0$,求得$m=(\frac{n^2}{2})^\frac{1}{3}$,$f(m)$在$(1,(\frac{n^2}{2})^\frac{1}{3}]$为减函数，$[(\frac{n^2}{2})^\frac{1}{3},n)$为增函数，因此$m=(\frac{n^2}{2})^\frac{1}{3}$时可求得$f(m)$最小值$n^{\frac{4}{3}}\times(2^{\frac{1}{3}}+2^{-\frac{2}{3}})=\frac{3n^\frac{4}{3}}{2^\frac{2}{3}}$,复杂度为$O(n^{4/3})$。<br>和Bert对比，假设n=512,则原始计算量为262144，而双层self-Attention当m=50时，计算量最优值仅为7740,远远小于262144。理论上如果和普通Bert相同的计算量，双层self-Attention可处理序列长度为7500，远大于512。<br>虽然从复杂度上看，$O(n^{4/3})$和$O(n^2)$差不了很多，但是从计算量角度还是有一定的效果，当然理论上self-Attention层越多越能减轻计算负担。</p>
<h1 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h1><p>本文尝试提出一种利用双层self-Attention降低Transformer复杂度的方案，利用块内自注意力和块间自注意力的结合，将普通Transformer自注意力机制的复杂度从$O(n^2)$降低到$O(n^\frac{4}{3})$。</p>
<h1 id="研究计划"><a href="#研究计划" class="headerlink" title="研究计划"></a>研究计划</h1><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><div id="refer-anchor"></div>

<p>[1] <a href="http://papers.nips.cc/paper/7181-Attention-is-all-you-%0Aneed.pdf" target="_blank" rel="noopener">Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems. 2017.</a><br>[2] <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional Transformers for language understanding.” arXiv preprint arXiv:1810.04805 (2018).</a><br>[3] <a href="https://arxiv.org/abs/1902.09113" target="_blank" rel="noopener">Guo, Qipeng, et al. “Star-Transformer.” arXiv preprint arXiv:1902.09113 (2019).</a><br>[4] <a href="https://openreview.net/forum?id=Hye87grYDH" target="_blank" rel="noopener">Zhao, Guangxiang, et al. “Sparse Transformer: Concentrated Attention Through Explicit Selection.” (2019).</a><br>[5] <a href="https://arxiv.org/abs/1911.04070" target="_blank" rel="noopener">Ye, Zihao, et al. “Bp-Transformer: Modelling long-range context via binary partitioning.” arXiv preprint arXiv:1911.04070 (2019).</a><br>[6] <a href="https://arxiv.org/abs/2004.05150" target="_blank" rel="noopener">Beltagy, Iz, Matthew E. Peters, and Arman Cohan. “Longformer: The long-document Transformer.” arXiv preprint arXiv:2004.05150 (2020).</a><br>[7] <a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener">Dai, Zihang, et al. “Transformer-xl: Attentive language models beyond a fixed-length context.” arXiv preprint arXiv:1901.02860 (2019).</a><br>[8] <a href="http://papers.nips.cc/paper/8812-xlnet-generalizedautoregressive-pretraining-for-language-understanding" target="_blank" rel="noopener">Yang, Zhilin, et al. “Xlnet: Generalized autoregressive pretraining for language understanding.” Advances in neural information processing systems 32 (2019).</a><br>[9] <a href="https://arxiv.org/abs/1905.07799" target="_blank" rel="noopener">Sukhbaatar, Sainbayar, et al. “Adaptive Attention span in Transformers.” arXiv preprint arXiv:1905.07799 (2019).</a><br>[10] <a href="https://arxiv.org/abs/2001.04451" target="_blank" rel="noopener">Kitaev, Nikita, Łukasz Kaiser, and Anselm Levskaya. “Reformer: The efficient Transformer.” arXiv preprint arXiv:2001.04451 (2020).</a><br>[11] <a href="https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00353/97776" target="_blank" rel="noopener">Roy, Aurko, et al. “Efficient content-based sparse Attention with routing Transformers.” Transactions of the Association for Computational Linguistics 9 (2021): 53-68.</a><br>[12] <a href="https://arxiv.org/abs/2012.15688" target="_blank" rel="noopener">Ding, Siyu, et al. “ERNIE-DOC: The Retrospective Long-Document Modeling Transformer.” arXiv preprint arXiv:2012.15688 (2020).</a></p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2021-11-16T15:49:26.695Z" itemprop="dateUpdated">2021-11-16 23:49:26</time>
</span><br>


        
        转载请标注:<a href="/2021/11/07/shuffleLM/" target="_blank" rel="external">https://lingyixia.github.io/2021/11/07/shuffleLM/</a>
        
    </div>
    
    <footer>
        <a href="https://lingyixia.github.io">
            <img src="/img/avatar.png" alt="陈飞宇">
            陈飞宇
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            

            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://lingyixia.github.io/2021/11/07/shuffleLM/&title=《双层自注意力Transformer》 — 灵翼俠的个人博客&pic=https://lingyixia.github.io/img/avatar.png" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://lingyixia.github.io/2021/11/07/shuffleLM/&title=《双层自注意力Transformer》 — 灵翼俠的个人博客&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://lingyixia.github.io/2021/11/07/shuffleLM/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《双层自注意力Transformer》 — 灵翼俠的个人博客&url=https://lingyixia.github.io/2021/11/07/shuffleLM/&via=https://lingyixia.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://lingyixia.github.io/2021/11/07/shuffleLM/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2021/10/31/bert-flow-whiten/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">bert-flow&amp;&amp;whiten</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'true' == 'true',
            verify: 'false' == 'true',
            appId: "Jh0zRO9WoRQ3PcY9WjRbmeXT-gzGzoHsz",
            appKey: "tOx3IvcUkfWzD24UccR4A41Y",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->







</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢~~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="bottom">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            <span>
            PLM's Notes &nbsp; &copy; &nbsp
            </span>
            2016 - 2021
        </p>
    </div>
</footer>
    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://lingyixia.github.io/2021/11/07/shuffleLM/&title=《双层自注意力Transformer》 — 灵翼俠的个人博客&pic=https://lingyixia.github.io/img/avatar.png" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://lingyixia.github.io/2021/11/07/shuffleLM/&title=《双层自注意力Transformer》 — 灵翼俠的个人博客&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://lingyixia.github.io/2021/11/07/shuffleLM/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《双层自注意力Transformer》 — 灵翼俠的个人博客&url=https://lingyixia.github.io/2021/11/07/shuffleLM/&via=https://lingyixia.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://lingyixia.github.io/2021/11/07/shuffleLM/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACLUlEQVR42u3aQW7DMAwEwPz/0ynQU4vCya6kFJA8OgWOnWh8ICiSj0e8nt/r6vrPb/9eubr/6qmrZxcsDAyMbRnPl+svI/mbZFsJI9kbBgbGfRjJ3yRPvd5ifn++NwwMDIw8BWw/J1cwMDAwxgJuksblqWeekmJgYGAkP5dvMT/itofbBWdxDAyMDRltY+A/P3+wv4GBgbEJYyZpmymlLU4iMTAwjmbkWxlL3fKCXVvOq/NNDAyMbRljYxNJ0Fw7WvHmhWJgYBzNmGlP5ox8c/mIxpuqIQYGxkGMVaMPYyMaLa9IDTEwMA5itMfasfQuuT4WsgdP5xgYGBsy2iJXsqGxg+hM8Q4DA+NsRhtw82RxBtPuBAMD41TGWFtxZit5Ulh8i4GBcQNGEnDzZsCql5I3UzEwMM5m5EfTtsH5KFd+MI5GxDAwMA5ltI+1wXdVW7R4KxgYGMcx8jA330jI25zFdBsGBsahjNza9hnmy2pFsoiBgXE0ox69KhO+diqtBWBgYNyHsaC81Zbvp1PSqR/CwMDYljEWcPMtjjVKi9/HwMA4mpGHwvzKsjD6iV4HBgbGVoxnudrjbhtk29fxq2qIgYFxKGMmzLVH33aMo70TAwPjbEYeZPPyfX58TVqYSeDGwMC4A6Mt2bdJWz6oMYbHwMDAaLPLPFjnCeIbNgYGBsZQMpdvOn9xGBgYd2asOpqOwRbUCDEwMI5mjA1etNMcq0a+2qYpBgbG5owvRMFBzyw/rSYAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '哎哎哎，网呢？？？';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body>
</html>