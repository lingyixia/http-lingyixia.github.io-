{"meta":{"title":"灵翼俠的个人博客","subtitle":"不做搬运工","description":null,"author":"陈飞宇","url":"https://lingyixia.github.io"},"pages":[{"title":"前路漫漫，当克己，当慎独。","date":"2021-09-19T18:01:25.967Z","updated":"2021-09-19T18:01:25.967Z","comments":true,"path":"about/index.html","permalink":"https://lingyixia.github.io/about/index.html","excerpt":"","text":"@timeline{ 2020@item{ 2020年7月10日入职字节跳动 } } @timeline{ 2019@item{ 2019年9月23日百度再见 } @item{ 2019年4月28日第二次实习————百度 } @item{ 2019年月1月5日秦皇岛，再见 燕大，再见。 } @item{ 2017年12月1日第一次实习————新意互动 } @item{ 2017年9月3日北师大，你好。 } @item{ 2017年7月6日本科毕业 } @item{ 2017年3月1日毕业设计 } @item{ 2016年8月6日华北五省计算机应用大赛 } @item{ 2013年9月3日秦皇岛,你好 燕大,你好。 } } 小菜一枚,欢迎来喷"},{"title":"","date":"2021-09-19T18:01:25.967Z","updated":"2021-09-19T18:01:25.967Z","comments":true,"path":"categories/index.html","permalink":"https://lingyixia.github.io/categories/index.html","excerpt":"","text":"title: categories date: 2018-12-20 15:35:06"},{"title":"","date":"2021-09-19T18:01:25.967Z","updated":"2021-09-19T18:01:25.967Z","comments":false,"path":"links/index.html","permalink":"https://lingyixia.github.io/links/index.html","excerpt":"","text":"title: 链接 date: 2019-07-10 10:33:37"},{"title":"meditation","date":"2019-07-11T12:21:41.000Z","updated":"2021-09-19T18:01:25.967Z","comments":true,"path":"meditations/index.html","permalink":"https://lingyixia.github.io/meditations/index.html","excerpt":"","text":"@card{ } @column-2{ @card{ 腹肌锻炼初见效,继续keep!!!!} @card{ 我可怜的脸啊，什么时候才能恢复!!!} } @card{ 如果我就这样死了，就证明我只不过是这种程度的男人 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;—索大} @card{ 别总数着失去的东西，失去就是失去了，想想你还剩下些什么。&emsp;&emsp;&emsp;&emsp;—甚平} @card{ &emsp;&emsp;&emsp;&emsp;听着，路飞，胜利和败北都要品尝，经历了四处逃窜的辛酸、痛苦 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;伤心的回忆，男人才能独当一面，即使痛苦流涕也没关系！&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;—香克斯} @column-3{ @card{ 这特么代码看的我想哭，怎么能这样!!!!!} @card{ 机会稍纵即逝，哪有那么多时间细想？} @card{ 不要在事情发生后才意识到严重性,想想多少次事中没感觉怎样，事后才发现当时有多严重。} }"},{"title":"","date":"2021-09-19T18:01:25.968Z","updated":"2021-09-19T18:01:25.968Z","comments":true,"path":"tags/index.html","permalink":"https://lingyixia.github.io/tags/index.html","excerpt":"","text":"title: tags date: 2018-12-20 15:35:03"}],"posts":[{"title":"神经网络生成模型学习总结","slug":"NetWorkGenerator","date":"2021-10-20T15:59:19.000Z","updated":"2021-10-23T11:15:41.811Z","comments":true,"path":"2021/10/20/NetWorkGenerator/","link":"","permalink":"https://lingyixia.github.io/2021/10/20/NetWorkGenerator/","excerpt":"","text":"生成模型$\\qquad$众所周知，神经网络是个判别模型，此处所指”生成模型”指的是功能性质，即利用神经网络生成我们需要的数据，比如对话生成/图像生成/语音生成。$\\qquad$试想，我们现在想生成一个图片，需要怎么做呢？ 需要知道我们要生成一个什么样图片,比如我们想生成一个卡通头像 需要找到类似所需卡通图片的很多卡通图像 用2中的所有图像训练一个模型 给3中的模型一个输入，得到所需卡通头像 AutoRegression AutoRegression 自回归是一种Step By Step的方式优点：对对话/语音这类顺序明显的类别比较友好缺点： 对图像而言，很难确定一个好的生成顺序 在inference阶段，当前步骤严重依赖前一步骤 慢AutoEncoderAutoEncoder 优点: 无需Step By Step，可以并行生成 生成数据更规则，一板一眼。 缺点：loss是对元素级别的监督度量，全局信息关注不足，因此模型泛化不够，两个非常接近的输入，可能输出天差地别。 GAN GAN 从任意分布（如高斯分布）sample一个向量，输入Generator得到和所需图片相同size的输出 把1中的输出当作副样本，从训练数据中sample一个正样本，以二分类的形式训练Discriminator(freeze Generator)。 重新sample一个向量，输入Generator得到和所需图片相同size的输出，将其当作正样本输入Discriminator，计算loss 训练Generator(freeze Discriminator)上诉三个步骤是一个轮次。这里面有两个问题： 为什么Generator不自己学？如果这样做的话训练应该是这样一个流程：从任意分布（如高斯分布）sample一个向量，输入Generator得到和所需图片相同size的输出，从训练数据中sample一个图像，和该输出计算loss并反向传播。答：Generatory自己学其实就是训练AutoDecoder的Decoder部分，这样计算loss的时候只是对元素级别的监督度量，全局信息关注不足。 为什么","categories":[],"tags":[]},{"title":"Lipschitz连续条件","slug":"Lipschitz","date":"2021-09-05T08:00:20.000Z","updated":"2021-09-19T18:03:58.740Z","comments":true,"path":"2021/09/05/Lipschitz/","link":"","permalink":"https://lingyixia.github.io/2021/09/05/Lipschitz/","excerpt":"","text":"Lipschitz约束$\\qquad$如果对于函数$f_w(x)$的定义域内任意输入$(x_1,f_w(x_1)),(x_2,f_w(x_2))$都满足存在一个常数$L_w$使得： ||f_w(x_1)-f_w(x_2)|| \\leq L_w||x_1-x_2|| \\tag{1}$\\qquad$则称$f_w(x)$满足利普希茨连续条件,其中，最小的$L_w$叫做Lipschitz常数。可以看到，Lipschitz连续条件约束的是函数的范数，当$f_w(x)$是实值函数时，该范数即是绝对值，该公示可以简单理解为，一个函数的一阶导数有界。 模型鲁棒性$\\qquad$做算法的会经常看到鲁棒性这个词，用来描述算法的抗干扰能力，这个性质就是说一个模型，对于两个很接近的输入，其输出也必须很接近，就说其鲁棒性强，用数学语言描述： \\lim\\limits_{\\Delta x\\to+0} f_w(x+\\Delta x) -f_w(x)\\rightarrow 0 \\tag{2}Lipschitz与激活函数$\\qquad$大家知道,我们常用的激活函数有这些$sigmoid$、$relu$,$tanh$,如果问为什么需要激活函数，回到应该是非线性化，那如果问为什么常用的激活函数是这三个呢？$x^3$行不行？$\\qquad$ 答案是不行，从Lipschitz的角度分析，这三个激活函数不仅仅是能够做到非线性化，而且其一阶导函数是有界的，如果使用$x^3$，当输入一个很大的数据$x$ 时，其一阶导函数必然非常大，则$f_w(x+\\Delta x)-f_w(x)$也必然很大，则在模型看来，两个很接近的数据其输出却差这么多，这样学下去模型很容易废掉。$\\qquad$ 而这三个激活函数不仅保证一阶函数有界，而且在最大导数点在原点，还能更方便的和Normalization结合。","categories":[{"name":"数学","slug":"数学","permalink":"https://lingyixia.github.io/categories/数学/"}],"tags":[]},{"title":"n维空间任意两向量几乎正交","slug":"ndimvectorangle","date":"2021-08-26T06:29:23.000Z","updated":"2021-10-26T03:54:53.012Z","comments":true,"path":"2021/08/26/ndimvectorangle/","link":"","permalink":"https://lingyixia.github.io/2021/08/26/ndimvectorangle/","excerpt":"","text":"命题本文要证明的是高n维空间中，任意两个向量都几乎正交，注意：不是两个向量垂直的概率大，而是”几乎”垂直，即夹角接近$\\frac{π}{2}$。基本思路就是考虑两个随机向量的夹角$\\theta$分布，然后求导得到概率密度，就可以看出在$\\theta$在哪个范围内最大。 命题重定义随机两个向量不好求，我们可以先固定一个，让另一个随机即可，假设固定向量为： x=(1,0,...,0) \\tag{1}随机向量为： y=(y_1,y_2,...,y_n) \\tag{2}现在我们把原命题重新定义为n维单位超球面上，任意一个点与原点组成的单位向量和$(1,0,0,…,0)$向量都几乎垂直直接计算可以得到: cos \\langle x,y \\rangle=\\frac{x_1}{\\sqrt(x_1^2+x_2^2+...x_n^2)} \\tag{3}现在要求的就是公式(3)的概率分布和密度，到这里还是一筹莫展。 球坐标系将$y$直角坐标转为球坐标系后为： \\left\\{ \\begin{aligned} y_1 & = cos(\\varphi_1) \\\\ y_2 & = sin(\\varphi_1)cos(\\varphi_2) \\\\ y_3 &= sin(\\varphi_1)sin(\\varphi_2) cos(\\varphi_3) \\\\ . . .\\\\ y_{n-1} &= sin(\\varphi_1)sin(\\varphi_2)...sina(\\varphi_{n-2}) cos(\\varphi_{n-1}) \\\\ y_{n} &= sin(\\varphi_1)sin(\\varphi_2)...sina(\\varphi_{n-2}) sin(\\varphi_{n-1}) \\end{aligned} \\right. \\tag{4}其中，$\\varphi_{n-1} \\in[0,2π]$, $\\varphi_{0…n-2} \\in[0,π]$此时,公式(3)中$cos \\langle x,y \\rangle$恰好等于$\\varphi_1$,即两者之间的角度就是$\\varphi_1$ P_n(\\varphi_1","categories":[{"name":"数学","slug":"数学","permalink":"https://lingyixia.github.io/categories/数学/"}],"tags":[]},{"title":"SpatialPyramidPooling","slug":"SpatialPyramidPooling","date":"2021-06-25T02:27:27.000Z","updated":"2021-09-19T18:01:25.948Z","comments":true,"path":"2021/06/25/SpatialPyramidPooling/","link":"","permalink":"https://lingyixia.github.io/2021/06/25/SpatialPyramidPooling/","excerpt":"中文名：空间金字塔卷积，通常卷积之后会跟一个全链接层，这就造成如果输入图片size不同，卷积之后size也不同，输入全链接层的feature就不同，这样压根不能运行，通常的解决方案是通过clip/wrap预先将图片处理成相同size，现在可以用该方法来代替。","text":"中文名：空间金字塔卷积，通常卷积之后会跟一个全链接层，这就造成如果输入图片size不同，卷积之后size也不同，输入全链接层的feature就不同，这样压根不能运行，通常的解决方案是通过clip/wrap预先将图片处理成相同size，现在可以用该方法来代替。 赘述通常的CNN结构是这样的： inputs \\rightarrow clip/wrap \\rightarrow CNN \\rightarrow flat \\rightarrow dense加入ssp后结构是这样的: inputs \\rightarrow CNN \\rightarrow ssp \\rightarrow dense对比 原始：$8 \\times 204 \\times 196 \\times 3 \\stackrel{cnn}{\\rightarrow} 8 \\times 13 \\times 14 \\times 256 \\stackrel{flat}{\\rightarrow} 8 \\times 46592$$8 \\times 302 \\times 197 \\times 3 \\stackrel{cnn}{\\rightarrow} 8 \\times 19 \\times 14 \\times 256 \\stackrel{flat}{\\rightarrow} 8 \\times 68096$ 显然两者最后的输出不可能输入在同一个dense中训练。 clip/warp$8 \\times 204 \\times 196 \\times 3 \\stackrel{clip}{\\rightarrow} 8 \\times 224 \\times 224 \\times 3 \\stackrel{cnn}{\\rightarrow} 8 \\times 13 \\times 13 \\times 256 \\stackrel{flat}{\\rightarrow} 8 \\times 43264$$8 \\times 302 \\times 197 \\times 3 \\stackrel{clip}{\\rightarrow} 8 \\times 224 \\times 224 \\times 3 \\stackrel{cnn}{\\rightarrow} 8 \\times 13 \\times 13 \\times 256 \\stackrel{flat}{\\rightarrow} 8 \\times 43264$ ssp$8 \\times 204 \\times 196 \\times 3 \\stackrel{cnn}{\\rightarrow} 8 \\times 13 \\times 14 \\times 256 \\stackrel{ssp}{\\rightarrow} 8 \\times 5376$$8 \\times 302 \\times 197 \\times 3 \\stackrel{cnn}{\\rightarrow} 8 \\times 19 \\times 14 \\times 256 \\stackrel{ssp}{\\rightarrow} 8 \\times 5376$ 代码123456789101112131415161718import tensorflow as tfdef SpatialPyramidPooling(previous_conv, out_pool_size_list): b, w, h, c = previous_conv.shape for index, pool_size in enumerate(out_pool_size_list): w_wid = tf.cast(tf.math.ceil(w / pool_size), tf.int64) h_wid = tf.cast(tf.math.ceil(h / pool_size), tf.int64) max_pooling = tf.keras.layers.MaxPooling2D(pool_size=(w_wid, h_wid), strides=(w_wid, h_wid), padding=&apos;same&apos;) result = tf.reshape(max_pooling(previous_conv), (b, -1)) spp = result if index == 0 else tf.concat([spp, result], axis=-1) return sppif __name__ == &apos;__main__&apos;: inputs = tf.random.normal(shape=(8, 19, 14, 256)) result = SpatialPyramidPooling(inputs, out_pool_size_list=[4, 2, 1]) print(result.shape)","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lingyixia.github.io/categories/神经网络/"}],"tags":[{"name":"卷积","slug":"卷积","permalink":"https://lingyixia.github.io/tags/卷积/"}]},{"title":"相对编码","slug":"realtive-position-transformer","date":"2021-04-04T15:16:54.000Z","updated":"2021-09-19T18:01:25.963Z","comments":true,"path":"2021/04/04/realtive-position-transformer/","link":"","permalink":"https://lingyixia.github.io/2021/04/04/realtive-position-transformer/","excerpt":"","text":"Relative Position Embedding论文地址参考博客1参考博客1 该论文的考虑出发点为原始的编码方式仅仅考虑了位置的距离关系，没有考虑位置的先后关系，本抛弃了vanilla transformer中静态位置编码，使用一个可训练的相对位置矩阵来表示位置信息。公示很简单：原始Attention： e_{ij}=\\frac{x_iW_q x_iW_k^T}{\\sqrt{d_{model}}} \\\\ a_{ij} = softmax(w_ij) \\\\ z_i = \\sum_{j=1}^n a_{ij}x_jW_vRelative Position Attention: e_{ij}=\\frac{x_iW_q (x_iW_k+a_{ij}^k)^T}{\\sqrt{d_{model}}} \\\\ a_{ij} = softmax(w_{ij}) \\\\ z_i = \\sum_{j=1}^n a_{ij}(x_jW_v+a_{ij}^v) 其中，$a_{ij}^k$和$a_{ij}^v$分别表示两个可学习的位置信息,至于为什么加在这两个地方，自然是因为这两个地方计算了相对位置。同时，作者发现如果两个单词距离超过某个阈值$k$提升不大，因此在此限制了位置最大距离，即超过$k$的距离也按照$k$距离的位置信息计算。位置信息本质是在训练两个矩阵$W^K=(w_{-k}^K,…,w_k^K)$和$W^V=(w_{-k}^V,…,w_k^V)$ a_{ij}^K=W_{clip(j-i,k)}^K \\\\ a_{ij}^V=W_{clip(j-i,k)}^V \\\\ clip(x,k)=max(-k,min(k,x)) 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import torchimport torch.nn as nntorch.manual_seed(2021)class RelativePosition(nn.Module): def __init__(self, num_units, max_relative_position): super().__init__() self.num_units = num_units self.max_relative_position = max_relative_position self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units)) nn.init.xavier_uniform_(self.embeddings_table) def forward(self, length_q, length_k): range_vec_q = torch.arange(length_q) range_vec_k = torch.arange(length_k) distance_mat = range_vec_k[None, :] - range_vec_q[:, None] distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position) final_mat = distance_mat_clipped + self.max_relative_position final_mat = torch.LongTensor(final_mat) embeddings = self.embeddings_table[final_mat] return embeddingsclass MultiHeadAttentionLayer(nn.Module): def __init__(self, hid_dim, n_heads, dropout, device): super().__init__() assert hid_dim % n_heads == 0 self.hid_dim = hid_dim self.n_heads = n_heads self.head_dim = hid_dim // n_heads self.max_relative_position = 2 self.relative_position_k = RelativePosition(self.head_dim, self.max_relative_position) self.relative_position_v = RelativePosition(self.head_dim, self.max_relative_position) self.fc_q = nn.Linear(hid_dim, hid_dim) self.fc_k = nn.Linear(hid_dim, hid_dim) self.fc_v = nn.Linear(hid_dim, hid_dim) self.fc_o = nn.Linear(hid_dim, hid_dim) self.dropout = nn.Dropout(dropout) self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device) def forward(self, query, key, value, mask=None): # query = [batch size, query len, hid dim] # key = [batch size, key len, hid dim] # value = [batch size, value len, hid dim] batch_size = query.shape[0] len_k = key.shape[1] len_q = query.shape[1] len_v = value.shape[1] query = self.fc_q(query) key = self.fc_k(key) value = self.fc_v(value) r_q1 = query.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) r_k1 = key.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2)) # q对k元素的attention r_k2 = self.relative_position_k(len_q, len_k) attn2 = torch.einsum(&apos;bhqe,qke-&gt;bhqk&apos;, r_q1, r_k2) # q对k位置的attention attn = (attn1 + attn2) / self.scale if mask is not None: attn = attn.masked_fill(mask == 0, -1e10) attn = self.dropout(torch.softmax(attn, dim=-1)) r_v1 = value.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) weight1 = torch.matmul(attn, r_v1) # qk对v元素的attention r_v2 = self.relative_position_v(len_q, len_v) weight2 = torch.einsum(&apos;bhav,ave-&gt;bhae&apos;, attn, r_v2) # qk对v位置的attention x = weight1 + weight2 x = x.permute(0, 2, 1, 3).contiguous() x = x.view(batch_size, -1, self.hid_dim) x = self.fc_o(x) return xif __name__ == &apos;__main__&apos;: multiHeadAttentionLayer = MultiHeadAttentionLayer(128, 8, 0.5, &apos;cpu&apos;) x = torch.randn(4, 43, 128) result = multiHeadAttentionLayer(x, x, x) print(result) # x = torch.randn(64, 8, 43, 16) # y = torch.randn(43, 43, 16) # print(torch.einsum(&apos;bhqe,qke-&gt;bhqk&apos;, [x, y])) TENER 本来应该先写Transformer_xl,但还没完全懂，用这个做过度。 论文地址参考博客1参考博客2该论文用在NER任务中，同样的，主要目的也是解决普通Transformer只有距离没有先后的问题，主要思路是回归了vanilla transformer的编码方式，但是将静态相对位置信息加入其中，其实是一个不可训练的相对位置矩阵(具体可看代码，其实表示思路和第一篇相似)。相对位置矩阵为： R_{t-j}=[...,sin(\\frac{t-j}{10000^{2i/d_{model}}}),cos(\\frac{t-j}{10000^{2i/d_{model}}})] 可以看到，对于sin而言，正负号是有影响的，但是cos无影响。 原始Transformer中Attention score计算公示为: A_{i,j}=\\underbrace{E_i^TW_qW_kE_j}_a+\\underbrace{E_i^TW_qW_kU_j}_b+ \\underbrace{U_i^TW_qW_kE_j}_c+\\underbrace{U_i^TW_qW_kU_j}_d 解释：a:第i个单词内容对第j个单词内容的scoreb:第i个单词内容对第j个单词位置的scorec:第i个单词位置对第j个单词内容的scored:第i个单词位置对第j个单词位置的score 更改后Attention score计算公示为： A_{i,j}=Q_tK_j^T+Q_tR_{i-j}^T+uK_j^T+vR_{i-j}这个看着不方便，其实就是Transformer_xl的公示，应该为： A_{i,j}=\\underbrace{E_i^TW_qW_{k,E}E_j}_a+\\underbrace{E_i^TW_qW_{k,R}R_{i-j}}_b+ \\underbrace{u^TW_kE_j}_c+\\underbrace{v^TW_kU_j}_d 不同于之前，这里的四项是分开计算的，其中a不变，b其实就是第一篇论文的$e_{ij}$计算部分，很显然，第一篇论文少了c和d","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"Attention","slug":"Attention","permalink":"https://lingyixia.github.io/tags/Attention/"},{"name":"Transformer","slug":"Transformer","permalink":"https://lingyixia.github.io/tags/Transformer/"}]},{"title":"泊松分布和指数分布","slug":"possionAndexp","date":"2020-09-20T08:38:57.000Z","updated":"2021-09-19T18:01:25.963Z","comments":true,"path":"2020/09/20/possionAndexp/","link":"","permalink":"https://lingyixia.github.io/2020/09/20/possionAndexp/","excerpt":"一直不理解一些分布是干什么用的，怎么来的，看了马同学高数才有了一些了解，目前的感觉是，似乎常见的分布都来自于二项分布。此处加以记录。","text":"一直不理解一些分布是干什么用的，怎么来的，看了马同学高数才有了一些了解，目前的感觉是，似乎常见的分布都来自于二项分布。此处加以记录。 从二项分布到泊松分布 已知：馒头老板统计了一周内每天卖的馒头数量 问题：一个馒头店每天需要准备多少馒头？最简单的办法是求平均值，每天就准备这么多馒头就行了，但是问题是如果每天卖出馒头数量方差较大，很容易有好几天准备不足或准备过剩。 分析：将一天销售时间均分为$n$个阶段，似的每一阶段只卖出一个馒头，那么，每一阶段卖出与不卖出馒头就是一个伯努利分布，$n$个阶段就是二项分布。设每天卖出馒头为随机变量X，则一天销售时间卖出$k$个馒头的概率为： P(X=K)=C_n^kp^k(1-p)^{n-k} \\tag{1}很显然，时间是连续的，不能这样分，但是当$n \\to+\\infty$的时候，可以认为是连续的。上诉二项分布$p$怎么求呢？由$np=\\mu$得$p=\\frac{\\mu}{p}$,$\\mu$可由一周内均值近似，则有： \\begin{aligned} P(X=k)&=\\lim_{n \\to +\\infty} C_n^k(\\frac{\\mu}{n})^k(1-\\frac{\\mu}{n})^{n-k}\\\\ &=\\lim_{x \\to +\\infty} \\frac{n(n-1)...(n-k+1)}{k!}\\frac{\\mu^k}{n^k}(1-\\frac{\\mu}{n})^{n-k} \\\\ &=\\lim_{x \\to +\\infty}\\frac{\\mu^k}{k!} \\frac{n}{n}\\frac{n-1}{n}...\\frac{n-k+1}{n}(1-\\frac{\\mu}{n})^{-k}(1-\\frac{\\mu}{n})^n \\end{aligned}其中$\\frac{n}{n} \\frac{n-1}{n}…\\frac{n-k+1}{n}$和$(1-\\frac{\\mu}{n})^{-k}$在$n \\to +\\infty$下都是1，对于最后一个因式: \\lim_{n \\to +\\infty}(1-\\frac{\\mu}{n})^n=\\lim_{n\\to+\\infty}((1+(-\\frac{1}{\\frac{n}{\\mu}}))^{-\\frac{n}{\\mu}})^{-\\mu} = e^{-\\mu}故而原式可化为$P(X=k)=\\frac{\\mu^k}{k!}e^{-\\mu}$,即泊松分布的公式。因此，可以认为，泊松分布是描述某段时间内，卖出多少馒头的分布，其中$\\mu$代表这段时间内卖出馒头的期望。从泊松分布到指数分布上诉泊松分布只告诉了我们确定时间段内的分布，改造该公式如下： P(X=k,t)=\\frac{(\\mu t)^k}{k!}e^{-\\mu t}称之为波松过程，可以看出，当$t=1$时就是普通的泊松分布，当$t=n$的时候表示时间间隔为$n$内卖出馒头的分布。对于馒头店老板而言，不仅仅需要知道每天要准备多少馒头，还需要知道卖出馒头的时间间隔，好以此适时调整服务员人数。设随机变量X表示两次卖出馒头之间的间隔，则有： P(Y>t)=P(X=0,0)=\\frac{(\\mu t)^0}{0!}e^{-\\mu t}=e^{-\\mu t} $P(X=k,t)$表示时间段t内卖出k个馒头的概率，P(Y&gt;t)表示两次卖出馒头时间间隔大于t的概率，则就是时间t内卖出0个馒头的概率。则: P(Y\\le t)=1-e^{-\\mu t}即指数分布","categories":[{"name":"概率论","slug":"概率论","permalink":"https://lingyixia.github.io/categories/概率论/"}],"tags":[{"name":"泊松分布","slug":"泊松分布","permalink":"https://lingyixia.github.io/tags/泊松分布/"},{"name":"指数分布","slug":"指数分布","permalink":"https://lingyixia.github.io/tags/指数分布/"}]},{"title":"神经网络到底保存了什么","slug":"NetWorkStorage","date":"2020-03-18T06:36:26.000Z","updated":"2021-09-21T06:29:00.303Z","comments":true,"path":"2020/03/18/NetWorkStorage/","link":"","permalink":"https://lingyixia.github.io/2020/03/18/NetWorkStorage/","excerpt":"","text":"都知道，无论使用什么框架，神经网络都是非常消耗显存的，那么，这些消耗的显存到底保存了什么？ 从一个例子开始 标注解释:$z_i^k$:第$k$层第$i$个$feature$的输出(不经过激活函数))$a_i^k$：其值为$g(z_i^k)$,$g(x)$为激活函数，这里把输入层数据$x_i$看作第0层。$w_{ij}^k$: 从第$k-1$层到第$k$层的传播权重这里去掉了偏置权重$b$ 正向传播: \\left\\{ \\begin{aligned} z_1^1=w_{11}^1a_1^0+w_{21}^1a_2^0 \\\\ z_2^1=w_{12}^1a_1^0+w_{22}^1a_2^0 \\\\ z_3^1=w_{13}^1a_1^0+w_{23}^1a_2^0 \\end{aligned} \\right. \\tag{1}a_i^1=g(z_i^1) \\tag{2} \\left\\{ \\begin{aligned} z_1^2=w_{11}^2a_1^1+w_{21}^2a_2^1+ w_{31}^2a_3^1\\\\ z_2^2=w_{12}^2a_1^1+w_{22}^2a_2^1+ w_{32}^2a_3^1\\\\ \\end{aligned} \\right. \\tag{3}a_i^2=g(z_i^2) \\tag{4}反向传播假设损失函数为MSE损失: Loss=\\frac{1}{2}\\sum_{i=0}^n(y_i-a_i^2)^2 \\tag{5}下面开始计算对各个参数$w_{ij}^k$的偏导。从最近的开始: \\left\\{ \\begin{aligned} \\frac{\\partial L}{\\partial w_{11}^2}= \\frac{\\partial L}{\\partial z_1^2} \\frac{\\partial z_1^2}{w_{11}^2}=\\frac{\\partial L}{\\partial z_1^2} a_1^1 \\\\ \\frac{\\partial L}{\\partial w_{21}^2}= \\frac{\\partial L}{\\partial z_1^2} \\frac{\\partial z_1^2}{w_{21}^2}=\\frac{\\partial L}{\\partial z_1^2} a_2^1 \\\\ \\frac{\\partial L}{\\partial w_{31}^2}= \\frac{\\partial L}{\\partial z_1^2} \\frac{\\partial z_1^2}{w_{31}^2}=\\frac{\\partial L}{\\partial z_1^2} a_3^1 \\\\\\\\ \\frac{\\partial L}{\\partial w_{12}^2}= \\frac{\\partial L}{\\partial z_2^2} \\frac{\\partial z_2^2}{w_{12}^2}=\\frac{\\partial L}{\\partial z_2^2} a_1^1 \\\\ \\frac{\\partial L}{\\partial w_{22}^2}= \\frac{\\partial L}{\\partial z_2^2} \\frac{\\partial z_2^2}{w_{22}^2}=\\frac{\\partial L}{\\partial z_2^2} a_2^1 \\\\ \\frac{\\partial L}{\\partial w_{32}^2}= \\frac{\\partial L}{\\partial z_2^2} \\frac{\\partial z_2^2}{w_{32}^2}=\\frac{\\partial L}{\\partial z_3^2} a_3^1 \\end{aligned} \\right. \\tag{6} \\left\\{ \\begin{aligned} \\frac{\\partial L}{\\partial w_{11}^1}= \\frac{\\partial L}{\\partial z_1^1} \\frac{\\partial z_1^1}{\\partial w_{11}^1}=\\frac{\\partial L}{\\partial z_1^1} a_1^0 \\\\ \\frac{\\partial L}{\\partial w_{21}^1}= \\frac{\\partial L}{\\partial z_1^1} \\frac{\\partial z_1^1}{\\partial w_{21}^1}=\\frac{\\partial L}{\\partial z_1^1} a_2^0 \\\\\\\\ \\frac{\\partial L}{\\partial w_{12}^1}= \\frac{\\partial L}{\\partial z_1^1} \\frac{\\partial z_2^1}{\\partial w_{12}^1}=\\frac{\\partial L}{\\partial z_2^1} a_1^0 \\\\ \\frac{\\partial L}{\\partial w_{22}^1}= \\frac{\\partial L}{\\partial z_2^1} \\frac{\\partial z_2^1}{\\partial w_{22}^1}=\\frac{\\partial L}{\\partial z_1^1} a_2^0 \\\\\\\\ \\frac{\\partial L}{\\partial w_{13}^1}= \\frac{\\partial L}{\\partial z_3^1} \\frac{\\partial z_3^1}{\\partial w_{13}^1}=\\frac{\\partial L}{\\partial z_3^1} a_1^0 \\\\ \\frac{\\partial L}{\\partial w_{23}^1}= \\frac{\\partial L}{\\partial z_3^1} \\frac{\\partial z_3^1}{\\partial w_{23}^1}=\\frac{\\partial L}{\\partial z_3^1} a_2^0 \\end{aligned} \\right. \\tag{7}由公示(6)(7)可以总结出来: \\frac{\\partial L}{\\partial w_{ij}^k}=\\frac{\\partial L}{\\partial z_j^k}a_i^{k-1} \\tag{8}其中$a_i^{k-1}$在正向传播中已经算出，下面计算$\\frac{\\partial L}{\\partial z_j^k}$: \\left\\{ \\begin{aligned} \\frac{\\partial L}{\\partial z_1^2}=\\frac{\\partial L}{\\partial a_1^2}g'(z_1^2) \\\\ \\frac{\\partial L}{\\partial z_2^2}=\\frac{\\partial L}{\\partial a_2^2}g'(z_2^2) \\end{aligned} \\right. \\tag{9} \\left\\{ \\begin{aligned} \\frac{\\partial L}{\\partial z_1^1}=\\frac{\\partial L}{\\partial a_1^1}g'(z_1^1)=(\\frac{\\partial L}{\\partial z_1^2}w_{11}^2+\\frac{\\partial L}{\\partial z_2^2}w_{12}^2)g'(z_1^1) \\\\ \\frac{\\partial L}{\\partial z_2^1}=\\frac{\\partial L}{\\partial a_2^1}g'(z_2^1)=(\\frac{\\partial L}{\\partial z_1^2}w_{21}^2+\\frac{\\partial L}{\\partial z_2^2}w_{22}^2)g'(z_2^1) \\\\ \\frac{\\partial L}{\\partial z_3^1}=\\frac{\\partial L}{\\partial a_3^1}g'(z_3^1)=(\\frac{\\partial L}{\\partial z_1^2}w_{31}^2+\\frac{\\partial L}{\\partial z_2^2}w_{32}^2)g'(z_3^1) \\end{aligned} \\right. \\tag{10}由(9)(10)可知，要求得$\\frac{\\partial L}{\\partial z_j^k}$，需要$w_{ij}^k$和$z_i^k$。 总结$\\qquad$现在可以得出结论，在整个神经网络训练过程中，出于反向传播的需要，我们需要在正向传播的时候在显存中记录$a_i^k$、$w_{ij}^k$和$z_i^k$。但是由于我们常用的激活函数都是可逆的，即知道$a_i^k$能反算出$z_i^k$，因此也可以只保存$a_i^k$、$w_{ij}^k$。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[]},{"title":"差分法","slug":"difference","date":"2019-09-23T00:20:44.000Z","updated":"2021-10-23T12:36:58.352Z","comments":true,"path":"2019/09/23/difference/","link":"","permalink":"https://lingyixia.github.io/2019/09/23/difference/","excerpt":"","text":"从一道例题开始 有一片大海，大海中隐藏着若干陆地，当海平面下降的时候漏出的连续陆地会组成一个小岛，问题：当海平面处于多高的时候才能使漏出的小岛数量最多？数量是多少？ 差分法解释一个数组$A[1,2,3,4,5,6,7,8,9]$他的差分数组就是$A[i]-A[i-1]$，为方便计算我们给数组$A$头部插入一个0，变为$A[0,1,2,3,4,5,6,7,8,9]$，则其差分数组为$B[0,1,1,1,1,1,1,1,1,1]$(头部0也是单独插入的)，差分数组又一个很重要的性质,$\\sum_0^nB[i]=A[n]$，即$B$的前缀和等于$A$的相应下表的值，这有什么用呢？例如我们想给$A[2:6]$区间+1，除了直接在$A$中循环，还可以把$B$更改为$[0,1,2,1,1,1,1,0,1,1]$,即$B[2]+1,B[7]-1$,然后在按照前缀和的形式加回去，也能算出来，这就是差分法，上题就是一个经典案例。 暴力法1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;int main()&#123; vector&lt;int&gt; nums = &#123;6, 5, 7, 1, 4, 2, 4&#125;; nums.insert(nums.begin(), 0); nums.push_back(0); int max_value = *max_element(nums.begin(), nums.end()); vector&lt;int&gt; levels(max_value + 1);//海平面 for(int i = 1; i &lt; nums.size() - 1; ++i) &#123; if(nums[i] &gt; nums[i - 1]) &#123; for(int j = nums[i - 1] + 1; j &lt;= nums[i]; ++j) &#123; levels[j]++; &#125; &#125; &#125; cout &lt;&lt; *max_element(levels.begin(), levels.end()); return 0;&#125; 很明显，复杂度$O(n^2)$ 差分法1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;void add_diff(vector&lt;int&gt;&amp; diff, int low, int high)&#123; diff[low]++; diff[high + 1]--;&#125;int main()&#123; vector&lt;int&gt; nums = &#123;6, 5, 7, 1, 4, 2, 4&#125;; nums.insert(nums.begin(), 0); nums.push_back(0); int max_value = *max_element(nums.begin(), nums.end()); vector&lt;int&gt; diff(max_value + 1);//差分数组 for(int i = 1; i &lt; nums.size() - 1; ++i) &#123; if(nums[i] &gt; nums[i - 1]) &#123; add_diff(diff, nums[i - 1] + 1, nums[i]); &#125; &#125; int maxn = 0, pre = 0; for(int p = 1; p &lt;= max_value; p++) &#123; pre += diff[p]; maxn = max(maxn, pre); &#125; cout &lt;&lt; maxn; return 0;&#125; 复杂度$O(n)$","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"差分法","slug":"差分法","permalink":"https://lingyixia.github.io/tags/差分法/"}]},{"title":"BPE 和 WordPiece","slug":"BPE","date":"2019-08-10T09:37:51.000Z","updated":"2021-09-19T18:01:25.943Z","comments":true,"path":"2019/08/10/BPE/","link":"","permalink":"https://lingyixia.github.io/2019/08/10/BPE/","excerpt":"","text":"WordPiece 是从 BPE(byte pair encoder) 发展而来的一种处理词的技术，目的是解决 OOV 问题,以翻译模型为例,原理是抽取公共二元串(bigram),首先看下BPE(Transformer的官方代码也是使用的这种方式): BPE关键代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109import re, collectionsdef get_pairs(word): &quot;&quot;&quot;Return set of symbol pairs in a word. Word is represented as a tuple of symbols (symbols being variable-length strings). &quot;&quot;&quot; pairs = set() prev_char = word[0] for char in word[1:]: pairs.add((prev_char, char)) prev_char = char return pairsdef encode(orig): &quot;&quot;&quot;Encode word based on list of BPE merge operations, which are applied consecutively&quot;&quot;&quot; word = tuple(orig) + (&apos;&lt;/w&gt;&apos;,) print(&quot;__word split into characters:__ &lt;tt&gt;&#123;&#125;&lt;/tt&gt;&quot;.format(word)) pairs = get_pairs(word) if not pairs: return orig iteration = 0 while True: iteration += 1 print(&quot;__Iteration &#123;&#125;:__&quot;.format(iteration)) print(&quot;bigrams in the word: &#123;&#125;&quot;.format(pairs)) print(pairs) bigram = min(pairs, key=lambda pair: bpe_codes.get(pair, float(&apos;inf&apos;))) print(&quot;candidate for merging: &#123;&#125;&quot;.format(bigram)) if bigram not in bpe_codes: print(&quot;__Candidate not in BPE merges, algorithm stops.__&quot;) break first, second = bigram new_word = [] i = 0 while i &lt; len(word): try: j = word.index(first, i) new_word.extend(word[i:j]) i = j except: new_word.extend(word[i:]) break if word[i] == first and i &lt; len(word) - 1 and word[i + 1] == second: new_word.append(first + second) i += 2 else: new_word.append(word[i]) i += 1 new_word = tuple(new_word) word = new_word print(&quot;word after merging: &#123;&#125;&quot;.format(word)) if len(word) == 1: break else: pairs = get_pairs(word) # don&apos;t print end-of-word symbols if word[-1] == &apos;&lt;/w&gt;&apos;: word = word[:-1] elif word[-1].endswith(&apos;&lt;/w&gt;&apos;): word = word[:-1] + (word[-1].replace(&apos;&lt;/w&gt;&apos;, &apos;&apos;),) return worddef get_stats(vocab): pairs = collections.defaultdict(int) for word, freq in vocab.items(): symbols = word.split() for i in range(len(symbols) - 1): pairs[symbols[i], symbols[i + 1]] += freq return pairsdef merge_vocab(pair, v_in): v_out = &#123;&#125; bigram = re.escape(&apos; &apos;.join(pair)) p = re.compile(r&apos;(?&lt;!\\S)&apos; + bigram + r&apos;(?!\\S)&apos;) for word in v_in: w_out = p.sub(&apos;&apos;.join(pair), word) v_out[w_out] = v_in[word] return v_outif __name__ == &apos;__main__&apos;: train_data = &#123;&apos;l o w&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 5, &apos;l o w e r&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 2, &apos;n e w e s t&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 6, &apos;w i d e s t&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 3&#125; bpe_codes = &#123;&#125; bpe_codes_reverse = &#123;&#125; num_merges = 1000 for i in range(num_merges): pairs = get_stats(train_data) if not pairs: break print(&quot;Iteration &#123;&#125;&quot;.format(i + 1)) best = max(pairs, key=pairs.get) train_data = merge_vocab(best, train_data) bpe_codes[best] = i bpe_codes_reverse[best[0] + best[1]] = best print(&quot;new merge: &#123;&#125;&quot;.format(best)) print(&quot;train data: &#123;&#125;&quot;.format(train_data)) 输出结果: Iteration 1new merge: (‘e’, ‘s’)train data: {‘l o w&lt;/w&gt;’: 5, ‘l o w e r&lt;/w&gt;’: 2, ‘n e w es t&lt;/w&gt;’: 6, ‘w i d es t&lt;/w&gt;’: 3}Iteration 2new merge: (‘es’, ‘t&lt;/w&gt;’)train data: {‘l o w&lt;/w&gt;’: 5, ‘l o w e r&lt;/w&gt;’: 2, ‘n e w est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}Iteration 3new merge: (‘l’, ‘o’)train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘n e w est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}Iteration 4new merge: (‘n’, ‘e’)train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘ne w est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}Iteration 5new merge: (‘ne’, ‘w’)train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘new est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}Iteration 6new merge: (‘new’, ‘est&lt;/w&gt;’)train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}Iteration 7new merge: (‘lo’, ‘w&lt;/w&gt;’)train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}Iteration 8new merge: (‘w’, ‘i’)train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘wi d est&lt;/w&gt;’: 3}Iteration 9new merge: (‘wi’, ‘d’)train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘wid est&lt;/w&gt;’: 3}Iteration 10new merge: (‘wid’, ‘est&lt;/w&gt;’)train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}Iteration 11new merge: (‘lo’, ‘w’)train data: {‘low&lt;/w&gt;’: 5, ‘low e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}Iteration 12new merge: (‘low’, ‘e’)train data: {‘low&lt;/w&gt;’: 5, ‘lowe r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}Iteration 13new merge: (‘lowe’, ‘r&lt;/w&gt;’)train data: {‘low&lt;/w&gt;’: 5, ‘lower&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3} 可以看到，首先输入是词典{单词:词频}的形式,在每一个轮次都会寻找一个最大的子串，上诉第一次频率最大的子串就是(‘e’, ‘s’),然后把字典中所有的(‘e’, ‘s’)合并就得到了{‘l o w &lt;\\/w&gt;’: 5, ‘l o w e r &lt;\\/w&gt;’: 2, ‘n e w es t &lt;\\/w&gt;’: 6, ‘w i d es &lt;\\/w&gt;’: 3, ‘f o l l o w &lt;\\/w&gt;’: 1},后面以此类推,直到最大的词频小于某个阈值为止，上面设置的是2，最终得到的词表是:train data: {‘low&lt;\\/w&gt;’: 5, ‘lower&lt;\\/w&gt;’: 2, ‘newest&lt;\\/w&gt;’: 6, ‘wides&lt;\\/w&gt;’: 3, ‘f o l low&lt;\\/w&gt;’: 1}， 这就是处理原语料的过程，在训练的时候，首先用上诉的encode代码把训练数据根据code.file映射到’voc.txt’中的词，然后进行训练(label方面的处理方式是独立的，也不一定需要BPE处理) subword-nmt使用数据准备类似：https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/tests/data/corpus.en subword-nmt learn-bpe -s {num_operations} &lt; {train_file} &gt; {codes_file} 123作用：生成分词器eg: subword-nmt learn-bpe -s 30000 &lt; corpus.en &gt; codes_filecodes_file生成的就是接下来用到的分词器，其实就是一个词对组成的文件，其中每一行都是当时预料中词对中频率最高的一个。（上诉代码对应这部分） subword-nmt apply-bpe -c {codes_file} &lt; {test_file} &gt; {out_file} 12345678910作用：用分词器处理预料eg:subword-nmt apply-bpe -c codes_file &lt; corpus.en &gt; out_fileout_file中就是靠分词器生成的语料这里的操作单元是对原始预料的各个单词，比如&apos;cement&apos;，分为&apos;c e m e n t&lt;/w&gt;&apos;1. 词对为:[&lt;c,e&gt;,&lt;e,m&gt;,&lt;m,e&gt;,&lt;e,n&gt;,&lt;n,t&lt;/w&gt;&gt;],其中[&lt;e,m&gt;,&lt;m,e&gt;,&lt;e,n&gt;]在codes_file,并且&lt;e,n&gt;在codes_file排名靠前(语料中词频高),合并结果为:&apos;c e m en t&lt;/w&gt;&apos;2. 词对为:[&lt;c,e&gt;,&lt;e,m&gt;,&lt;m,en&gt;,&lt;en,t&lt;/w&gt;&gt;],其中[&lt;e,m&gt;,&lt;en,t&lt;/w&gt;&gt;]在codes_file,并且&lt;en,t&lt;/w&gt;&gt;在codes_file中排名靠前，合并结果为:&apos;c e m ent&lt;/w&gt;&apos;...最终合并结果为:&apos;c ement&lt;/w&gt;&apos;此时只有一个词对&lt;c,ement&lt;/w&gt;&gt;,并且不再codes_file中，因此合并停止，该词分为两个子词:c,ement,在预料中为:c@@ ement subword-nmt get-vocab —train_file {train_file} —vocab_file {vocab_file} 123作用：生成词典（训练模型要用）eg: subword-nmt get-vocab --input out_file --output vocab_filevocab_file就是预料对应的词典(把out_file 中的单词set一便即可)，即接下来用vocab_file作为词典，out_file作为语料训练模型即可 模型训练完成后，在具体场景使用的时候，必定会有@（因为词典中有@，用来区分该单词是前缀还是独立单词），因此要对后缀是@的单词跟下一个单词合并。 WordPiece WordPiece是Bert使用的处理方式,这个过两天在写吧，有点事。。。 参考: https://github.com/wszlong/sb-nmt https://blog.csdn.net/u013453936/article/details/80878412 http://ufal.mff.cuni.cz/~helcl/courses/npfl116/ipython/byte_pair_encoding.html","categories":[{"name":"NLP","slug":"NLP","permalink":"https://lingyixia.github.io/categories/NLP/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/tags/算法/"}]},{"title":"顿悟最大熵","slug":"maxlikehood","date":"2019-07-28T07:27:59.000Z","updated":"2021-09-30T06:08:06.246Z","comments":true,"path":"2019/07/28/maxlikehood/","link":"","permalink":"https://lingyixia.github.io/2019/07/28/maxlikehood/","excerpt":"","text":"虽然以前也了解最大熵模型，甚至以为自己完全理解了最大熵,知道今天才发现我以前都回了点啥。。。。今天发现自己顿悟了一下，特此记录，希望能给看到的人一点帮助，再次膜拜孔圣人的远见”温故而知新，可以为师矣.”(未完待续) 从最大熵思想开始其实在我看来，所谓的最大熵思想就是对已知的条件充分考虑,对未知的条件不做任何假设,这就是最大熵的真谛. 这就是最大熵模型5个海贼团$A、B、C、D、E$去争夺罗杰的宝藏,宝藏只有一份,问谁能得到?我们给他建立的模型是均匀模型$P(X=A)=\\frac{1}{6},P(X=B)=\\frac{1}{6},P(X=C)=\\frac{1}{6},P(X=D)=\\frac{1}{6},P(X=E)=\\frac{1}{6},P(X=6)=\\frac{1}{6}$我们为毛会建立这样的模型呢?因为均匀模型的熵最大,我们对这个5个海贼团一无所知,因此只能创建均匀模型来保证熵最大，这就是最大熵的一个最简单应用 5个海贼团$A、B、C、D、E$去争夺罗杰的宝藏,我们已知:$A$得到宝藏的概率和$B$一样,都是$\\frac{1}{10}$,$C$和$D$一样,都是$\\frac{3}{10}$,而$E$就比较牛逼了,他的概率是$\\frac{5}{10},因为他叫路飞$,宝藏只有一份,问谁能得到?现在我们有了条件，就不能简单的创建均匀模型了,因为此时我们要找的模型不是$P(X,\\theta)$,而是$P(Y|X,\\theta)$,我们需要的是$P(Y|X,\\theta)$的熵最大，这便是最大熵模型 看看公式已知:$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N) }$目标:运用最大熵原理找最大熵模型由上诉最大熵模型的由来我们很容易得出公式,不是要找一个模型,在$X$的条件下这个模型的$P(Y|X,\\theta)$的熵最大么,很明显这是个条件熵呀,那么这个模型在这份数据上的条件熵的公式: H(Y|X) = \\sum_{x \\in X} \\widetilde{P}(x)H(Y|X=x)) \\tag{1}其中$\\widetilde{P}(x)=\\frac{N_x}{N}$现在我们的目标确定了,我们要从P(Y|X,\\theta_1)，P(Y|X,\\theta_2)，P(Y|X,\\theta_3)，P(Y|X,\\theta_4),,,很多的模型中找到一个模型,这个模型充分考虑了已知数据$T$,而且对未知不做任何假设，不做任何假设解决了,我们让公式$(1)$要尽量大即可,那么充分考虑已知数据要怎样表示呢?说到这有人可能会有些思绪了,其实就是从$T$中找一个用来表示充分考虑已知条件的约束,让公式$(1)$在这个约数下尽量大.找到这个约束,一个最优化问题就出现了,现在我们的目的就是找这个约束。 充分考虑现在我们想想，什么是充分考虑了已知数据呢?比如我们找到了一个模型$P(X|Y，\\theta)$,要考量它是对已知数据的考虑程度,把这句话数学化就是考量这个模型对原始数据特征的考虑程度,怎样表示原始数据的特征呢?特征函数出现了！！！我们一般用这样一个特征函数来描述特征: f(x,y)=\\begin{cases} 1 这个x和y满足某一事实\\\\ 0 这个x和y不满足某一事实\\\\ \\end{cases}那么这分数据特征的总值是多少呢?找期望呗 E_{\\widetilde p}(f)=\\sum_{x,y}\\widetilde P(x,y)f(x,y) \\tag{2}其中$\\widetilde P(x,y)=\\frac{N{xy}}{N}$，谨记:$E{\\widetilde p}(f)$是原始数据的特征总值.对于我们找的的模型$P(X|Y，\\theta)$在这份数据的特征总值是多少呢? E_{p}(f)=\\sum_{x,y}\\widetilde P(x)P(y|x,\\theta)f(x,y) \\tag{3}要想让$P(Y|X,\\theta)$充分考虑原始数据咋做呢?两个特征总值相等呗 E_{\\widetilde p}(f)=E_{p}(f) \\tag{4}这就是约束条件了. 最大熵模型现在我们的目标确定了,我们要让公式$(1)$在约束条件为公式$(4)$的条件下越大越好,现在我们把公式$(1)$展开: \\begin{align} H(Y|X) &= \\sum_{x \\in X} \\widetilde{P}(x)H(Y|X=x)) \\\\ &=-\\sum_{x \\in X}\\widetilde P(x)\\sum_{y \\in Y}P(y|X=x) \\log P(Y|X=x)\\\\ &= -\\sum_{x \\in X}\\sum_{y \\in Y}\\widetilde P(x)P(y|X=x) \\log P(Y|X=x)\\\\ &=-\\sum_{x ,y}\\widetilde P(x)P(y|x) \\log P(y|x) \\end{align}所以最终公式为: \\max \\quad \\quad \\quad -\\sum_{x ,y}\\widetilde P(x)P(y|x) \\log P(y|x) \\\\ s.t. \\quad \\quad \\quad \\quad E_{\\widetilde p}(f_i)=E_{p}(f_i) \\quad i=1,2,3... \\\\ \\quad \\quad \\quad \\quad \\sum_{y}P(y|x)=1(隐含条件)其中i代表第i个特征函数完毕散花!!!!卧槽写完后发现写的好清楚…不信你看了还不明白！ 最大熵模型求解1.首先引入拉格朗日乘子$w_0,w_1…$ \\begin{align} L(P(y|x),w)&=-\\sum_{x ,y}\\widetilde P(x)P(y|x) \\log P(y|x)+w_0(1-\\sum_{y}P(y|x))+\\sum_{i=0}^{n}w_i(E_{\\widetilde p}(f_i)-E_{p}(f_i))\\\\ &=\\sum_{x ,y}\\widetilde P(x)P(y|x) \\log P(y|x)+w_0(1-\\sum_{y}P(y|x))+\\sum_{i=1}^n w_i(\\sum_{x,y} w_i\\widetilde P(x,y)f_i(x,y)-\\sum_{x,y}\\widetilde P(y|x)f_i(x,y)) \\end{align}原始问题是: \\min_{P(y|x)} \\max_wL(P(y|x),w)对偶问题是: \\max_{w} \\min_{P(y|x)}L(P(y|x),w)令: \\Psi(w)=\\min_{P(y|x)}L(P(y|x),w)首先求内部: \\begin{align} \\frac{\\partial P(y|x,w)}{\\partial P(y|x)}&=\\sum_{x,y}\\widetilde P(x)(\\log P(y|x)+1)-\\sum_y w_0 - \\sum_{x,y}(\\widetilde P(x)\\sum_{i=1}^n w_if_i(x,y)) \\\\ &=\\sum_{x,y}\\widetilde P(x)(\\log P(y|x)+1-w_0-\\sum_{i=1}^n w_if_i(x,y)) \\end{align}令偏导数为0，得: P(y|x)=e^{\\sum_{i=1}^nw_if_i(x,y)+w_0-1}=\\frac{e^{\\sum_{i=1}^nw_if_i(x,y)}}{e^{1-w_0}}因为有: \\sum_{y}P(y|x)=1因此: e^{1-w_0}=\\sum_y e^{\\sum_{i=1}^n w_if_i(x,y)}=Z_w(x)因此，最终求得: P(y|x,\\theta)=e^{\\sum_{i=1}^nw_if_i(x,y)+w_0-1}=\\frac{e^{\\sum_{i=1}^nw_if_i(x,y)}}{\\sum_y e^{\\sum_{i=1}^n w_if_i(x,y)}}注意看这里，这不就是Softmax么!!!!!!!!!!!!!!!!!!最后一步在求$\\max_w$即可. 最大似然估计最大似然估计的一般公式为: L(P_w)=log \\prod_{x,y}P(y|x)^{\\widetilde P(x,y)}=\\sum_{x,y}\\widetilde P(x,y)logP(x|y)我们求得的最大熵模型为: \\Psi(w)=\\sum_{x ,y}\\widetilde P(x)P(y|x) \\log P(y|x)+w_0(1-\\sum_{y}P(y|x))+\\sum_{i=1}^n w_i(\\sum_{x,y} w_i\\widetilde P(x,y)f_i(x,y)-\\sum_{x,y}\\widetilde P(y|x)f_i(x,y))我们要证明这两个式子求得得结果是相同的,即: P(y|x)=\\frac{e^{\\sum_{i=1}^nw_if_i(x,y)}}{e^{1-w_0}}带入得: \\begin{align} L(P_w)&=\\sum_{x,y}\\widetilde P(x,y)\\sum_{i=1}^nw_if_i(x,y)-\\sum_{x,y}\\widetilde P(x,y)\\log Z_w(x) \\\\ &=\\sum_{x,y}\\widetilde P(x,y)\\sum_{i=1}^nw_if_i(x,y)-\\sum_x \\widetilde P(x)\\log Z_w(x) \\end{align}得: \\begin{align} \\Psi(w)&=\\sum_{x,y}^n\\widetilde P(x,y)\\sum_{i=1}^nw_if_i(x,y)+\\sum_{x,y}\\widetilde P(x)P(y|x)(\\log P(y|x)-\\sum_{i=1}w_if_i(x,y))\\\\ &=\\sum_{x,y}^n\\widetilde P(x,y)\\sum_{i=1}^nw_if_i(x,y)-\\sum_{x,y}\\widetilde P(x)P_w(y|x)\\log Z_w(x)\\\\ &=\\sum_{x,y}^n\\widetilde P(x,y)\\sum_{i=1}^nw_if_i(x,y)-\\sum_{x}\\widetilde P(x)\\log Z_w(x) \\end{align}因此，当带入$P(y|x,\\theta)$时，得到得公式是相同得，因此求得的$\\max_w$也一定相同. 从最大熵模型到逻辑回归二元逻辑回归的似然函数为: L(\\theta)=\\prod_{i}^n[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}在这里，其实$\\pi(x_i)$就是$P(Y=1|X=x_i)$而且$y_i$完全可以写成数据中$Y=1$的数量你说对也不对。。其实在细想还可以写成数据中$X=x_i,Y=1$的数量.所以可以写成: L(\\theta)=P(Y=1|X=x_i)^{N_{1x_i}}P(Y=0|X=x_i)^{N_{0x_i}}在开个N(N是数据总量)次方得: L(\\theta)=P(Y=1|X=x_i)^\\frac{N_{1x_i}}{N}P(Y=0|X=x_i)^\\frac{N_{0x_i}}{N}想想$\\frac{N_{0x_i}}{N}$是啥,,,这不就是$\\widetilde{P}(x,y)$么,,,而且开个N次方对优化也没有影响,拿这个公式和$公式(3)$去掉log,即$P(y|x)^{\\widetilde{p}(x,y)}$对比一下，这不一样么.哈哈哈，有没有恍然大明白的感觉。 再到最大似然估计概率论与数理统计中的最大似然概率为公式为: \\begin{align} L(\\theta)&=log\\prod_iP(x_i;\\theta) \\\\ &=log\\prod_iP(x_i;\\theta)^{Nx_i} \\\\ &=\\sum_iN{x_i}logP(x_i;\\theta) \\\\ &=N\\sum_i\\frac{Nx_i}{N}logP(x_i;\\theta)\\\\ &=N\\sum_i\\widetilde{p}(x_i)logP(x_i;\\theta)\\\\ &=Nlog\\prod_iP(x_i;\\theta)^{\\widetilde{P}(x_i)} \\end{align}这里只是一元$x$,怎么对应到二元$(x,y)$呢？其实两者同理，需要正确的理解无论是几元，这里的$p(x_i;\\theta)$的真正意义是变量特征函数的联合分布，对应到二元就是: L(\\theta)=Nlog\\prod_{i,j}P(x_i,y_j;\\theta)^{\\widetilde{P}(x_i,y_j)}也就是说概率论与数理统计中的最大似然估计其实完全针对的是联合分布，特征和标签的地位完全一样,得到的似然函数公式其实是一个信息熵。但是我们的目的得到输入任何$x$后$y$的分布，是个边缘分布,则该边缘分布的熵就是个条件熵： \\begin{align} H(Y|x)&={P}(x) \\sum_{y\\in Y}P(x|y;\\theta)logP(y|x;\\theta) \\\\ &=\\sum_{y\\in Y}P(x,y;\\theta)logP(y|x;\\theta) \\\\ &=\\prod_y logP(y|x;\\theta)^{P(x,y;\\theta)} \\\\ &=\\prod_y logP(y|x;\\theta)^{\\widetilde P(x,y)} (近似用数出来的数据代替P(x,y;\\theta))\\\\ \\end{align}","categories":[{"name":"数学","slug":"数学","permalink":"https://lingyixia.github.io/categories/数学/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/tags/机器学习/"}]},{"title":"贪心","slug":"greedy","date":"2019-07-27T02:37:31.000Z","updated":"2021-09-19T18:01:25.958Z","comments":true,"path":"2019/07/27/greedy/","link":"","permalink":"https://lingyixia.github.io/2019/07/27/greedy/","excerpt":"","text":"Jump Game贪心一: 每到一个i,如果i&lt;=reach意味着[0,i-1]的坐标能达到reach,如果i&gt;reach,则意味着根本就到不了这里,无需继续。 12345678910111213bool canJump(vector&lt;int&gt;&amp; nums) &#123; int reach = 0; for (int i =0; i &lt; nums.size(); i++) &#123; if (i &gt; reach || i &gt;= nums.size() - 1) break; else &#123; reach = max(reach, i + nums[i]); &#125; &#125; return reach &gt;= nums.size() - 1;&#125; 贪心二: 和上诉形式不一样,思想差不多 123456789101112bool canJump(vector&lt;int&gt;&amp; nums) &#123; int len = nums.size(); int curMax = nums[0]; for (int i = 0; i &lt;= curMax; i++) &#123; if (nums[i] + i &gt;= len - 1) return true; curMax = max(curMax, nums[i] + i); &#125; return false;&#125; 动态规划: dp[i]表示到达i时候最多还剩下多少步 12345678910bool canJump(vector&lt;int&gt;&amp; nums) &#123; vector&lt;int&gt; dp(nums.size(), 0); for (int i = 1; i &lt; nums.size(); ++i) &#123; dp[i] = max(dp[i - 1], nums[i - 1]) - 1; if (dp[i] &lt; 0) return false; &#125; return true;&#125; 推销员 阿明是一名推销员，他奉命到螺丝街推销他们公司的产品。螺丝街是一条死胡同，出口与入口是同一个，街道的一侧是围墙，另一侧是住户。螺丝街一共有 N 家住户，第 i 家住户到入口的距离为 Si 米。由于同一栋房子里可以有多家住户，所以可能有多家住户与入口的距离相等。阿明会从入口进入，依次向螺丝街的 X 家住户推销产品，然后再原路走出去。阿明每走 1 米就会积累 1 点疲劳值，向第 i 家住户推销产品会积累 Ai 点疲劳值。阿明是工作狂，他想知道，对于不同的 X，在不走多余的路的前提下，他最多可以积累多少点疲劳值。 暂存 #","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[]},{"title":"Bert","slug":"Bert","date":"2019-07-22T13:55:40.000Z","updated":"2021-09-19T18:01:25.943Z","comments":true,"path":"2019/07/22/Bert/","link":"","permalink":"https://lingyixia.github.io/2019/07/22/Bert/","excerpt":"","text":"本文不讲Bert原理,拟进行Bert源码分析和应用 Bert源码分析组织结构 这是Bert Github地址,打开后会看到这样的结构: 下面我将逐个分析上诉图片中加框的文件,其他的文件不是源码,不用分析. modeling.py 该文件是整个Bert模型源码,包含两个类: BertConfig:Bert配置类 BertModel:Bert模型类 embedding_lookup:用来返回函数token embedding词向量 embedding_postprocessor:得到token embedding+segment embedding+position embedding create_attention_mask_from_input_mask得到mask,用来attention该attention的部分 transformer_model和attention_layer:Transform的ender部分,也就是self-attention,不解释了，看太多遍了. 注意上面的顺序,不是乱写的,是按照BertModel调用顺序组织的. BertConfig1234567891011121314151617181920212223242526272829303132333435363738394041424344class BertConfig(object): def __init__(self, vocab_size,#词表大小 hidden_size=768,#即是词向量维度又是Transform的隐藏层维度 num_hidden_layers=12,#Transformer encoder中的隐藏层数,普通Transform中是6个 num_attention_heads=12,multi-head attention 的head的数量,普通Transform中是8个 intermediate_size=3072,encoder的“中间”隐层神经元数,普通Transform中是一个feed-forward hidden_act=&quot;gelu&quot;,#隐藏层激活函数 hidden_dropout_prob=0.1,#隐层dropout率 attention_probs_dropout_prob=0.1,#注意力部分的dropout max_position_embeddings=512,#最大位置编码长度,也就是序列的最大长度 type_vocab_size=16,#token_type_ids的大小,所谓的token_type_ids在Bert中是0或1，也就是上句标记为0，下句标记为1，鬼知道默认为16是啥意思。。。 initializer_range=0.02):随机初始化的参数 self.vocab_size = vocab_size self.hidden_size = hidden_size self.num_hidden_layers = num_hidden_layers self.num_attention_heads = num_attention_heads self.hidden_act = hidden_act self.intermediate_size = intermediate_size self.hidden_dropout_prob = hidden_dropout_prob self.attention_probs_dropout_prob = attention_probs_dropout_prob self.max_position_embeddings = max_position_embeddings self.type_vocab_size = type_vocab_size self.initializer_range = initializer_range @classmethod def from_dict(cls, json_object): config = BertConfig(vocab_size=None) for (key, value) in six.iteritems(json_object): config.__dict__[key] = value return config @classmethod def from_json_file(cls, json_file): with tf.gfile.GFile(json_file, &quot;r&quot;) as reader: text = reader.read() return cls.from_dict(json.loads(text)) def to_dict(self): output = copy.deepcopy(self.__dict__) return output def to_json_string(self): return json.dumps(self.to_dict(), indent=2, sort_keys=True) + &quot;\\n&quot; BertModel 现在进入正题,开始分析Bert模型源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class BertModel(object): def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None,use_one_hot_embeddings=False, scope=None): config = copy.deepcopy(config) if not is_training: config.hidden_dropout_prob = 0.0 config.attention_probs_dropout_prob = 0.0 input_shape = get_shape_list(input_ids, expected_rank=2) batch_size = input_shape[0] seq_length = input_shape[1] if input_mask is None: input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32) if token_type_ids is None: token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32) with tf.variable_scope(scope, default_name=&quot;bert&quot;): with tf.variable_scope(&quot;embeddings&quot;): (self.embedding_output, self.embedding_table) = embedding_lookup( input_ids=input_ids, vocab_size=config.vocab_size, embedding_size=config.hidden_size, initializer_range=config.initializer_range, word_embedding_name=&quot;word_embeddings&quot;, use_one_hot_embeddings=use_one_hot_embeddings)#调用embedding_lookup得到初始词向量 self.embedding_output = embedding_postprocessor( input_tensor=self.embedding_output, use_token_type=True, token_type_ids=token_type_ids, token_type_vocab_size=config.type_vocab_size, token_type_embedding_name=&quot;token_type_embeddings&quot;, use_position_embeddings=True, position_embedding_name=&quot;position_embeddings&quot;, initializer_range=config.initializer_range, max_position_embeddings=config.max_position_embeddings, dropout_prob=config.hidden_dropout_prob) with tf.variable_scope(&quot;encoder&quot;): attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask) self.all_encoder_layers = transformer_model( input_tensor=self.embedding_output, attention_mask=attention_mask, hidden_size=config.hidden_size, num_hidden_layers=config.num_hidden_layers, num_attention_heads=config.num_attention_heads, intermediate_size=config.intermediate_size, intermediate_act_fn=get_activation(config.hidden_act), hidden_dropout_prob=config.hidden_dropout_prob, attention_probs_dropout_prob=config.attention_probs_dropout_prob, initializer_range=config.initializer_range, do_return_all_layers=True) self.sequence_output = self.all_encoder_layers[-1] with tf.variable_scope(&quot;pooler&quot;): first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1) self.pooled_output = tf.layers.dense( first_token_tensor, config.hidden_size, activation=tf.tanh, kernel_initializer=create_initializer(config.initializer_range)) def get_pooled_output(self): return self.pooled_output def get_sequence_output(self): return self.sequence_output def get_all_encoder_layers(self): return self.all_encoder_layers def get_embedding_output(self): return self.embedding_output def get_embedding_table(self): return self.embedding_table 参数说明: config:一个BertConfig实例 is_training:bool类型,是否是训练流程,用类控制是否dropout input_ids:输入Tensor, shape是[batch_size, seq_length]. input_mask:shape是[batch_size, seq_length]无需细讲 token_type_ids:shape是[batch_size, seq_length],bert中就是0或1 use_one_hot_embeddings:在embedding_lookup返回词向量的时候使用,详细见embedding_lookup函数 embedding_lookup 为了得到进入模型的词向量(token embedding) 12345678910111213def embedding_lookup(input_ids,vocab_size,embedding_size=128,initializer_range=0.02,word_embedding_name=&quot;word_embeddings&quot;,use_one_hot_embeddings=False): if input_ids.shape.ndims == 2: input_ids = tf.expand_dims(input_ids, axis=[-1]) embedding_table = tf.get_variable(name=word_embedding_name,shape=[vocab_size, embedding_size],initializer=create_initializer(initializer_range)) flat_input_ids = tf.reshape(input_ids, [-1]) if use_one_hot_embeddings: one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size) output = tf.matmul(one_hot_input_ids, embedding_table) else: output = tf.gather(embedding_table, flat_input_ids) input_shape = get_shape_list(input_ids) output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size]) return (output, embedding_table) 参数说明： input_ids：[batch_size, seq_length] vocab_size:词典大小 initializer_range：初始化参数 word_embedding_name:不解释 use_one_hot_embeddings:是否使用one_hot方式初始化(为啥我感觉这里是True还是False结果得到的结果是一样的？？？？？)如下代码. return:token embedding:[batch_size, seq_length, embedding_size].和embedding_table(不解释) 12345678910111213141516import tensorflow as tftf.enable_eager_execution()flat_input_ids = [2, 4, 5]embedding_table = tf.constant(value=[[1, 2, 3, 4], [5, 6, 7, 8], [9, 1, 2, 3], [5, 6, 7, 8], [6, 4, 78, 9], [6, 8, 9, 3]],dtype=tf.float32)one_hot_input_ids = tf.one_hot(flat_input_ids, depth=6)output = tf.matmul(one_hot_input_ids, embedding_table)print(output)print(100*&apos;*&apos;)output = tf.gather(embedding_table, flat_input_ids)print(output) embedding_postprocessor bert模型的输入向量有三个,embedding_lookup得到的是token embedding 我们还需要segment embedding和position embedding,这三者的维度是完全相同的(废话不相同怎么加啊。。。)本部分代码会将这三个embeddig加起来并dropout 123456789101112131415161718192021222324252627282930313233343536373839404142def embedding_postprocessor(input_tensor, use_token_type=False, token_type_ids=None, token_type_vocab_size=16, token_type_embedding_name=&quot;token_type_embeddings&quot;, use_position_embeddings=True, position_embedding_name=&quot;position_embeddings&quot;, initializer_range=0.02, max_position_embeddings=512, dropout_prob=0.1): input_shape = get_shape_list(input_tensor, expected_rank=3) batch_size = input_shape[0] seq_length = input_shape[1] width = input_shape[2] output = input_tensor if use_token_type: if token_type_ids is None: raise ValueError(&quot;`token_type_ids` must be specified if&quot;&quot;`use_token_type` is True.&quot;) token_type_table = tf.get_variable(name=token_type_embedding_name, shape=[token_type_vocab_size, width], initializer=create_initializer(initializer_range)) flat_token_type_ids = tf.reshape(token_type_ids, [-1]) one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size) token_type_embeddings = tf.matmul(one_hot_ids, token_type_table) token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width]) output += token_type_embeddings if use_position_embeddings: assert_op = tf.assert_less_equal(seq_length, max_position_embeddings) with tf.control_dependencies([assert_op]): full_position_embeddings = tf.get_variable(name=position_embedding_name, shape=[max_position_embeddings, width], initializer=create_initializer(initializer_range)) position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1]) num_dims = len(output.shape.as_list()) position_broadcast_shape = [] for _ in range(num_dims - 2): position_broadcast_shape.append(1) position_broadcast_shape.extend([seq_length, width]) position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape) output += position_embeddings output = layer_norm_and_dropout(output, dropout_prob) return output 参数说明: input_tensor:token embedding[batch_size, seq_length, embedding_size] use_token_type是否使用segment embedding token_type_ids:[batch_size, seq_length],这两个参数其实就是控制生成segment embedding的,上诉代码中的output += token_type_embeddings就是得到token embedding+segment embedding use_position_embeddings:是否使用位置信息 max_position_embeddings:序列最大长度注: 本部分代码中的width其实就是词向量维度(换个embedding_size能死啊。。。) 可以看出位置信息跟Transform的固定方式不一样，它是训练出来的. output += position_embeddings就得到了三者的想加结果return :token embedding+segment embedding+position_embeddings create_attention_mask_from_input_mask 目的是将本来shape为[batch_size, seq_length]转为[batch_size, seq_length,seq_length],为什么要这样的维度呢?因为…..算了麻烦不写了，去我的另一篇Transform中看吧 12345678910def create_attention_mask_from_input_mask(from_tensor, to_mask): from_shape = get_shape_list(from_tensor, expected_rank=[2, 3]) batch_size = from_shape[0] from_seq_length = from_shape[1] to_shape = get_shape_list(to_mask, expected_rank=2) to_seq_length = to_shape[1] to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32) broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32) mask = broadcast_ones * to_mask return mask 参数说明: from_tensor:[batch_size, seq_length]. to_mask:[batch_size, seq_length]注:Transform中的mask和平常用的不太一样,这里的mask是为了在计算attention的时候”看不到不应该看到的内容”,计算方式为该看到的mask为0，不该看到的mask为一个负的很大的数字,然后两者相加(平常使用mask是看到的为1，看不到的为0，然后两者做点乘)，这样在计算softmax的时候那些负数的attention会非常非常小,也就基本看不到了. transformer_model 这一部分是Transform部分,但是只有encoder部分,从BertModel中的with tf.variable_scope(&quot;encoder&quot;):这一部分也可以看出来 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576def transformer_model(input_tensor, attention_mask=None, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, intermediate_act_fn=gelu, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, initializer_range=0.02, do_return_all_layers=False): if hidden_size % num_attention_heads != 0: raise ValueError( &quot;The hidden size (%d) is not a multiple of the number of attention &quot; &quot;heads (%d)&quot; % (hidden_size, num_attention_heads)) attention_head_size = int(hidden_size / num_attention_heads) input_shape = get_shape_list(input_tensor, expected_rank=3) batch_size = input_shape[0] seq_length = input_shape[1] input_width = input_shape[2] if input_width != hidden_size: raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; % (input_width, hidden_size)) prev_output = reshape_to_matrix(input_tensor)#这个不单独写了,就是将[batch_size, seq_length, embedding_size]的input 给reahpe为[batch_size*seq_length,embedding_size] all_layer_outputs = [] for layer_idx in range(num_hidden_layers): with tf.variable_scope(&quot;layer_%d&quot; % layer_idx): layer_input = prev_output with tf.variable_scope(&quot;attention&quot;): attention_heads = [] with tf.variable_scope(&quot;self&quot;): attention_head = attention_layer(from_tensor=layer_input, to_tensor=layer_input, attention_mask=attention_mask, num_attention_heads=num_attention_heads, size_per_head=attention_head_size, attention_probs_dropout_prob=attention_probs_dropout_prob, initializer_range=initializer_range, do_return_2d_tensor=True, batch_size=batch_size, from_seq_length=seq_length, to_seq_length=seq_length) attention_heads.append(attention_head) attention_output = None if len(attention_heads) == 1: attention_output = attention_heads[0] else: attention_output = tf.concat(attention_heads, axis=-1) with tf.variable_scope(&quot;output&quot;): attention_output = tf.layers.dense(attention_output, hidden_size, kernel_initializer=create_initializer(initializer_range)) attention_output = dropout(attention_output, hidden_dropout_prob) attention_output = layer_norm(attention_output + layer_input) with tf.variable_scope(&quot;intermediate&quot;):#feed-forword部分 intermediate_output = tf.layers.dense(attention_output, intermediate_size, activation=intermediate_act_fn, kernel_initializer=create_initializer(initializer_range)) with tf.variable_scope(&quot;output&quot;): layer_output = tf.layers.dense(intermediate_output, hidden_size, kernel_initializer=create_initializer(initializer_range)) layer_output = dropout(layer_output, hidden_dropout_prob) layer_output = layer_norm(layer_output + attention_output) prev_output = layer_output all_layer_outputs.append(layer_output) if do_return_all_layers: final_outputs = [] for layer_output in all_layer_outputs: final_output = reshape_from_matrix(layer_output, input_shape) final_outputs.append(final_output) return final_outputs else: final_output = reshape_from_matrix(prev_output, input_shape) return final_output 参数说明: input_tensor:token embedding+segment embedding+position embedding [batch_size, seq_length, embedding_size] attention_mask:[batch_size, seq_length,seq_length] hidden_size:不解释 num_hidden_layers:多少个ecncoder block num_attention_heads:多少个head intermediate_size:feed forward隐藏层维度 intermediate_act_fn:feed forward激活函数其他的不解释了return [batch_size, seq_length, hidden_size], attention_layer 其实就是self-attention,但是在计算的时候全都转换为了二维矩阵，按注释的意思是避免反复reshape,因为reshape在CPU/GPU上易于实现，但是在TPU上不易实现,这样可以加速训练. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def attention_layer(from_tensor, to_tensor, attention_mask=None, num_attention_heads=1, size_per_head=512, query_act=None, key_act=None, value_act=None, attention_probs_dropout_prob=0.0, initializer_range=0.02, do_return_2d_tensor=False, batch_size=None, from_seq_length=None, to_seq_length=None): def transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, width): output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width]) output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3]) return output_tensor from_shape = get_shape_list(from_tensor, expected_rank=[2, 3]) to_shape = get_shape_list(to_tensor, expected_rank=[2, 3]) if len(from_shape) != len(to_shape): raise ValueError(&quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;) if len(from_shape) == 3: batch_size = from_shape[0] from_seq_length = from_shape[1] to_seq_length = to_shape[1] elif len(from_shape) == 2: if (batch_size is None or from_seq_length is None or to_seq_length is None): raise ValueError( &quot;When passing in rank 2 tensors to attention_layer, the values &quot; &quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot; &quot;must all be specified.&quot;) from_tensor_2d = reshape_to_matrix(from_tensor) to_tensor_2d = reshape_to_matrix(to_tensor) query_layer = tf.layers.dense(from_tensor_2d, num_attention_heads * size_per_head, activation=query_act, name=&quot;query&quot;, kernel_initializer=create_initializer(initializer_range)) key_layer = tf.layers.dense(to_tensor_2d, num_attention_heads * size_per_head, activation=key_act, name=&quot;key&quot;, kernel_initializer=create_initializer(initializer_range)) value_layer = tf.layers.dense(to_tensor_2d, num_attention_heads * size_per_head, activation=value_act, name=&quot;value&quot;, kernel_initializer=create_initializer(initializer_range)) query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads, from_seq_length, size_per_head) key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, to_seq_length, size_per_head) attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True) attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head))) if attention_mask is not None: attention_mask = tf.expand_dims(attention_mask, axis=[1]) adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0 attention_scores += adder#这里就是使用mask来attention该attention的部分 attention_probs = tf.nn.softmax(attention_scores) attention_probs = dropout(attention_probs, attention_probs_dropout_prob) value_layer = tf.reshape(value_layer, [batch_size, to_seq_length, num_attention_heads, size_per_head]) value_layer = tf.transpose(value_layer, [0, 2, 1, 3]) context_layer = tf.matmul(attention_probs, value_layer) context_layer = tf.transpose(context_layer, [0, 2, 1, 3]) if do_return_2d_tensor: context_layer = tf.reshape(context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head]) else: context_layer = tf.reshape(context_layer, [batch_size, from_seq_length, num_attention_heads * size_per_head]) return context_layer 参数说明: from_tensor在Transform中被转为二维[batch_size*seq_length, embedding_size] to_shape:传过来的参数跟from_tensor一毛一样,在这里没什么卵用其实,因为q和k的length是一样的 attention_mask:[batch_size, seq_length,seq_length] num_attention_heads:head数量 size_per_head:每一个head维度,代码中是用总维度除以head数量得到的:attention_head_size = int(hidden_size / num_attention_heads)return: return :[batch_size, from_seq_length,num_attention_heads * size_per_head]. 激活函数1234def gelu(x): cdf = 0.5 * (1.0 + tf.tanh( (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))) return x * cdf 这个激活函数很有特色，其实这个公式就是$x \\times \\Phi(x)$,后一项是正态函数,也就是说,gelu中后面那一大堆其实近似等于$\\int_{-\\infty}^{x}\\frac{1}{\\sqrt(2\\pi)}e^{-\\frac{x^2}{2}}dx$,至于咋来的这个近似值，还不清楚。测试函数:123456789101112from scipy import statsimport matha = stats.norm.cdf(2, 0, 1)def gelu(x): return 0.5 * (1.0 + math.tanh((math.sqrt(2 / math.pi) * (x + 0.044715 * math.pow(x, 3)))))print(a)print(gelu(2))#结果:#0.9772498680518208#0.9772988470438875 总结一:看完模型感觉真特么简单这模型,似乎除了self-attention就啥都没有了,但是先别着急,一般情况下模型是重点，但是对于Bert而言，模型却仅仅是开始，真正的创新点还在下面. create_pretraining_data.py 这部分代码用来生成训练样本,我们从main函数开始看起,首先进入tokenization.py def main12345678910111213141516171819202122232425262728def main(_): tf.logging.set_verbosity(tf.logging.INFO) tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case) input_files = [] for input_pattern in FLAGS.input_file.split(&quot;,&quot;): input_files.extend(tf.gfile.Glob(input_pattern)) tf.logging.info(&quot;*** Reading from input files ***&quot;) for input_file in input_files: tf.logging.info(&quot; %s&quot;, input_file) rng = random.Random(FLAGS.random_seed) instances = create_training_instances(input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor, FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq, rng) output_files = FLAGS.output_file.split(&quot;,&quot;) tf.logging.info(&quot;*** Writing to output files ***&quot;) for output_file in output_files: tf.logging.info(&quot; %s&quot;, output_file) write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length, FLAGS.max_predictions_per_seq, output_files) class TrainingInstance 单个训练样本类,看__init__就能看出来，没什么其他东西 123456789101112131415161718192021class TrainingInstance(object): def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels, is_random_next): self.tokens = tokens self.segment_ids = segment_ids self.is_random_next = is_random_next self.masked_lm_positions = masked_lm_positions self.masked_lm_labels = masked_lm_labels def __str__(self): s = &quot;&quot; s += &quot;tokens: %s\\n&quot; % (&quot; &quot;.join( [tokenization.printable_text(x) for x in self.tokens])) s += &quot;segment_ids: %s\\n&quot; % (&quot; &quot;.join([str(x) for x in self.segment_ids])) s += &quot;is_random_next: %s\\n&quot; % self.is_random_next s += &quot;masked_lm_positions: %s\\n&quot; % (&quot; &quot;.join( [str(x) for x in self.masked_lm_positions])) s += &quot;masked_lm_labels: %s\\n&quot; % (&quot; &quot;.join( [tokenization.printable_text(x) for x in self.masked_lm_labels])) s += &quot;\\n&quot; return s def create_training_instances 这个函数是重中之重，用来生成123456789101112131415161718192021222324252627282930313233def create_training_instances(input_files, tokenizer, max_seq_length, dupe_factor, short_seq_prob, masked_lm_prob, max_predictions_per_seq, rng): all_documents = [[]]#外层是文档，内层是文档中的每个句子 for input_file in input_files: with tf.gfile.GFile(input_file, &quot;r&quot;) as reader: while True: line = tokenization.convert_to_unicode(reader.readline()) if not line: break line = line.strip() if not line:# 空行表示文档分割 all_documents.append([]) tokens = tokenizer.tokenize(line) if tokens: all_documents[-1].append(tokens) all_documents = [x for x in all_documents if x] rng.shuffle(all_documents) vocab_words = list(tokenizer.vocab.keys()) instances = [] for _ in range(dupe_factor): for document_index in range(len(all_documents)): instances.extend( create_instances_from_document(all_documents, document_index, max_seq_length, short_seq_prob, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)) rng.shuffle(instances) return instances 参数说明:dupe_factor:每一个句子用几次:因为如果一个句子只用一次的话那么mask的位置就是固定的，这样我们把每个句子在训练中都多用几次,而且没次的mask位置都不相同,就可以防止某些词永远看不到short_seq_prob:长度小于“max_seq_length”的样本比例。因为在fine-tune过程里面输入的target_seq_length是可变的（小于等于max_seq_length），那么为了防止过拟合也需要在pre-train的过程当中构造一些短的样本max_predictions_per_seq:一个句子里最多有多少个[MASK]标记masked_lm_prob:多少比例的Token被MASK掉rng:随机率 def create_instances_from_document 一个文档中抽取训练样本,重中之重 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091def create_instances_from_document(all_documents, document_index, max_seq_length, short_seq_prob, masked_lm_prob, max_predictions_per_seq, vocab_words, rng): document = all_documents[document_index] # 为[CLS], [SEP], [SEP]预留三个空位 max_num_tokens = max_seq_length - 3 target_seq_length = max_num_tokens # 以short_seq_prob的概率随机生成（2~max_num_tokens）的长度 if rng.random() &lt; short_seq_prob: target_seq_length = rng.randint(2, max_num_tokens) instances = [] current_chunk = [] current_length = 0 i = 0 while i &lt; len(document): segment = document[i] current_chunk.append(segment) current_length += len(segment) # 将句子依次加入current_chunk中，直到加完或者达到限制的最大长度 if i == len(document) - 1 or current_length &gt;= target_seq_length: if current_chunk: # `a_end`是第一个句子A结束的下标 a_end = 1 if len(current_chunk) &gt;= 2: a_end = rng.randint(1, len(current_chunk) - 1) tokens_a = [] for j in range(a_end): tokens_a.extend(current_chunk[j]) tokens_b = [] is_random_next = False # `a_end`是第一个句子A结束的下标 if len(current_chunk) == 1 or rng.random() &lt; 0.5: is_random_next = True target_b_length = target_seq_length - len(tokens_a) # 随机的挑选另外一篇文档的随机开始的句子 # 但是理论上有可能随机到的文档就是当前文档，因此需要一个while循环 # 这里只while循环10次，理论上还是有重复的可能性，但是我们忽略 for _ in range(10): random_document_index = rng.randint(0, len(all_documents) - 1) if random_document_index != document_index: break random_document = all_documents[random_document_index] random_start = rng.randint(0, len(random_document) - 1) for j in range(random_start, len(random_document)): tokens_b.extend(random_document[j]) if len(tokens_b) &gt;= target_b_length: break # 对于上述构建的随机下一句，我们并没有真正地使用它们 # 所以为了避免数据浪费，我们将其“放回” num_unused_segments = len(current_chunk) - a_end i -= num_unused_segments else: is_random_next = False for j in range(a_end, len(current_chunk)): tokens_b.extend(current_chunk[j]) # 如果太多了，随机去掉一些 truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng) assert len(tokens_a) &gt;= 1 assert len(tokens_b) &gt;= 1 tokens = [] segment_ids = [] # 处理句子A tokens.append(&quot;[CLS]&quot;) segment_ids.append(0) for token in tokens_a: tokens.append(token) segment_ids.append(0) # 句子A结束，加上【SEP】 tokens.append(&quot;[SEP]&quot;) segment_ids.append(0) # 处理句子B for token in tokens_b: tokens.append(token) segment_ids.append(1) # 句子B结束，加上【SEP】 tokens.append(&quot;[SEP]&quot;) segment_ids.append(1) # 调用 create_masked_lm_predictions来随机对某些Token进行mask (tokens, masked_lm_positions, masked_lm_labels) = create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng) instance = TrainingInstance(tokens=tokens, segment_ids=segment_ids, is_random_next=is_random_next, masked_lm_positions=masked_lm_positions, masked_lm_labels=masked_lm_labels) instances.append(instance) current_chunk = [] current_length = 0 i += 1 return instances def create_masked_lm_predictions 真正的mask在这里实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng): cand_indexes = [] # [CLS]和[SEP]不能用于MASK for (i, token) in enumerate(tokens): if token == &quot;[CLS]&quot; or token == &quot;[SEP]&quot;: continue if (FLAGS.do_whole_word_mask and len(cand_indexes) &gt;= 1 and token.startswith(&quot;##&quot;)): cand_indexes[-1].append(i) else: cand_indexes.append([i]) rng.shuffle(cand_indexes) output_tokens = list(tokens) num_to_predict = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob)))) masked_lms = [] covered_indexes = set() for index_set in cand_indexes: if len(masked_lms) &gt;= num_to_predict: break if len(masked_lms) + len(index_set) &gt; num_to_predict: continue is_any_index_covered = False for index in index_set: if index in covered_indexes: is_any_index_covered = True break if is_any_index_covered: continue for index in index_set: covered_indexes.add(index) masked_token = None # 80% of the time, replace with [MASK] if rng.random() &lt; 0.8: masked_token = &quot;[MASK]&quot; else: # 10% of the time, keep original if rng.random() &lt; 0.5: masked_token = tokens[index] # 10% of the time, replace with random word else: masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)] output_tokens[index] = masked_token masked_lms.append(MaskedLmInstance(index=index, label=tokens[index])) assert len(masked_lms) &lt;= num_to_predict # 按照下标重排，保证是原来句子中出现的顺序 masked_lms = sorted(masked_lms, key=lambda x: x.index) masked_lm_positions = [] masked_lm_labels = [] for p in masked_lms: masked_lm_positions.append(p.index) masked_lm_labels.append(p.label) return (output_tokens, masked_lm_positions, masked_lm_labels) 代码流程是这样的:首先嫁给你一个句子随机打乱,并确定一个句子的15%是多少个token，设num_to_predict,然后对于[0,是多少个token，设num_to_predict]token，以80%的概率替换为[mask],10%的概率替换，10%的概率保持,这样就做到了对于15%的toke80% [mask],10%替换,10%保持。而预测的不是那15%的80（标注问题），而是全部15%。为什么要mask呢？你想啊，我们的目的是得到这样一个模型:输入一个句子，输出一个能够尽可能表示该句子的向量(用最容易理解的语言就是我们不知道输入的是什么玩意，但是我们需要知道输出的向量是什么),如果不mask直接训练那不就相当于用1来推导1？而如果我们mask一部分就意味着并不知道输入(至少不知道全部),至于为什么要把15不全部mask，我觉得这个解释很不错，但是过于专业化: 如果把 100% 的输入替换为 [MASK]：模型会偏向为 [MASK] 输入建模，而不会学习到 non-masked 输入的表征。 如果把 90% 的输入替换为 [MASK]、10% 的输入替换为随机 token：模型会偏向认为 non-masked 输入是错的。 如果把 90% 的输入替换为 [MASK]、维持 10% 的输入不变：模型会偏向直接复制 non-masked 输入的上下文无关表征。所以，为了使模型可以学习到相对有效的上下文相关表征，需要以 1:1 的比例使用两种策略处理 non-masked 输入。论文提及，随机替换的输入只占整体的 1.5%，似乎不会对最终效果有影响（模型有足够的容错余量）。通俗点说就是全部mask的话就意味着用mask来预测真正的单词,学习的仅仅是mask(而且mask的每个词都不一样，学到的mask表示也不一样，很显然不合理)，加入10%的替换就意味着用错的词预测对的词，而10%保持不变意味着用1来推导1，因此后两个10%的作用其实是为了学到没有mask的部分。或者还有一种解释方式: 因为每次都是要学习这15%的token，其他的学不到(认识到这一点很重要)倘若某一个词在训练模型的时候被mask了，而微调的时候出现了咋办？因此不管怎样，都必须让模型好歹”认识一下”这个词.tokenization.py按照create_pretraining_data.py中main的调用顺序，先看FullTokenizer类 FullTokenizer12345678910111213141516171819class FullTokenizer(object): def __init__(self, vocab_file, do_lower_case=True): self.vocab = load_vocab(vocab_file) self.inv_vocab = &#123;v: k for k, v in self.vocab.items()&#125; self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case) self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab) def tokenize(self, text): split_tokens = [] for token in self.basic_tokenizer.tokenize(text): for sub_token in self.wordpiece_tokenizer.tokenize(token): split_tokens.append(sub_token) return split_tokens def convert_tokens_to_ids(self, tokens): return convert_by_vocab(self.vocab, tokens) def convert_ids_to_tokens(self, ids): return convert_by_vocab(self.inv_vocab, ids) 在__init__中可以看到，又得先分析BasicTokenizer类和WordpieceTokenizer类(哎呀真烦，最后在回来做超链接吧),除此之外就是调用了几个小函数,load_vocab它的输入参数是bert模型的词典,返回的是一个OrdereDict:{词:词号}.其他的不说了，没啥意思。 class BasicTokenizer 目的是根据空格，标点进行普通的分词，最后返回的是关于词的列表，对于中文而言是关于字的列表。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class BasicTokenizer(object): def __init__(self, do_lower_case=True): self.do_lower_case = do_lower_case def tokenize(self, text): ##其实就是把字符串转为了list，分英文单词和中文单词处理 ##eg:Mr. Cassius crossed the highway, and stopped suddenly.转为[&apos;mr&apos;, &apos;.&apos;, &apos;cassius&apos;, &apos;crossed&apos;, &apos;the&apos;, &apos;highway&apos;, &apos;,&apos;, &apos;and&apos;, &apos;stopped&apos;, &apos;suddenly&apos;, &apos;.&apos;] text = convert_to_unicode(text) text = self._clean_text(text) text = self._tokenize_chinese_chars(text) orig_tokens = whitespace_tokenize(text)#无需细说，就是把string按照空格切分为list split_tokens = [] for token in orig_tokens: if self.do_lower_case: token = token.lower() token = self._run_strip_accents(token)#这个函数干了什么我也没看明白,但是对正题流程不重要,略过吧 split_tokens.extend(self._run_split_on_punc(token)) output_tokens = whitespace_tokenize(&quot; &quot;.join(split_tokens)) return output_tokens def _run_strip_accents(self, text): text = unicodedata.normalize(&quot;NFD&quot;, text) output = [] for char in text: cat = unicodedata.category(char) if cat == &quot;Mn&quot;: continue output.append(char) return &quot;&quot;.join(output) def _run_split_on_punc(self, text): chars = list(text) i = 0 start_new_word = True output = [] while i &lt; len(chars): char = chars[i] if _is_punctuation(char): output.append([char]) start_new_word = True else: if start_new_word: output.append([]) start_new_word = False output[-1].append(char) i += 1 return [&quot;&quot;.join(x) for x in output] def _tokenize_chinese_chars(self, text): # 按字切分中文，其实就是英文单词不变,中文在字两侧添加空格 output = [] for char in text: cp = ord(char) if self._is_chinese_char(cp): output.append(&quot; &quot;) output.append(char) output.append(&quot; &quot;) else: output.append(char) return &quot;&quot;.join(output) def _is_chinese_char(self, cp): # 判断是否是汉字,这个函数很有意义，值得借鉴 # refer：https://www.cnblogs.com/straybirds/p/6392306.html if ((cp &gt;= 0x4E00 and cp &lt;= 0x9FFF) or # (cp &gt;= 0x3400 and cp &lt;= 0x4DBF) or # (cp &gt;= 0x20000 and cp &lt;= 0x2A6DF) or # (cp &gt;= 0x2A700 and cp &lt;= 0x2B73F) or # (cp &gt;= 0x2B740 and cp &lt;= 0x2B81F) or # (cp &gt;= 0x2B820 and cp &lt;= 0x2CEAF) or (cp &gt;= 0xF900 and cp &lt;= 0xFAFF) or # (cp &gt;= 0x2F800 and cp &lt;= 0x2FA1F)): # return True return False def _clean_text(self, text): # 去除无意义字符以及空格 output = [] for char in text: cp = ord(char) if cp == 0 or cp == 0xfffd or _is_control(char): continue if _is_whitespace(char): output.append(&quot; &quot;) else: output.append(char) return &quot;&quot;.join(output) class WordpieceTokenizer 这个才是重点,跑test的时候出现的那些##都是从这里拿来的，其实就是把未登录词在词表中匹配相应的前缀. 1234567891011121314151617181920212223242526272829303132333435363738class WordpieceTokenizer(object): def __init__(self, vocab, unk_token=&quot;[UNK]&quot;, max_input_chars_per_word=200): self.vocab = vocab self.unk_token = unk_token self.max_input_chars_per_word = max_input_chars_per_word def tokenize(self, text): text = convert_to_unicode(text) output_tokens = [] for token in whitespace_tokenize(text): chars = list(token) if len(chars) &gt; self.max_input_chars_per_word: output_tokens.append(self.unk_token) continue is_bad = False start = 0 sub_tokens = [] while start &lt; len(chars): end = len(chars) cur_substr = None while start &lt; end: substr = &quot;&quot;.join(chars[start:end]) if start &gt; 0: substr = &quot;##&quot; + substr if substr in self.vocab: cur_substr = substr break end -= 1 if cur_substr is None: is_bad = True break sub_tokens.append(cur_substr) start = end if is_bad: output_tokens.append(self.unk_token) else: output_tokens.extend(sub_tokens) return output_tokens tokenize说明: 使用贪心的最大正向匹配算法 eg:input = “unaffable” output = [“un”, “##aff”, “##able”],首先看”unaffable”在不在词表中，在的话就当做一个词，也就是WordPiece，不在的话在看”unaffabl”在不在，也就是while中的end-=1,最终发现”un”在词表中,算是一个WordPiece,然后start=2,也就是代码中的start=end,看”##affable”在不在词表中,在看”##affabl”(##表示接着前面)，最终返回[“un”, “##aff”, “##able”].注意，这样切分是可逆的，也就是可以根据词表重载”攒回”原词，以此便解决了oov问题.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://lingyixia.github.io/categories/NLP/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lingyixia.github.io/tags/论文/"}]},{"title":"dataAugmentation","slug":"dataAugmentation","date":"2019-07-16T12:34:44.000Z","updated":"2021-09-19T18:01:25.958Z","comments":true,"path":"2019/07/16/dataAugmentation/","link":"","permalink":"https://lingyixia.github.io/2019/07/16/dataAugmentation/","excerpt":"","text":"记录一下常用的NLP数据增加方式,数据增强常用于样本不够或者样本严重不均衡的情况下 随机drop和shuffle也就是把一个样本随机打乱词语顺序或者扔掉一些词语,当做新的样本,但是不能做过多的drop和shuffle，防止更改了原义 同义词替换回译这个很有技巧性，就是吧样本翻译成其他语言，然后在翻译回来，当做新的样本 生成对抗网络","categories":[],"tags":[]},{"title":"stackAndqueue","slug":"stackAndqueue","date":"2019-07-13T09:04:58.000Z","updated":"2021-09-19T18:01:25.966Z","comments":true,"path":"2019/07/13/stackAndqueue/","link":"","permalink":"https://lingyixia.github.io/2019/07/13/stackAndqueue/","excerpt":"","text":"弹栈压栈 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）算法描述: 只需要按照顺序走一遍即可1234567891011121314bool IsPopOrder(vector&lt;int&gt; pushV,vector&lt;int&gt; popV) &#123; stack&lt;int&gt; s; int index=0; for(int i =0;i&lt;pushV.size();i++) &#123; s.push(pushV[i]); while(!s.empty() &amp;&amp; s.top()==popV[index]) &#123; s.pop(); index++; &#125; &#125; return s.empty(); &#125; Tip: 对于一个入栈顺序的弹栈序列必然有这么一个特征:出栈序列中每个数后面的比它小的数必然按照降序排列比如入栈顺序是:1,2,3,4 4,1,2,3不可能是出栈顺序,因为4后面比4小的数1,2,3不是降序排列 3,1,4,2也不合法,3后面比3小的数1,2不是降序排列 1,2,3,4合法,当前每个数后面没有比它小的 删除相邻重复字符串123456789101112131415161718192021string removeDuplicates(string S)&#123; stack&lt;char&gt; s; for (auto ch:S) &#123; if (s.empty() || ch != s.top()) &#123; s.push(ch); &#125; else &#123; s.pop(); &#125; &#125; string result = &quot;&quot;; while (!s.empty()) &#123; result = s.top()+result; s.pop(); &#125; return result;&#125; 两个队列实现栈12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class MyStack&#123;private: queue&lt;int&gt; *q1; queue&lt;int&gt; *q2;public: MyStack() &#123; q1 = new queue&lt;int&gt;(); q2 = new queue&lt;int&gt;(); &#125; void push(int x) &#123; queue&lt;int&gt; *currentQ = q1-&gt;empty() ? q2 : q1; currentQ-&gt;push(x); &#125; int pop() &#123; queue&lt;int&gt; *currentQ = q1-&gt;empty() ? q2 : q1; queue&lt;int&gt; *emptyQ = q1-&gt;empty() ? q1 : q2; int current; while (!currentQ-&gt;empty()) &#123; current = currentQ-&gt;front(); currentQ-&gt;pop(); if (currentQ-&gt;empty()) break; emptyQ-&gt;push(current); &#125; return current; &#125; int top() &#123; queue&lt;int&gt; *currentQ = q1-&gt;empty() ? q2 : q1; queue&lt;int&gt; *emptyQ = q1-&gt;empty() ? q1 : q2; int current; while (!currentQ-&gt;empty()) &#123; current = currentQ-&gt;front(); currentQ-&gt;pop(); emptyQ-&gt;push(current); &#125; return current; &#125; bool empty() &#123; return q1-&gt;empty() &amp;&amp; q2-&gt;empty(); &#125;&#125;; 两个栈实现队列12345678910111213141516171819202122232425262728class MyQueue&#123;public: void push(int node) &#123; stack1.push(node); &#125; int pop() &#123; int result=0; int temp=0; if(stack2.empty()) &#123; while(!stack1.empty()) &#123; temp = stack1.top(); stack1.pop(); stack2.push(temp); &#125; &#125; result = stack2.top(); stack2.pop(); return result; &#125;private: stack&lt;int&gt; stack1; stack&lt;int&gt; stack2;&#125;;","categories":[],"tags":[]},{"title":"请叫我调参工程师","slug":"parameters","date":"2019-07-11T11:50:42.000Z","updated":"2021-09-19T18:01:25.962Z","comments":true,"path":"2019/07/11/parameters/","link":"","permalink":"https://lingyixia.github.io/2019/07/11/parameters/","excerpt":"","text":"这篇博客会一直更新.对于初学者而言，兴趣往往都在模型构成上面,但是，只有真正成为调参工程师才能深刻体会到调参对于模型的重要性，虽然我也只能算入门级,但已经被坑了好几次了。。。。。说多了都是泪啊。总的来说,训练神经网络模型的超参数一般可以分为两类: 训练参数:学习率,正则项系数,epoch数,batchsize 模型参数:模型层数,隐藏层参数 调参技巧 当你想实现一个模型的时候,一定要用最精简的结构去实现它,尽量不要在任何地方做其他tip,任何tip都要尽量在保证模型无误的条件下进行，否则你永远不知道你所谓的一个小tip对你的模型有多大影响(血的教训,当初在做一个文本生成模型的时候所有激活函数都用的relu,这个bug让我一顿好找。。。) 保证每次随机种子不变,才能让实验更有效的对比 训练参数学习率作为一个调参工程师，私以为,学习率是最重要的参数,对于初学者来说，往往最容易忽视学习率的作用.举一个我最开始被坑的例子,当初是做了一个NER模型,写完之后各项指标增长速度特别特别慢,慢到令人发指(一小时从1%到2%),但问题是确实是不断增长，由于刚刚接触也不知道增长速度应该是怎样就一直等着，凉了一天，发现还是那个速度,等不及了开始检查模型问题,由于刚刚接触TF，对自己没把握就一直找一直找，几乎找了一个星期愣是没改出来.不得已，要不调调参数把,还好第一个改的就是学习率,其实原来是0.01我以为已经够小了,改成了0.001,刚一运行，200个step后precision直接到了20%,我他娘的就。。。。原来问题在这。所以以后我在设置学习率的时候都会从一个特别小的数开始，比如0.0001，看看指标的变化，在增大一点学习率比如0.001，再看看变化，确定模型没问题，然后在开始。当然，学习率的设置还有很多方式,比如模拟退火方式,Transform中的学习率代码就使用了这种:1234567891011class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule): def __init__(self, d_model, warmup_steps=4000): super(CustomSchedule, self).__init__() self.d_model = d_model self.d_model = tf.cast(self.d_model, tf.float32) self.warmup_steps = warmup_steps def __call__(self, step): arg1 = tf.math.rsqrt(step) arg2 = step * (self.warmup_steps ** -1.5) return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) 也就是让学习率先快诉增大，当step达到warmup_steps后后在慢慢减小bert中的学习率是这样设置的:12345678910111213141516learning_rate = tf.train.polynomial_decay( learning_rate, global_step, num_train_steps, end_learning_rate=0.0, power=1.0, cycle=False) if num_warmup_steps: global_steps_int = tf.cast(global_step, tf.int32) warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32) global_steps_float = tf.cast(global_steps_int, tf.float32) warmup_steps_float = tf.cast(warmup_steps_int, tf.float32) warmup_percent_done = global_steps_float / warmup_steps_float warmup_learning_rate = init_lr * warmup_percent_done is_warmup = tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32) learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate) 它也是先让学习率快速增大，当step达到warmup_steps时，在安装多项式方式衰减 顺便提一下，learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)这行代码值得学习 batchsize batch 和 batch 之间差别太大,训练难以收敛,形成震荡 batchsize 增大会使梯度优化方向更准 随着 batch_size 增大，处理相同数据量的速度越快。 随着 batch_size 增大，达到相同精度所需要的 epoch 数量越来越多。 由于上述两种因素的矛盾，batch_size 增大到某个时候，达到时间上的最优。 增大 batchsize 能够有效的利用GPU并行能力 GPU对batchsize为2的整数次幂效果更好 模型参数首先需要知道的是，对于隐藏层和层数刚开始的设置要紧盯训练样本的量，要保证模型的参数量不高于样本量的一半，有权威称$\\frac{1}{10}$最好.反正你不要写完模型后参数太大,你想想用一千条数据去训练好几百万的参数能学到点啥？下面给出个统计模型参数的tf代码:123456789def __get_parametres(self): total_parameters = 0 for variable in tf.trainable_variables(): shape = variable.get_shape() variable_parameters = 1 for dim in shape: variable_parameters *= dim.value total_parameters += variable_parameters tf.logging.info(&quot;总参数量为:&#123;total_parameters&#125;&quot;.format(total_parameters=total_parameters))","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[]},{"title":"总有一个算法让你惊艳","slug":"interestAlgorighm","date":"2019-07-06T05:34:05.000Z","updated":"2021-09-20T03:42:25.852Z","comments":true,"path":"2019/07/06/interestAlgorighm/","link":"","permalink":"https://lingyixia.github.io/2019/07/06/interestAlgorighm/","excerpt":"","text":"总有一个算法让你惊艳 随机洗牌问:一副牌54张，如何洗牌才能让最公平。什么叫公平?就是每张牌在每个位置的概率都一样就是公平, Knuth 老爷子给出来这样的算法: 1. 初始化任意顺序 2. 从最后一张牌开始,设为第K张,然后从[1,K]张中任选一张与其交换 3. 从[1,K-1]张牌中任选一张和第K-1张交换... ... 伪代码:1234for(int i =n;i&gt;=1;i--)&#123; swap(array[i],random(1,i))&#125; 是不是贼简单？那为啥这个算法能做到呢？下面证明一下:假设现在有5张牌，初始顺序为$1,2,3,4,5$首先，从1~5(这是下标)中任选一张比如选到了2,那么用2和5交换得到$1,5,3,4,2$,也就是说，第一次交换2在最后一个位置的概率是1/5(也可以说任意一个数字在最后一个位置的概率是1/5),那么我们进行第二次交换,从1~4(这是下标)中任选一个和第4个交换,比如我们选到了1,在此之前要保证1没有在之前的步骤被选中也就是4/5,现在选中1的概率是1/4,那么两者相乘得到1/5,也就是1在第4个位置的概率是1/5也可以说任意一个数字在最后一个位置的概率是1/5),后面的不用多说了吧。这题和蓄水池采样算法由异曲同工之妙。用这个算法还可以在中途随意停止,比如有54张牌,我们要找任意10张牌进行公平洗牌,那么只需要上诉步骤执行10次就可以了. 轮盘赌随机算法$\\qquad$俄罗斯轮盘赌（Russian roulette）是一种残忍的赌博游戏。与其他使用扑克、色子等赌具的赌博不同的是，俄罗斯轮盘赌的赌具是左轮手枪和人的性命。俄罗斯轮盘赌的规则很简单：在左轮手枪的六个弹槽中放入一颗或多颗子弹，任意旋转转轮之后，关上转轮。游戏的参加者轮流把手枪对着自己的头，扣动板机；中枪的当然是自动退出，怯场的也为输，坚持到最后的就是胜者。旁观的赌博者，则对参加者的性命压赌注。$\\qquad$现在我们把问题抽象化，一个饼图，上面有一个指针，我们我们如何通过一个算法来确定到底每次拨动指针后会指向哪一个？ 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;ctime&gt;#include &lt;vector&gt;#include &lt;cstdlib&gt;using namespace std;#define N 999 //精度为小数点后面3位int RouletteWheelSelection()&#123; srand((unsigned) time(NULL)); float m = rand() % (N + 1) / (float)(N + 1); cout &lt;&lt; m &lt;&lt; endl; float Probability_Total = 0; int Selection = 0; vector&lt;float&gt; Probabilities = &#123;0.14, 0.35, 0.2, 0.07, 0.23, 0.01&#125;; for(int i = 0; i &lt; Probabilities.size(); i++) &#123; Probability_Total += Probabilities[i]; if(Probability_Total &gt;= m) &#123; Selection = i; break; &#125; &#125; return Selection;&#125;int main()&#123; cout &lt;&lt; RouletteWheelSelection(); return 0;&#125; 其实只要意识到需要最终各部分的概率与占比相同即可。 切分句子在NLP中经常会有这样的需求,对于训练数据有少部分会特别长,远远超出平均长度,那么我们就需要对句子进行拆分,但是不能直接安长度切，这样很可能会切断关键词,切分方法一般是在无用的地方切分,比如标点符号,现在给出一个算法实现这个功能:123456789101112def data_cut(sentence, cut_chars, cut_length, min_length): if len(sentence) &lt;= cut_length: return [sentence] else: for char in cut_chars: start = min_length # 防止直接从头几个就找到了，这样切的太短 end = len(sentence) - (min_length - 1) # 防止从最后几个找到了,这样切的也太短 if char in sentence[start:end]: index = sentence[start:end].index(char) return data_cut(sentence[:start + index], cut_chars, cut_length, min_length) + \\ data_cut(sentence[start + index:], cut_chars, cut_length, min_length) return [sentence] # 如果没有找到切分点那就不管句子长度直接返回 &gt;参数说明:sentence是一个list,内容是sentence每个字符 cut_chars是一个list,内容是切分字符，按优先级排序 cut_length是int类型，表示切分多长的句子,这个长度以及以下的直接返回 min_length:切分后子句子的最短长度return:二维list,注意，每个切分后的句子不一定全小于cut_length,有些另类的句子可能没有切分点,这样的需要手动处理eg:12345678910111213141516171819def data_cut(sentence, cut_chars, cut_length, min_length): if len(sentence) &lt;= cut_length: return [sentence] else: for char in cut_chars: start = min_length end = len(sentence) - (min_length - 1) if char in sentence[start:end]: index = sentence[start:end].index(char) return data_cut(sentence[:start + index], cut_chars, cut_length, min_length) + data_cut( sentence[start + index:], cut_chars, cut_length, min_length) return [sentence]if __name__ == &apos;__main__&apos;: sentence = &quot;三种上下文特征：单词、n-gram 和字符在词嵌入文献中很常用。大多数词表征方法本质上利用了词-词的共现统计，即使用词作为上下文特征（词特征）。受语言建模问题的启发，开发者将 n-gram 特征引入了上下文中。词到词和词到 n-gram 的共现统计都被用于训练 n-gram 特征。对于中文而言，字符（即汉字）通常表达了很强的语义。为此，开发者考虑使用词-词和词-字符的共现统计来学习词向量。字符级的 n-gram 的长度范围是从 1 到 4（个字符特征）。&quot; l = data_cut(list(sentence), [&apos;，&apos;, &apos;。&apos;], 150, 5) print(l)#自己运行看看吧，不好写 约瑟夫环 n个人拉成圈，按照1~n标号,从1开始报数,报到m的人出局，每一次有人出局后重新排号，1号是重新排号前m的下一个人. 12345int JosephRing(int n, int m)&#123; if (n == 1) return n; return (JosephRing(n - 1, m) + m - 1) % n + 1;&#125; 解释:对于任何一个状态而言，每个位置设为$old_i$,如果现在把该状态的第$m$个剔除掉，则重新排号后每个位置设为$new_i$,两者的关系 old_i = (new +m-1)%n+1$JosephRing(n,m)$表示有$n$个人,每次删除$m$号人最终剩下的号码，每次递归回来其实都是再算 点和线一个知识点定义: 平面上的三点$A(x1,y1),B(x2,y2),C(x3,y3)$的面积量: S(A,B,C)=\\frac{1}{2} \\left|\\begin{array}{} x_1 & y_1 & 1 \\\\ x_2 & y_2 & 1\\\\ x_3 & y_3 & 1 \\end{array}\\right|其中: 当A、B、C逆时针时S为正的,反之S为负的。证明如图: 也就是说，平面三点一定能写成一个直角梯形减两个直角三角形的形式.即: S(A,B,C)=\\frac{(x_1y_2+x_3y_1+x_2y_3-x_1y_3-x_2y_1-x_3y_2)}{2}正好是上诉行列式. 一个应用令矢量的起点为A，终点为B，判断的点为C，如果S(A，B，C)为正数，则C在矢量AB的左侧；如果S(A，B，C)为负数，则C在矢量AB的右侧；如果S(A，B，C)为0，则C在直线AB上 点和矩形 判断坐标系内某电点是否在某个矩形内部 只需要将点与四个角连接，计算形成的四个三角形面积(海伦公式)和是否等于矩形面积,等于则在内部，否则在外部。 判断平面内两线段相交 计算两线段所在的直线的的交点(如果有),然后看该交点是否在两条线段上即可 若两直线相交,则如图:,,,算了不弄图了，也简单，假设有线段AB 和 CD 若相交则必定C 和 D 在 AB 的两侧，则有$\\vec{AB} \\times \\vec{AC} $和$\\vec{AB} \\times \\vec{AD} $必定异号 $\\vec{AB} \\times \\vec{AC} $和$\\vec{CA} \\times \\vec{CD} $必定异号，判断这个即可","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[]},{"title":"二进制相关","slug":"Binary","date":"2019-06-23T14:49:15.000Z","updated":"2021-09-19T18:01:25.944Z","comments":true,"path":"2019/06/23/Binary/","link":"","permalink":"https://lingyixia.github.io/2019/06/23/Binary/","excerpt":"","text":"数组与$2^k$ 一个整形数组，每次取的元素和必须是$2^k$,问至少多少次能取完?假设一定能完成这个任务。 答:只需要把所有的数字加起来，然后换为二进制，里面有几个1就有几次.(找不到标准答案，感觉应该对)","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[]},{"title":"概率计算问题","slug":"machineProblem","date":"2019-06-23T11:14:33.000Z","updated":"2021-09-19T18:01:25.960Z","comments":true,"path":"2019/06/23/machineProblem/","link":"","permalink":"https://lingyixia.github.io/2019/06/23/machineProblem/","excerpt":"若干有趣的有关概率计算的小问题","text":"若干有趣的有关概率计算的小问题 抛硬币 一枚均匀硬币，平均抛多少次才能连续两次正面向上? 答:设N_k表示连续$k$次正面向上的随机变量,则E(N_k)表示连续$k$面朝上需要抛硬币次数的均值,则: N_k=N_{k-1}+p+(1-p)[1+N_k]下面解释这个式子.要想得到N_k次正面向上之前必须有N_{k-1}次正面向上,因此第一项的N_{k-1}就是这个意思,第二项的意思是达到$N_{k-1}$之的下一个伯努利实验如果成功,则实验到此为止。第三项的意思是如果不成功,则一切从头开始. E(N_k)=E(N_{k-1})+p+(1-p)[1+E(N_k)]得: E(N_k) = \\frac{E(N_{k-1})+1}{p}得: \\begin{align} E(N_K)&=\\frac{1}{p^N}+\\frac{1}{p^{N-1}}+...+\\frac{1}{p} \\\\ &=\\frac{p^N-1 }{p^N(p-1)} \\end{align}羊、车、门 有3扇关闭的门，一扇门后面停着汽车，另外两扇门后是山羊，只有主持人知道每扇门后面是什么。参赛者先选择一扇门，在开启它之前，主持人会开启另外一扇门，露出门后的山羊，然后如果你是参赛者，你换不换门? 正常人会认为换不换几率都是1/2,但是真的是这样么?现在我们把此题用解答一下: 如果换门，那么所有的情况只有三种: 参赛者挑山羊一号，主持人挑山羊二号。转换将成功 参赛者挑山羊二号，主持人挑山羊一号。转换将成功 参赛者挑汽车，主持人挑两头山羊的任何一头。转换将失败或者说不换门所有的情况只有三种: 参赛者挑山羊一号，主持人挑山羊二号。不换将失败 参赛者挑山羊二号，主持人挑山羊一号。不换则失败 参赛者挑汽车，主持人挑两头山羊的任何一头。转换将成功或者这样说:已知第一个步骤得到车的概率是1/3，那第二个步骤换门得到车的概率是0，—&gt; 1/3 0 =0已知第一个步骤得到羊的概率是2/3，那第二个步骤换门得到车的概率是1，—&gt; 2/3 1 =2/3总的来说，换门得到的概率是2/3 现在把这个问题普遍化，问题就是计算换门和不换门两种情况下成功的概率,假设$N$扇门中有一个车，其他的都是羊 不换门:成功的条件是第一次选择了车 设事件A=第一次选择了车 则$P(A)=\\frac{1}{N}$ 换门:成功的条件是第一次选择了羊 第二次选择了车 设事件B=第一次选择了羊，事件C=第二次选择了车 $P(C|B)=\\frac{N-1}{N} \\times \\frac{1}{N-2} = \\frac{N-1}{N-2} \\times \\frac{1}{N} &gt; \\frac{1}{N}$实例代码,更改N可以更改门的数量:1234567891011121314151617181920212223242526272829import time # 导入时间库from random import choiceTIMES = 100000 # 用来标记做了多少次选择che = 0 # 用来记录多少次选择了车yang = 0 # 用来记录多少次选了羊N = 5 # 标记多少个门time.clock() # 开始计时# for 循环用于选择并判断多少次选择了车for i in range(TIMES): l = [&apos;c&apos;] + [&apos;s&apos; + str(i + 1) for i in range(N - 1)] # eg: l = [&apos;c&apos;, &apos;s1&apos;, &apos;s2&apos;, &apos;s3&apos;, &apos;s4&apos;] # 选择内容，c代表车，sn代表第N只羊 first = choice(l) # 随机从中选择一个 if first == &apos;c&apos;: yang = yang + 1 # 当选择车时，主持人亮出一只羊，更改选择后就是选择了羊 else: l.remove(first) # 扔掉first选的的羊 sheeps = l.copy() sheeps.remove(&apos;c&apos;)#主持人在剩下的羊中选择一只 sheep = choice(sheeps) # 主持人选了这个羊 l.remove(sheep) second = choice(l) if second == &apos;c&apos;:#如果第二次选中了车 che = che + 1car = che / TIMES # 得出选择车的概率sheep = yang / TIMES # 得出选择羊的概率print(&quot;car =&quot;, car)print(&quot;sheep =&quot;, sheep) 圆上任取三点 圆上任取三点组成锐角三角形的概率是多少?此题可以转化为圆上任取三点不在同一半圆内的概率是多少? 假设先任取A和B，他们一定在同一半圆内,假设他们连接O组成的角$AOB$为X(x$\\in[0,\\pi]$)，则C点必定在OA和OB反向延长线的夹角内才能保证不在同一半圆内,该角度也必定为X,此时概率为:$\\frac{x}{2 \\pi}$ P= \\frac{1}{\\pi} \\times \\int_0^{\\pi} \\frac{x}{2 \\pi} dx = \\frac{1}{4}生日悖论至少有多少人才能保证有两人生日相同?答案是366，一点没错，但是我们改一下题:一个屋子里有23人，计算有两人生日相同的概率。 P = 1- \\frac{C_{365}^{23}}{365^{23}}≈1-0.493=0.507也就是说，当有23人的时候，就有50%的概率有两人生日相同,当人70人时，在通过上诉公示可以得到概率达到了99.9%,几乎是100%，也就是说，不需要366人，只需要70人基本上就可以断定至少两人生日相同了.","categories":[{"name":"数学","slug":"数学","permalink":"https://lingyixia.github.io/categories/数学/"}],"tags":[{"name":"概率论","slug":"概率论","permalink":"https://lingyixia.github.io/tags/概率论/"}]},{"title":"常见博弈游戏","slug":"gametheory","date":"2019-06-23T07:07:55.000Z","updated":"2021-09-19T18:01:25.958Z","comments":true,"path":"2019/06/23/gametheory/","link":"","permalink":"https://lingyixia.github.io/2019/06/23/gametheory/","excerpt":"","text":"Nim Game 有n堆石子，每堆石子有若干石子，两个人轮流从某一堆取任意多的物品，规定每次至少取一个，多者不限。取走最后石子的人获胜。","categories":[],"tags":[]},{"title":"常用分布","slug":"commondistribution","date":"2019-06-23T07:03:04.000Z","updated":"2021-09-19T18:01:25.957Z","comments":true,"path":"2019/06/23/commondistribution/","link":"","permalink":"https://lingyixia.github.io/2019/06/23/commondistribution/","excerpt":"常用分布若干知识点记录","text":"常用分布若干知识点记录 伯努利分布 进行一次实验,实验的结果只有两种:成功的概率是$p$,则失败的概率是$1-p$。eg:一个硬币抛一次人结果。 P(X=1)=p \\\\ P(X=0)=1-p期望:$E(X)=P$方差:$D(X)=P \\times (1-P)$ 二项分布 n次伯努利实验,成功k次。g:一个硬币抛n次，k次正面朝上。 P(X=k)=C_n^kp^{k}(1-p)^{n-k}期望: \\begin{aligned} E(X) &=\\sum_{k=0}^nk C_n^kp^{k}(1-p)^{n-k} \\\\ &=\\sum_{k=0}^nk \\frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k} \\\\ &=n\\sum_{k=0}^n \\frac{(n-1)!}{(k-1)!(n-k)!} p^{k}(1-p)^{n-k} \\\\ &= np\\sum_{k=0}^n C_{n-1}^{k-1}p^{k-1}(1-p)^{(n-1)-(k-1)} \\\\ &=np(p+(1-p))^{n-1} \\\\ &=np \\end{aligned} D(X)=np(1-p)过两天在证明 几何分布 考虑伯努利试验，每次成功的概率为$p$,$0&lt;p&lt;1$,重复试验直到试验首次成功为止。令X表示需要试验的次数，那么: P(X=n)=(1−p)^{n−1}p $E(X)=\\frac{1}{p}$ $D(X)=\\frac{1-p}{p^2}$参考 补充: \\begin{aligned} D(X) &= E(X-E(X)^2) \\\\ &=E(X^2-2XE(X)+E(X)^2) \\\\ &=E(X^2)-2E(x)^2+E(X)^2 \\\\ &=E(X^2)-E(X)^2 \\end{aligned}","categories":[{"name":"数学","slug":"数学","permalink":"https://lingyixia.github.io/categories/数学/"}],"tags":[{"name":"概率论","slug":"概率论","permalink":"https://lingyixia.github.io/tags/概率论/"}]},{"title":"数组常用算法","slug":"array","date":"2019-06-23T04:12:57.000Z","updated":"2021-09-19T18:01:25.949Z","comments":true,"path":"2019/06/23/array/","link":"","permalink":"https://lingyixia.github.io/2019/06/23/array/","excerpt":"","text":"两排序数组最大差值 时间复杂度能达到O(n),未排序数组先排序即可 12345678910111213141516171819202122232425int computeMinimumDistanceB(vector&lt;int&gt; a, vector&lt;int&gt; b)&#123; int aindex = 0; int bindex = 0; int min = abs(a[0] - b[0]); while (true) &#123; if (a[aindex] &gt; b[bindex]) &#123; bindex++; &#125; else &#123; aindex++; &#125; if (aindex &gt;= a.size() || bindex &gt;= b.size()) &#123; break; &#125; if (abs(a[aindex] - b[bindex]) &lt; min) &#123; min = abs(a[aindex] - b[bindex]); &#125; &#125; return min;&#125; 有序数组并集交集12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;vector&lt;int&gt; aggregate(vector&lt;int&gt; A, vector&lt;int&gt; B)&#123; vector&lt;int&gt; result; int pointA = 0; int pointB = 0; while (pointA &lt; A.size() &amp;&amp; pointB &lt; B.size()) &#123; if (A[pointA] &lt; B[pointB]) &#123; result.push_back(A[pointA++]); &#125; else if (A[pointA] &gt; B[pointB]) &#123; result.push_back(B[pointB++]); &#125; else &#123; result.push_back(A[pointA++]); pointB++; &#125; &#125; return result;&#125;vector&lt;int&gt; intersection(vector&lt;int&gt; A, vector&lt;int&gt; B)&#123; vector&lt;int&gt; result; int pointA = 0; int pointB = 0; while (pointA &lt; A.size() &amp;&amp; pointB &lt; B.size()) &#123; if (A[pointA] &lt; B[pointB]) &#123; pointA++; &#125; else if (A[pointA] &gt; B[pointB]) &#123; pointB++; &#125; else &#123; result.push_back(A[pointA]); pointA++; pointB++; &#125; &#125; return result;&#125;int main()&#123; vector&lt;int&gt; A = &#123;1, 3, 4, 5, 7&#125;; vector&lt;int&gt; B = &#123;2, 3, 5, 8, 9&#125;; vector&lt;int&gt; result = aggregate(A, B); for (auto r:result) &#123; cout &lt;&lt; r &lt;&lt; endl; &#125; return 0;&#125; 找到所有数组中消失的数字 要我说这就是个’伪’O(n) 1234567891011121314151617181920vector&lt;int&gt; findDisappearedNumbers(vector&lt;int&gt; &amp;nums)&#123; vector&lt;int&gt; result; for (int i = 0; i &lt; nums.size(); ++i) &#123; if (nums[nums[i] - 1] != nums[i]) &#123; swap(nums[nums[i] - 1], nums[i]); i--; &#125; &#125; for (int i = 0; i &lt; nums.size(); ++i) &#123; if (nums[i] != i + 1) &#123; result.push_back(i + 1); &#125; &#125; return result;&#125; Ksum 数组中K个数相加为target(不定几个用背包) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; result;void kSum(vector&lt;int&gt; &amp;nums, int k, int start, int end, int target, vector&lt;int&gt; current)&#123; if (k == 2) &#123; int low = start; int high = end; while (low &lt; high) &#123; if (nums[low] + nums[high] == target) &#123; current.push_back(nums[low]); current.push_back(nums[high]); result.push_back(current); while (low &lt; end &amp;&amp; nums[low + 1] == nums[low]) low++; while (low &lt; end &amp;&amp; nums[high - 1] == nums[high]) high--; low++; high--; current.pop_back(); current.pop_back(); &#125; else if (nums[low] + nums[high] &lt; target) &#123; low++; &#125; else &#123; high--; &#125; &#125; &#125; else &#123; for (int i = start; i &lt;= end - (k - 1); ++i) &#123; if (i &gt; start &amp;&amp; nums[i] == nums[i - 1])continue; current.push_back(nums[i]); kSum(nums, k - 1, i + 1, end, target - nums[i], current); current.pop_back(); &#125; &#125;&#125;vector&lt;vector&lt;int&gt;&gt; fourSum(vector&lt;int&gt; &amp;nums, int target)&#123; vector&lt;int&gt; current; sort(nums.begin(), nums.end()); kSum(nums, 4, 0, nums.size() - 1, target, current); return result;&#125;&#125;; 数组中右面第一个比它大的距离123456789101112131415161718192021222324 vector&lt;int&gt; dailyTemperatures(vector&lt;int&gt; T)&#123; vector&lt;int&gt; result(T.size()); stack&lt;int&gt; record; int pos = 0; while (pos &lt; T.size()) &#123; if (record.empty()) &#123; record.push(pos++); &#125; else &#123; if (T[pos] &lt;= T[record.top()]) &#123; record.push(pos++); &#125; else &#123; result[record.top()] = pos-record.top(); record.pop(); &#125; &#125; &#125; return result;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[]},{"title":"DFSandBFS","slug":"DFSandBFS","date":"2019-06-23T02:20:44.000Z","updated":"2021-09-19T18:01:25.945Z","comments":true,"path":"2019/06/23/DFSandBFS/","link":"","permalink":"https://lingyixia.github.io/2019/06/23/DFSandBFS/","excerpt":"","text":"稍微总结一下DFS和BFS,一般情况下DFS用递归，BFS用队列 有向图的遍历DFS1234567891011121314151617181920212223242526272829303132333435363738#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;using namespace std;void dfs(vector&lt;bool&gt; &amp;visisted, vector&lt;vector&lt;int&gt;&gt; &amp;neibourhoods, int index)&#123; visisted[index] = true; cout &lt;&lt; index &lt;&lt; &quot;--&gt;&quot;; for (int i = 0; i &lt; neibourhoods[index].size(); ++i) &#123; if (!visisted[neibourhoods[index][i]]) &#123; dfs(visisted, neibourhoods, neibourhoods[index][i]); &#125; &#125;&#125;void dfsInit(int numCourses, vector&lt;vector&lt;int&gt;&gt; &amp;neibourhoods)&#123; vector&lt;bool&gt; visisted(numCourses, false); for (int i = 0; i &lt; numCourses; ++i) &#123; if (!visisted[i]) &#123; dfs(visisted, neibourhoods, i); &#125; &#125;&#125;int main()&#123; vector&lt;vector&lt;int&gt;&gt; neibourhoods = &#123;&#123;1, 3&#125;,&#123;2, 3&#125;,&#123;&#125;,&#123;4&#125;,&#123;5, 6&#125;,&#123;2&#125;,&#123;3&#125;&#125;; dfsInit(7, neibourhoods); return 0;&#125; BFS 其实就是层序遍历 12345678910111213141516171819202122232425262728293031323334#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;using namespace std;void bfs(int numCourses, vector&lt;vector&lt;int&gt;&gt; &amp;neibourhoods)&#123; vector&lt;bool&gt; visisted(numCourses, false); queue&lt;int&gt; q; q.push(0);//其实就是树的层序遍历，首先需要知道一个入度为零的点 visisted[0] = true; while (!q.empty()) &#123; int current = q.front(); cout &lt;&lt; current &lt;&lt; &quot;--&gt;&quot;; q.pop(); for (auto node:neibourhoods[current]) &#123; if (!visisted[node]) &#123; q.push(node); visisted[node] = true; &#125; &#125; &#125;&#125;int main()&#123; vector&lt;vector&lt;int&gt;&gt; neibourhoods = &#123;&#123;1, 3&#125;,&#123;2, 3&#125;,&#123;&#125;,&#123;4&#125;,&#123;5, 6&#125;,&#123;2&#125;,&#123;3&#125;&#125;; bfs(7, neibourhoods); return 0;&#125; 迷宫最短路径问题dfs1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;vector&lt;vector&lt;int&gt;&gt; directions = &#123;&#123;0, 1&#125;, &#123;0, -1&#125;, &#123;1, 0&#125;, &#123;-1, 0&#125;&#125;;vector&lt;vector&lt;int&gt;&gt; finalPath;void findShortPath_DFS(vector&lt;vector&lt;int&gt;&gt; &amp;maze, vector&lt;vector&lt;int&gt;&gt; &amp;currentPath, int startX, int startY, int endX, int endY)&#123; if (startX == endY &amp;&amp; startY == endY) &#123; finalPath = finalPath.size() &lt; currentPath.size() &amp;&amp; !finalPath.empty() ? finalPath : currentPath; &#125; maze[startX][startY] = 1; for (auto direction:directions) &#123; int currentX = startX + direction[0]; int currentY = startY + direction[1]; if (currentX &gt;= 0 &amp;&amp; currentX &lt; maze.size() &amp;&amp; currentY &gt;= 0 &amp;&amp; currentY &lt; maze[0].size() &amp;&amp; maze[currentX][currentY] == 0) &#123; currentPath.push_back(vector&lt;int&gt;&#123;currentX, currentY&#125;); findShortPath_DFS(maze, currentPath, currentX, currentY, endX, endY); currentPath.pop_back(); maze[currentX][currentY] = 0; &#125; &#125;&#125;int main()&#123; vector&lt;vector&lt;int&gt;&gt; maze = &#123;&#123;0, 0, 1, 0, 0&#125;, &#123;0, 0, 0, 0, 0&#125;, &#123;0, 0, 0, 1, 0&#125;, &#123;1, 1, 0, 1, 1&#125;, &#123;0, 0, 0, 0, 0&#125;&#125;; vector&lt;vector&lt;int&gt;&gt; currentPath; int startX = 0; int startY = 0; int endX = 4; int endY = 4; findShortPath_DFS(maze, currentPath, startX, startY, endX, endY); for (auto pos:finalPath) &#123; cout &lt;&lt; pos[0] &lt;&lt; &quot; &quot; &lt;&lt; pos[1] &lt;&lt; endl; &#125; return 0;&#125; bfs12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;struct Point&#123; int x; int y; Point() : x(0), y(0) &#123;&#125; Point(int x, int y) : x(x), y(y) &#123;&#125;&#125;;vector&lt;vector&lt;int&gt;&gt; directions = &#123;&#123;0, 1&#125;, &#123;0, -1&#125;, &#123;1, 0&#125;, &#123;-1, 0&#125;&#125;;void findShortPath_BFS(vector&lt;vector&lt;int&gt;&gt; &amp;maze, vector&lt;vector&lt;Point&gt;&gt; &amp;pre, int startX, int startY, int endX, int endY)&#123; queue&lt;Point&gt; q; q.push(Point(startX, startY)); maze[startX][startY] = 1; while (!q.empty()) &#123; Point current = q.front(); q.pop(); if (current.x == endX &amp;&amp; current.y == endY) &#123; break; &#125; for (auto direction:directions) &#123; int currentX = current.x + direction[0]; int currentY = current.y + direction[1]; if (currentX &gt;= 0 &amp;&amp; currentX &lt; maze.size() &amp;&amp; currentY &gt;= 0 &amp;&amp; currentY &lt; maze[0].size() &amp;&amp; maze[currentX][currentY] == 0) &#123; q.push(Point(currentX, currentY)); maze[currentX][currentY] = 1; pre[currentX][currentY] = current; &#125; &#125; &#125;&#125;void print(Point point, vector&lt;vector&lt;Point&gt;&gt; &amp;pre)&#123; if (point.x == 0 &amp;&amp; point.y == 0) &#123; cout &lt;&lt; point.x &lt;&lt; &quot; &quot; &lt;&lt; point.y &lt;&lt; endl; return; &#125; print(pre[point.x][point.y], pre); cout &lt;&lt; point.x &lt;&lt; &quot; &quot; &lt;&lt; point.y &lt;&lt; endl;&#125;int main()&#123; vector&lt;vector&lt;int&gt;&gt; maze = &#123;&#123;0, 0, 1, 0, 0&#125;, &#123;0, 0, 0, 0, 0&#125;, &#123;0, 0, 0, 1, 0&#125;, &#123;1, 1, 0, 1, 1&#125;, &#123;0, 0, 0, 0, 0&#125;&#125;; vector&lt;vector&lt;Point&gt;&gt; pre(5, vector&lt;Point&gt;(5)); int startX = 0; int startY = 0; int endX = 4; int endY = 4; findShortPath_BFS(maze, pre, startX, startY, endX, endY); print(Point(endX, endY), pre); return 0;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"DFS","slug":"DFS","permalink":"https://lingyixia.github.io/tags/DFS/"},{"name":"BFS","slug":"BFS","permalink":"https://lingyixia.github.io/tags/BFS/"}]},{"title":"繁琐记录","slug":"personal","date":"2019-06-22T14:06:41.000Z","updated":"2021-09-19T18:01:25.963Z","comments":true,"path":"2019/06/22/personal/","link":"","permalink":"https://lingyixia.github.io/2019/06/22/personal/","excerpt":"The article has been encrypted, please enter your password to view.","text":"Nothing Valuable Incorrect Password! No content to display! U2FsdGVkX1+oOgxCtOldHMh9PJ4agE9nH7MyVr33g42D2tupiymSrdA+QUI+i9Kd5AGYpCOVU6klcHiDufCyKSFqzIZrBBSf5p2MM0hvQGj/Mj+Gvo2Js7mpDkBBxMgG/kw29WGsZKb3LBMi5p2WLwwvMzAAVNzPu9drR+wpfNaIBA3Ymx5k4qH28E1ONaQNAq9UfwXqJCEvILlhvaFPqJqhaJEagSuEuS4TEIcuqvz7amKVRbzk+NyONabqO15vk02KoehA+4dX4VzsPRShL7m0Pr7JbAhAA11r2mI93lU2A/IjU3xOkgl+uGfpQNntLxa+vJc5rg4VLeoslPtIemhc5gzKvip5U00/a9jK+HXSVc1XD0D2jZo3zTtTvUxXHeokiWrkFcpE/2vzXiytv83r2HIItNmIR1rw5hgxYYGSD8n4GNT6vzLyjyq8aP+p8DQx9V4QhEyUvo8gdkhasbHYDyXQESGLHQFI3lIo9IOnOaznltvkhNNJJ32Dw6wkUCou7UBXHXJAfnXsEQmpdoxl0hiWOkNoelXazFTSSmYAcO03ZTvjBww9R7Cpt3tLUxukSM192vH2LVgqIjYlXsx34/ofQrwQy5kA6zjNHWQZIRXPYlsa+SPSur1I7xH+N40c4D+3iOdFsl4G8uj+1GIWBGWVxgd4gbsKAGps5AHIKWDgypvNFrBKfzQ7hl8/i6p0JgWZoyZ3+/A/FmxpxgIfNnvDjge5cZNOpCIKZJUaPk69qmv0RhXlv5ksk+5dojU5pMZQ26hgjvxTEUGUk0JzDqd9icOtJUByXJgcDmDqwuNKCIsbBaxHe9vWflzSttrk+JC883CvXb26LjONCD4elIlxShU58PvlRtmmUdq2F7VvZaQ5CzZ1cnXoJ2CWejzqO099Blaq6XVPO2QBRpNJJPXlZX/TsaxRSndKIoQgy8BXojlanW0SvhSfis7fmJ48VG5/v3vwm3cl2S8+KLvsRX8GLf/2mFxMpY67RiATfBm7SOt51nIPm+c2H7Fouh16Yy5r7SFfhzRF20EFDH+6gdJIAOPtiXCsffVO7cyszOEn+W0YdIfyMiHClQ2pXUWhXdftjOEkGF9oCbiKGkYtkSc0fGb7bvWGxCrGtB+rkwaABpuJY8cB8KP0f00p1yN0zmO0X1V20WDx+jnJjWh8Pz2LhK+ITWNuSKX/ydEUJd0oCkKL4MqW2kTL0AzIwtqKT/y8aFE9wBK6qK5XVUlcRQXIEaSaTmWJzJTuPqUKxQ2Vf2VMrWLti8enZky6kpWpUC8GlIdqXGQ8zKU9U6jyyyhbhNCSIgRitRAmVxm8mFXm4+caxM+z2YcMezkD3UOVAEeKXyBcYhKlHVeyR2Y8UM+KyjwzlakjF+0TORd7UqqMB9HC8JK2HqAbsVMbeUtdnExUumes85/9RCHswrcDJHnTFvO9FFtMWJOKLiNYT+kQYy2dNO/1Dygj1yYB","categories":[{"name":"个人","slug":"个人","permalink":"https://lingyixia.github.io/categories/个人/"}],"tags":[{"name":"繁琐记录","slug":"繁琐记录","permalink":"https://lingyixia.github.io/tags/繁琐记录/"}]},{"title":"股票利润问题","slug":"stockProblem","date":"2019-06-22T10:50:47.000Z","updated":"2021-09-19T18:01:25.966Z","comments":true,"path":"2019/06/22/stockProblem/","link":"","permalink":"https://lingyixia.github.io/2019/06/22/stockProblem/","excerpt":"","text":"动态规划中的股票问题汇总记录 一次买卖1234567891011121314151617//第一种方法：记录到i-1为止，最小下标，然后比较prices[i]-prices[currintMinIndex]int maxProfit(vector&lt;int&gt;&amp; prices) &#123; int currintMinIndex = 0; int currentMax = 0; int maxSum = 0; for(int i = 1;i&lt;prices.size();i++) &#123; if(prices[i-1]&lt;prices[currintMinIndex]) &#123; currintMinIndex = i-1; &#125; currentMax = prices[i] - prices[currintMinIndex]; maxSum = maxSum&gt;currentMax?maxSum:currentMax; &#125; return maxSum;&#125; 容易理解的写法 123456789101112int maxProfit(vector&lt;int&gt; &amp;prices)&#123; if(prices.empty()) return 0; int maxProfits = INT_MIN; int currentMinPirce = prices[0]; for (int i = 1; i &lt; prices.size(); ++i) &#123; maxProfits = max(maxProfits, prices[i] - currentMinPirce); currentMinPirce = min(currentMinPirce, prices[i]); &#125; return maxProfits&gt;0?maxProfits:0;&#125; 123456789101112131415//第二种方法：计算prices[i]-price[i-1],然后题目就可以转为连续数组最大值问题int maxProfit(vector&lt;int&gt;&amp; prices) &#123; int currentProfit = 0; int currentMaxProfit=0; int maxProfit = 0;//此处不能是INT_MIN，因为有可能[7,6,4,3,1]，即一直下降，此时可以选择不买不买，即利润最低为0 for(int i = 1;i&lt;prices.size();i++) &#123; currentProfit = prices[i]-prices[i-1]; currentMaxProfit += currentProfit; maxProfit = currentMaxProfit&gt;maxProfit?currentMaxProfit:maxProfit; currentMaxProfit = currentMaxProfit&lt;0?0:currentMaxProfit; &#125; return maxProfit&gt;0;&#125; 不限次数(https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/)12345678910111213int maxProfit(vector&lt;int&gt;&amp; prices) &#123; if(prices.empty()) return 0; int result=0; for(int i =1;i&lt;prices.size();i++) &#123; if(prices[i]&gt;prices[i-1]) &#123; result+=prices[i]-prices[i-1]; &#125; &#125; return result; &#125; 两次买卖 最多两次买卖 12345678910111213int maxProfit(vector&lt;int&gt; &amp;prices)&#123; if (prices.empty()) return 0; int sell1=0, sell2=0, buy1 = INT_MIN, buy2 = INT_MIN; for (int i = 0; i &lt; prices.size(); ++i) &#123; buy1 = max(buy1, -prices[i]); sell1 = max(sell1, prices[i] + buy1); buy2 = max(buy2, sell1 - prices[i]);//如果sell1=prices[i] + buy1,在此处减去prices[i]就相当于改点没卖，提现了&quot;最多两次&quot; sell2 = max(sell2, prices[i] + buy2); &#125; return sell2;&#125; K次买卖 动态规划:global[i][j]的意义是第i天以及之前最多进行j次交易所能获得的最大利润,local[i][j]的意义是在第i天以及之前最多进行j次交易且在第i天卖出股票所能得到的最大利润(如果global[i][j]是在第i天卖出,则global[i][j]=local[i][j]),地推公式为: \\begin{align} diff &= prices[i] - prices[i - 1] \\\\ local[i][j] &= max(global[i - 1][j - 1] + max(diff, 0), local[i - 1][j] + diff)\\\\ global[i][j] &= max(global[i - 1][j], local[i][j]) \\end{align}解释:local公式:$global[i-1][j-1]+max(diff,0)$的意思是global[i-1][j-1]必然不可能在第i-1天买入(买入就没办法卖出了,利润自然要小),如果diff&gt;0则在加上i-1天买,第i天卖的利润(一共j次交易)，否则就不算了(一共j-1次交易)。$local[i - 1][j] + diff$的意思是$第i-1天不卖了,改为第i天卖.gobal公式较简单,不解释. 12345678910111213141516 int maxProfitK(int k, vector&lt;int&gt; &amp;prices)&#123; if(prices.size()&lt;2) return 0; vector&lt;vector&lt;int&gt;&gt; local(prices.size(), vector&lt;int&gt;(k+1, 0)); vector&lt;vector&lt;int&gt;&gt; global(prices.size(), vector&lt;int&gt;(k+1, 0)); for (int i = 1; i &lt; prices.size(); ++i) &#123; int diff = prices[i] - prices[i - 1]; for (int j = 1; j &lt;= k; ++j) &#123; local[i][j] = max(global[i - 1][j - 1] + max(diff, 0), local[i - 1][j] + diff); global[i][j] = max(global[i - 1][j], local[i][j]); &#125; &#125; return global[prices.size() - 1][k];&#125; 空间优化 1234567891011121314151617181920int maxProfitK(int k, vector&lt;int&gt; &amp;prices)&#123; if(prices.size()&lt;2) return 0; vector&lt;int&gt; local(k + 1); vector&lt;int&gt; global(k + 1); vector&lt;int&gt; localCopy; vector&lt;int&gt; globalCopy; for (int i = 1; i &lt; prices.size(); ++i) &#123; localCopy = local; globalCopy = global; int diff = prices[i] - prices[i - 1]; for (int j = 1; j &lt;= k; ++j) &#123; local[j] = max(globalCopy[j - 1] + max(diff, 0), localCopy[j] + diff); global[j] = max(global[j], local[j]); &#125; &#125; return global[k];&#125; 注意,当k&gt;prices.size()/2的时候其实就退化成了不限次数问题，因此这里可以做一些优化. 股票冷冻期 Leetcode309动态规划:buy[i]数组:第i天以及之前最后一个操作是buy所能得到的最大利润sell[i]数组:第i天以及之前最后一个操作是sell所能得到的最大利润cooldown[i]数组:第i天以及之前最后一个操作是cooldown所能得到的最大利润地推公式为: \\begin{align} buy[i] &= max(buy[i - 1], cooldown[i - 1] - prices[i]); \\\\ sell[i] &= max(sell[i - 1], buy[i] + prices[i]); \\\\ cooldown[i] &= max(cooldown[i - 1], max(buy[i - 1], sell[i - 1])); \\\\ \\end{align}解释:每一个公式的前一项都表示在第i天不进行相应的操作会怎样,第二项表示在第i天进行相应的操作会怎样.buy:第一项表示如果在第i天不buy,则自然$buy[i]=buy[i-1]$,如果在第i天buy,则buy之前的状态必然是cooldown,因此$buy[i]=cooldown[i-1]-prices[i]$sell:第一项表示如果在第i天不sell,则自然$sell[i]=sell[i-1]$,如果在第i天sell,则sell之前的状态必然是buy,因此$sell[i]=buy[i-1]+prices[i]$cooldown:第一项表示如果在第i天cooldown,则自然$cooldown[i]=cooldown[i-1]$,如果在第i天cooldown,则cooldown[i]=max(sell[i-1],buy[i-1]) 123456789101112131415 int maxProfit(vector&lt;int&gt; &amp;prices)&#123; if(prices.size()&lt;2) return 0; vector&lt;int&gt; buy(prices.size()); vector&lt;int&gt; sell(prices.size()); vector&lt;int&gt; cooldown(prices.size()); buy[0]=-prices[0]; for (int i = 1; i &lt; prices.size(); ++i) &#123; buy[i] = max(buy[i - 1], cooldown[i - 1] - prices[i]); sell[i] = max(sell[i - 1], buy[i] + prices[i]); cooldown[i] = max(cooldown[i - 1], max(buy[i - 1], sell[i - 1])); &#125; return sell.back();//利润最高的最后一次操作必然是sell&#125; 由于$cooldown[i] = sell[i-1]$(cooldown[i]的意思是i以及之前的最后一次操作是cooldown,也就是i-1以及之前的最后一次操作是sell,也就是sell[i-1])地推公式可以归位两个: \\begin{align} buy[i] &= max(sell[i-2] - price, buy[i-1]) \\\\ sell[i] &= max(buy[i-1] + price, sell[i-1]) \\end{align} 123456789101112131415 int maxProfit(vector&lt;int&gt; &amp;prices)&#123; if(prices.size()&lt;2) return 0; vector&lt;int&gt; buy(prices.size()); vector&lt;int&gt; sell(prices.size()); buy[0]=-prices[0]; sell[0]=0; buy[1]=-(prices[0]&gt;prices[1]?prices[1]:prices[0]); sell[1]=prices[1]-prices[0]&gt;0?prices[1]-prices[0]:0; for (int i = 2; i &lt; prices.size(); ++i) &#123; buy[i] = max(buy[i - 1], sell[i - 2] - prices[i]); sell[i] = max(sell[i - 1], buy[i] + prices[i]); &#125; return sell.back(); 最后优化 123456789101112int maxProfit(vector&lt;int&gt;&amp; prices) &#123; int buy = INT_MIN, pre_buy = 0, sell = 0, pre_sell = 0; for (int price : prices) &#123; pre_buy = buy; buy = max(pre_sell - price, pre_buy); pre_sell = sell; sell = max(pre_buy + price, pre_sell); &#125; return sell;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"https://lingyixia.github.io/tags/leetcode/"},{"name":"动态规划","slug":"动态规划","permalink":"https://lingyixia.github.io/tags/动态规划/"}]},{"title":"Python可视化","slug":"pythonVisual","date":"2019-06-16T08:07:28.000Z","updated":"2021-09-19T18:01:25.963Z","comments":true,"path":"2019/06/16/pythonVisual/","link":"","permalink":"https://lingyixia.github.io/2019/06/16/pythonVisual/","excerpt":"常用数据可视化方式参考","text":"常用数据可视化方式参考 折线图 Line Chart 用于观察数据和比较走势，比如横坐标时月份，纵坐标是销售额 12345678910111213import pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltif __name__ == &apos;__main__&apos;: A = [0, 10, 20, 10, 0, 10, 20, 30, 5, 0, 10, 20, 10, 0, 20] B = [0, 1.0, 2.9, 3.61, 3.249, 3.9241, 5.5317, 7.9785, 7.6807, 6.9126, 7.2213, 8.4992, 8.6493, 7.7844, 8.0059] C = [0, 10.0, 15.2632, 13.321, 9.4475, 9.5824, 11.8057, 15.2932, 13.4859, 11.2844, 11.0872, 12.3861, 12.0536, 10.4374, 10.38077] data = pd.DataFrame(&#123;&apos;A&apos;: A, &apos;B&apos;: B, &apos;C&apos;: C&#125;) # data.plot.line() sns.lineplot(data=data) plt.show() 注释部分是使用seaborn绘图得到 柱状图(条形图) Bar Chart 柱状图(条形图)表示数据的大小.和Line Chart其实可以是一样的数据,不过Bar Chart重在观察和比较数据之间的差距 1234567891011import pandas as pdimport matplotlib.pyplot as pltif __name__ == &apos;__main__&apos;: A = [0, 10, 20, 10, 0, 10, 20, 30, 5, 0, 10, 20, 10, 0, 20] B = [0, 1.0, 2.9, 3.61, 3.249, 3.9241, 5.5317, 7.9785, 7.6807, 6.9126, 7.2213, 8.4992, 8.6493, 7.7844, 8.0059] C = [0, 10.0, 15.2632, 13.321, 9.4475, 9.5824, 11.8057, 15.2932, 13.4859, 11.2844, 11.0872, 12.3861, 12.0536, 10.4374, 10.3807] data = pd.DataFrame(&#123;&apos;A&apos;: A, &apos;B&apos;: B, &apos;C&apos;: C&#125;) data.plot.bar() plt.show() data.plot.bar()横向柱状图,bar(stacked=True)堆叠柱状图 直方图 Histgram 直方图表示数据的多少.和Bar Chart的区别是Histgram统计的是数据数量的多少,它的纵坐标和数据的大小无关,只和数据的多少有关.通常横坐标表示某个数据,纵坐标表示该数据数据数量。 1234567891011import pandas as pdimport matplotlib.pyplot as pltif __name__ == &apos;__main__&apos;: A = [0, 10, 20, 10, 0, 10, 20, 30, 5, 0, 10, 20, 10, 0, 20] B = [0, 1.0, 2.9, 3.61, 3.249, 3.9241, 5.5317, 7.9785, 7.6807, 6.9126, 7.2213, 8.4992, 8.6493, 7.7844, 8.0059] C = [0, 10.0, 15.2632, 13.321, 9.4475, 9.5824, 11.8057, 15.2932, 13.4859, 11.2844, 11.0872, 12.3861, 12.0536, 10.4374, 10.38077] data = pd.DataFrame(&#123;&apos;A&apos;: A, &apos;B&apos;: B, &apos;C&apos;: C&#125;) data.hist() plt.show() 其他图 未完待续… …","categories":[{"name":"数据科学","slug":"数据科学","permalink":"https://lingyixia.github.io/categories/数据科学/"}],"tags":[]},{"title":"方向导数和梯度","slug":"derivative","date":"2019-06-14T08:54:10.000Z","updated":"2021-09-19T18:01:25.958Z","comments":true,"path":"2019/06/14/derivative/","link":"","permalink":"https://lingyixia.github.io/2019/06/14/derivative/","excerpt":"方向导数和梯度","text":"方向导数和梯度 方向导数 方向导数是一个数值 函数$f(x,y)$在点$(x_0,y_0)$处沿$ \\overrightarrow{l}$方向的方向导数定义为: \\frac{\\partial f}{\\partial l}=\\lim_{t \\rightarrow 0^+} \\frac{f(x_0+t\\cos \\alpha,y_0+tcos \\beta) - f(x_0,y_0)}{t} \\tag{1}其中$\\alpha,\\beta,\\gamma$分别是$\\overrightarrow{l}$三个相应方向的余弦值.(1)公是方向导数的定义,要证明的是该式子等于: \\frac{\\partial f}{\\partial l}=f_x(x_0,y_0) \\cos \\alpha+f_y(x_0,y_0) \\cos \\beta \\tag{2}证明: \\begin{align} \\frac{\\partial f}{\\partial l} &=\\lim_{t\\rightarrow0^+}\\frac{f(x_0+t\\cos \\alpha,y_0+tcos \\beta) - f(x_0,y_0)}{t} \\\\ &=\\lim_{t\\rightarrow 0^+} \\frac{f(x_0+t\\cos \\alpha,y_0+t\\cos\\beta)-f(x_0,y_0+t\\cos \\beta)}{t}+\\frac{f(x_0,y_0+t\\cos \\beta) -f(x_0,y_0)}{t}\\\\ &=\\lim_{t\\rightarrow 0^+}\\frac{f_x(\\xi_x,y_0+t\\cos\\beta)t\\cos\\alpha}{t} + \\frac{f_y(x_0,\\xi_y)t\\cos\\beta}{t} \\qquad \\xi_x\\in[x_0,x_0+t\\cos\\alpha] \\quad \\xi_y\\in[y_0,y_0+tcos\\beta](拉格朗日中值定理) \\\\ &=\\lim_{t\\rightarrow 0 +}f_x(\\xi_x,y_0+t\\cos\\beta) \\cos\\alpha + f_y(x_0,\\xi_y)\\cos\\beta \\\\ &=f_x(x_0,y_0)\\cos\\alpha+f_y(x_0,y_0)\\cos\\beta \\end{align} 方向导数的直观意义是函数在该点的偏导数向量在此方向上的投影 \\frac{\\partial f}{\\partial l} =(f_x(x_0,y_0),f_y(x_0,y_0)) ·（\\cos\\alpha, \\cos\\beta） = |(cos\\alpha,cos\\beta)|·|(f_x(x_0,y_0),f_y(x_0,y_0))| \\cos \\theta 梯度 是一个向量 grad f(x_0,y_0)=(f_x(x_0,y_0),f_y(x_0,y_0))方向导数和梯度 \\frac{\\partial f}{\\partial l}=gradf(x_0,y_0)· \\overrightarrow{l}ss","categories":[{"name":"数学","slug":"数学","permalink":"https://lingyixia.github.io/categories/数学/"}],"tags":[]},{"title":"权利的游戏","slug":"gameofThrones","date":"2019-06-10T08:09:49.000Z","updated":"2021-09-19T18:01:25.958Z","comments":true,"path":"2019/06/10/gameofThrones/","link":"","permalink":"https://lingyixia.github.io/2019/06/10/gameofThrones/","excerpt":"从权利的游戏学到了什么(未完待续… …)","text":"从权利的游戏学到了什么(未完待续… …) 见多识广有总有好处。至今为止,我发现权游中的人物各个都见多识广,好多人物比如弑君者,只问了布蕾妮的名字就能知道他父亲的名字,祖籍何处。再比如凯特琳,作为一个家庭妇女的角色,在抓捕被她误会的提利昂的客栈，竟然能同时认出那么多人,就好像多年未见的朋友一样,也正因此才能得到众人的帮助，成功抓获提利昂。权游中每一个人物几乎都有这样的特点,也许这是这些家族的家教如此,但确实值得学习。","categories":[{"name":"影视","slug":"影视","permalink":"https://lingyixia.github.io/categories/影视/"}],"tags":[]},{"title":"德行","slug":"morals","date":"2019-06-09T13:43:43.000Z","updated":"2021-09-19T18:01:25.961Z","comments":true,"path":"2019/06/09/morals/","link":"","permalink":"https://lingyixia.github.io/2019/06/09/morals/","excerpt":"The article has been encrypted, please enter your password to view.","text":"Nothing Valuable Incorrect Password! No content to display! U2FsdGVkX19BHfa+W8EmrAKa5uBuNIrpdQ3FNrVT8+QMvBh7IV9XMMaaoOK/TP/4G/LNrLX2SYYku7B6GL3pV38bHAx473Ma3DE1+k3O+nZT8X8f8/TrLWMwEfbqPttGVsbHSiEBmreEHQMjbpxdQalPxbXYLSYUHQq9M+FWItcZWx2KjUI+gs9oUo9KfTMn4z0lBzSLgzeaHWbDr2SnmTt3yiaVEcvnqGjavK5lSrNSK4k1Zej3ZBfLHpVHRsBGBBdViXFBRRoo4oaKxw/iH4xqZj0wK0B7AOKS86u7AJoDAe77Kip1XDl1E4D3TTznNDcQPmABvb6iJ/Sa6FT5CVZD38qRUdmh4HJaokeuqawWA7M/2u3kTKiEuICxOdg1xGB9fersecr9xG3r+Ea6KiGhQQRFuj3lKPv2Y4Hem0ixTGdQP4He3MaN90i1W24/pVZ5riplHOEp+9Vf5rP2JAQduAl/0WlrTTOxI7mqzLoSv4+3ycpKeDuN6LrrTfSpQsSXKpVKk7nP8E2qDc5phbYDz9L381J+BFTi4cAXOx3ZH8Gqh7NBRtwjSGxrynp4if31SDJwnRLXW5wWHR28g0N6XT44reoiMSkt6IIjJiCArw3m3uGsVzjVJVqUAzK2RBLstG2cVTlnPdtwj/bfRwKwMqYwretdUWMSxiKMGd1aLApHUi+4tJ/hz9ikX6yxIRqsc8g1U0yyUQLRqXch4ZJVe2/BDbZ6aXQiUS92Ch4=","categories":[{"name":"个人","slug":"个人","permalink":"https://lingyixia.github.io/categories/个人/"}],"tags":[{"name":"感悟","slug":"感悟","permalink":"https://lingyixia.github.io/tags/感悟/"}]},{"title":"C++初始化列表知识","slug":"C-InitTable","date":"2019-06-02T08:58:52.000Z","updated":"2021-09-19T18:01:25.944Z","comments":true,"path":"2019/06/02/C-InitTable/","link":"","permalink":"https://lingyixia.github.io/2019/06/02/C-InitTable/","excerpt":"使用初始化列表初始化成员变量的时候要注意,初始化的顺序和初始化列表无关,顺序只和变量声明的顺序有关","text":"使用初始化列表初始化成员变量的时候要注意,初始化的顺序和初始化列表无关,顺序只和变量声明的顺序有关 eg:123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;using namespace std;class A&#123;private: int n1; int n2;publ#include &lt;iostream&gt;using namespace std;class A&#123;private: int n2; int n1;public: A() :n2(0), n1(n2 + 2) &#123;&#125; void Print() &#123; cout &lt;&lt; n1 &lt;&lt; &quot; &quot; &lt;&lt; n2 &lt;&lt; endl; &#125;&#125;;int main()&#123; A a; a.Print(); return 0;&#125;ic: A() :n2(0), n1(n2 + 2) &#123;&#125; void Print() &#123; cout &lt;&lt; n1 &lt;&lt; &quot; &quot; &lt;&lt; n2 &lt;&lt; endl; &#125;&#125;;int main()&#123; A a; a.Print(); return 0;&#125; 结果是:一个随机数,0.因为变量声明的顺序是先n1后n2,因此初始化列表中先n1初始化,此时n2尚未初始化,因此只能是一个随机数 12345678910111213141516171819202122#include &lt;iostream&gt;using namespace std;class A&#123;private: int n2; int n1;public: A() :n2(0), n1(n2 + 2) &#123;&#125; void Print() &#123; cout &lt;&lt; n1 &lt;&lt; &quot; &quot; &lt;&lt; n2 &lt;&lt; endl; &#125;&#125;;int main()&#123; A a; a.Print(); return 0;&#125; 输出:2,0,因为n2先于n1声明,因此初始化列表中先初始化n2,后n1","categories":[{"name":"基础","slug":"基础","permalink":"https://lingyixia.github.io/categories/基础/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://lingyixia.github.io/tags/C/"}]},{"title":"otherProblems","slug":"otherProblems","date":"2019-05-29T08:21:08.000Z","updated":"2021-09-19T18:01:25.962Z","comments":true,"path":"2019/05/29/otherProblems/","link":"","permalink":"https://lingyixia.github.io/2019/05/29/otherProblems/","excerpt":"","text":"大数相乘12345678910111213141516171819202122232425string multiply(string num1, string num2)&#123; vector&lt;int&gt; record(num1.size() + num2.size()); for (int i = num1.size() - 1; i &gt;= 0; --i) &#123; for (int j = num2.size() - 1; j &gt;= 0; --j) &#123; int mul = (num1[i] - &apos;0&apos;) * (num2[j] - &apos;0&apos;); int pos1 = i + j; int pos2 = i + j + 1; int temp = mul + record[pos2]; record[pos1] += (temp / 10); record[pos2] = temp % 10; &#125; &#125; string result = &quot;&quot;; bool flag = false; for (auto num:record) &#123; if (num != 0) flag = true; if (flag) result += to_string(num); &#125; return result.empty() ? &quot;0&quot; : result;&#125;","categories":[],"tags":[]},{"title":"Bleu","slug":"bleu","date":"2019-05-26T08:38:13.000Z","updated":"2021-09-19T18:01:25.951Z","comments":true,"path":"2019/05/26/bleu/","link":"","permalink":"https://lingyixia.github.io/2019/05/26/bleu/","excerpt":"转载自这里: https://www.cnblogs.com/weiyinfu/p/9853769.html","text":"转载自这里: https://www.cnblogs.com/weiyinfu/p/9853769.html 理解bleubleu全称为Bilingual Evaluation Understudy(双语评估替换),是2002年提出的用于评估机器翻译效果的一种方法，这种方法简单朴素、短平快、易于理解。因为其效果还算说得过去，因此被广泛迁移到自然语言处理的各种评估任务中。这种方法可以说是：山上无老虎，猴子称大王。时无英雄遂使竖子成名。蜀中无大将，廖化做先锋。 问题描述首先，对bleu算法建立一个直观的印象。有两类问题： 给定一个句子和一个候选句子集，求bleu值，此问题称为sentence_bleu 给定一堆句子和一堆候选句子集，求bleu值，此问题称为corpus_bleu 机器翻译得到的句子称为candidate，候选句子集称为references。计算方式就是计算candidate和references的公共部分。公共部分越多，说明翻译结果越好。 给定一个句子和一个候选句子集计算bleu值bleu考虑1，2，3，4共4个n-gram，可以给每个n-gram指定权重。 对于n-gram： 对candidate和references分别分词（n-gram分词） 统计candidate和references中每个word的出现频次对于candidate中的每个word，它的出现频次不能大于references中最大出现频次 这一步是为了整治形如the the the the the这样的candidate，因为the在candidate中出现次数太多了，导致分值为1。为了限制这种不正常的candidate，使用正常的references加以约束。 candidate中每个word的出现频次之和除以总的word数，即为得分score score乘以句子长度惩罚因子即为最终的bleu分数这一步是为了整治短句子，比如candidate只有一个词：the，并且the在references中出现过，这就导致得分为1。也就是说，有些人因为怕说错而保持沉默。 bleu的发展不是一蹴而就的，很多人为了修正bleu，不断发现bleu的漏洞并提出解决方案。从bleu的发展历程上，我们可以学到如何设计规则整治badcase。 最后,对于1-gram，2-gram，3-gram的组合，应该采用几何平均，也就是$s_1^{w_1} \\times s_2^{w_2} \\times s_3^{w_3}$，而不是算术平均$w_1 \\times s_1+w_2 \\times s_2+w_3 \\times s_3$。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from collections import Counterimport numpy as npfrom nltk.translate import bleu_scoredef bp(references, candidate): # brevity penality,句子长度惩罚因子 ind = np.argmin([abs(len(i) - len(candidate)) for i in references]) if len(references[ind]) &lt; len(candidate): return 1 scale = 1 - (len(candidate) / len(references[ind])) return np.e ** scaledef parse_ngram(sentence, gram): # 把一个句子分成n-gram return [sentence[i:i + gram] for i in range(len(sentence) - gram + 1)] # 此处一定要注意+1，否则会少一个gramdef sentence_bleu(references, candidate, weight): bp_value = bp(references, candidate) s = 1 for gram, wei in enumerate(weight): gram = gram + 1 # 拆分n-gram ref = [parse_ngram(i, gram) for i in references] can = parse_ngram(candidate, gram) # 统计n-gram出现次数 ref_counter = [Counter(i) for i in ref] can_counter = Counter(can) # 统计每个词在references中的出现次数 appear = sum(min(cnt, max(i.get(word, 0) for i in ref_counter)) for word, cnt in can_counter.items()) score = appear / len(can) # 每个score的权值不一样 s *= score ** wei s *= bp_value # 最后的分数需要乘以惩罚因子 return sreferences = [ &quot;the dog jumps high&quot;, &quot;the cat runs fast&quot;, &quot;dog and cats are good friends&quot;]candidate = &quot;the d o g jump s hig&quot;weights = [0.25, 0.25, 0.25, 0.25]print(sentence_bleu(references, candidate, weights))print(bleu_score.sentence_bleu(references, candidate, weights)) 给定一组句子和一个组候选句子集计算bleu值 一个corpus是由多个sentence组成的，计算corpus_bleu并非求sentence_bleu的均值，而是一种略微复杂的计算方式，可以说是没什么道理的狂想曲。一个文档包含3个句子，句子的分值分别为a1/b1，a2/b2，a3/b3。那么全部句子的分值为：(a1+a2+a3)/(b1+b2+b3)惩罚因子也是一样：三个句子的长度分别为l1,l2,l3，对应的最接近的reference分别为k1,k2,k3。那么相当于bp(l1+l2+l3,k1+k2+k3)。也就是说：对于corpus_bleu不是单纯地对sentence_bleu求均值，而是基于更统一的一种方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from collections import Counterimport numpy as npfrom nltk.translate import bleu_scoredef bp(references_len, candidate_len): if references_len &lt; candidate_len: return 1 scale = 1 - (candidate_len / references_len) return np.e ** scaledef parse_ngram(sentence, gram): return [sentence[i:i + gram] for i in range(len(sentence) - gram + 1)]def corpus_bleu(references_list, candidate_list, weights): candidate_len = sum(len(i) for i in candidate_list) reference_len = 0 for candidate, references in zip(candidate_list, references_list): ind = np.argmin([abs(len(i) - len(candidate)) for i in references]) reference_len += len(references[ind]) s = 1 for index, wei in enumerate(weights): up = 0 # 分子 down = 0 # 分母 gram = index + 1 for candidate, references in zip(candidate_list, references_list): # 拆分n-gram ref = [parse_ngram(i, gram) for i in references] can = parse_ngram(candidate, gram) # 统计n-gram出现次数 ref_counter = [Counter(i) for i in ref] can_counter = Counter(can) # 统计每个词在references中的出现次数 appear = sum(min(cnt, max(i.get(word, 0) for i in ref_counter)) for word, cnt in can_counter.items()) up += appear down += len(can) s *= (up / down) ** wei return bp(reference_len, candidate_len) * sreferences = [ [ &quot;the dog jumps high&quot;, &quot;the cat runs fast&quot;, &quot;dog and cats are good friends&quot;], [ &quot;ba ga ya&quot;, &quot;lu ha a df&quot;, ]]candidate = [&quot;the d o g jump s hig&quot;, &apos;it is too bad&apos;]weights = [0.25, 0.25, 0.25, 0.25]print(corpus_bleu(references, candidate, weights))print(bleu_score.corpus_bleu(references, candidate, weights)) 简化代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576from collections import Counterimport numpy as npfrom nltk.translate import bleu_scoredef bp(references_len, candidate_len): return np.e ** (1 - (candidate_len / references_len)) if references_len &gt; candidate_len else 1def nearest_len(references, candidate): return len(references[np.argmin([abs(len(i) - len(candidate)) for i in references])])def parse_ngram(sentence, gram): return [sentence[i:i + gram] for i in range(len(sentence) - gram + 1)]def appear_count(references, candidate, gram): ref = [parse_ngram(i, gram) for i in references] can = parse_ngram(candidate, gram) # 统计n-gram出现次数 ref_counter = [Counter(i) for i in ref] can_counter = Counter(can) # 统计每个词在references中的出现次数 appear = sum(min(cnt, max(i.get(word, 0) for i in ref_counter)) for word, cnt in can_counter.items()) return appear, len(can)def corpus_bleu(references_list, candidate_list, weights): candidate_len = sum(len(i) for i in candidate_list) reference_len = sum(nearest_len(references, candidate) for candidate, references in zip(candidate_list, references_list)) bp_value = bp(reference_len, candidate_len) s = 1 for index, wei in enumerate(weights): up = 0 # 分子 down = 0 # 分母 gram = index + 1 for candidate, references in zip(candidate_list, references_list): appear, total = appear_count(references, candidate, gram) up += appear down += total s *= (up / down) ** wei return bp_value * sdef sentence_bleu(references, candidate, weight): bp_value = bp(nearest_len(references, candidate), len(candidate)) s = 1 for gram, wei in enumerate(weight): gram = gram + 1 appear, total = appear_count(references, candidate, gram) score = appear / total # 每个score的权值不一样 s *= score ** wei # 最后的分数需要乘以惩罚因子 return s * bp_valueif __name__ == &apos;__main__&apos;: references = [ [ &quot;the dog jumps high&quot;, &quot;the cat runs fast&quot;, &quot;dog and cats are good friends&quot;], [ &quot;ba ga ya&quot;, &quot;lu ha a df&quot;, ] ] candidate = [&quot;the d o g jump s hig&quot;, &apos;it is too bad&apos;] weights = [0.25, 0.25, 0.25, 0.25] print(corpus_bleu(references, candidate, weights)) print(bleu_score.corpus_bleu(references, candidate, weights)) print(sentence_bleu(references[0], candidate[0], weights)) print(bleu_score.sentence_bleu(references[0], candidate[0], weights))","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"https://lingyixia.github.io/categories/自然语言处理/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://lingyixia.github.io/tags/转载/"}]},{"title":"膨胀卷积","slug":"DilatedConvolution","date":"2019-05-13T13:37:06.000Z","updated":"2021-09-19T18:01:25.945Z","comments":true,"path":"2019/05/13/DilatedConvolution/","link":"","permalink":"https://lingyixia.github.io/2019/05/13/DilatedConvolution/","excerpt":"膨胀卷积是为了解决CNN中pooling损失信息而诞生的,CNN中pooling的作用一般有二,其一是为了减少参数,其二是为了扩大感受野","text":"膨胀卷积是为了解决CNN中pooling损失信息而诞生的,CNN中pooling的作用一般有二,其一是为了减少参数,其二是为了扩大感受野 感受野 通俗解释就是当前层的一个单位是由输入层多少单位计算得到的. eg: 图右侧的一个单位是输入层9个单位计算得到的,感受野是9 大小卷积核 多层小卷积核能够获得和单层大卷积核相同的感受野,但是大大减少参数量 eg1: 图左侧感受野是$5 \\times 5 = 25$,要想达到这个感受野使用一个卷积核只能是$5 \\times 5$,即$(5 \\times 5 +1) \\times n$个参数,但是图中进行了两次卷积,使用了两个卷积核,同样达到了$5 \\times 5$的感受野,但是使用了两个$3 \\times 3$的卷积核,即参数大小为:$(3 \\times 3 +1) \\times n + (3 \\times 3 +1) \\times n$个参数,比原来少了$6n$个参数。 eg2: 图左侧感受野是$7 \\times 7 = 49$,要想达到这个感受野使用一个卷积核只能是$7 \\times 7$,即$(7 \\times 7 +1) \\times n$个参数,但是图中进行了三次卷积,使用了三个卷积核,同样达到了$7 \\times 7$的感受野,但是使用了三个卷积个$3 \\times 3$的卷积核,即参数大小为:$(3 \\times 3 +1) \\times n + (3 \\times 3 +1) \\times n + (3 \\times 3 +1) \\times n$个参数,比原来少了$20n$个参数。 感受野计算公式$rn = (r{n-1}-1) \\times stride+ksize$,其中,$m$表示上层感受野(初始感受野为1),$r$表示本层感受野,$stride$表示步长,$ksize$表示卷积核大小 膨胀卷积普通卷积: 膨胀卷积: 膨胀卷积核计算 将原始卷积核填充若干行\\列0,得到更大的卷积核,卷积的时候stride为1$n-dilated convolution$,一般$n$是$2$的整数次幂(a) 普通卷积,$1-dilated convolution$,卷积核的感受野为$3 \\times 3$(当n=1的时候也就是普通的CNN)(b) 扩张卷积,$2-dilated convolution$,卷积核的感受野为$7 \\times 7$(c) 扩张卷积,4-dilated convolution,卷积核的感受野为$15 \\times 15$上诉三个感受野是怎么计算的呢?首先三个膨胀卷积核大小为:$3 \\times 3$,$5 \\times 5$,$9 \\times 9$(a) 感受野为:$(1-1) \\times 1+3 = 3$(b) 感受野为:$(3-1) \\times 1+5 = 7$(c) 感受野为:$(7-1) \\times 1+9 = 15$ 一般情况下第一层先用普通卷积,然后下几层用n-dilated convolution 可以看出,两者蓝色部分感受野分别为5和7 参考一参考二参考三","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"https://lingyixia.github.io/tags/CNN/"},{"name":"卷积核","slug":"卷积核","permalink":"https://lingyixia.github.io/tags/卷积核/"},{"name":"感受野","slug":"感受野","permalink":"https://lingyixia.github.io/tags/感受野/"}]},{"title":"滑动窗口系列","slug":"SlideWindow","date":"2019-05-11T09:04:10.000Z","updated":"2021-09-19T18:01:25.948Z","comments":true,"path":"2019/05/11/SlideWindow/","link":"","permalink":"https://lingyixia.github.io/2019/05/11/SlideWindow/","excerpt":"","text":"滑动窗口 最长连续不重复字串 leetcode3其实思想就是用一个窗口[left,right]表示该窗口中没有重复数字,那么下一步要窗口右移,即第right+1个,如果第right+1个在[left,right]中,那么就找出其坐标,使left更新为该坐标,否则加入窗口即可,这是法二的思维,法一的思维是如果在窗口中就不断右移left,直到该坐标位置,其实思想是一样的. 法一1234567891011121314151617181920212223int lengthOfLongestSubstring(string s)&#123; if(s.empty()) return 0; unordered_set&lt;char&gt; records; int left = 0; int right = -1; int maxLength = INT_MIN; while (left&lt;s.size()) &#123; if (right+1&lt;s.size() &amp;&amp; records.count(s[right+1])==0) &#123; records.insert(s[right+1]); right++; &#125; else &#123; records.erase(s[left]); left++; &#125; maxLength = max(maxLength,right-left+1); &#125; return maxLength;&#125; 法二123456789101112131415int lengthOfLongestSubstring(string s)&#123; unordered_map&lt;char,int&gt; records; int maxLength = INT_MIN; for (int right = 0,left=0; right &lt; s.size(); right++) &#123; if (records.count(s[right])&gt;0) &#123; left = max(records[s[right]], left); &#125; maxLength = max(maxLength,right-left+1); records[s[right]] = right+1; &#125; return maxLength;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[]},{"title":"矩阵若干题解","slug":"matrix","date":"2019-05-07T08:47:53.000Z","updated":"2021-09-19T18:01:25.960Z","comments":true,"path":"2019/05/07/matrix/","link":"","permalink":"https://lingyixia.github.io/2019/05/07/matrix/","excerpt":"","text":"矩阵的题往往和回溯有关,然后在回溯的基础上用动态规划提升效率 机器人路径 地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445bool checkout(int threshold, int row, int col, vector &lt;vector&lt;bool&gt;&gt; &amp;visited)&#123; if (row &gt;= 0 &amp;&amp; row &lt; visited.size() &amp;&amp; col &gt;= 0 &amp;&amp; col &lt; visited[0].size() &amp;&amp; !visited[row][col] &amp;&amp; getSum(row) + getSum(col) &lt;= threshold) return true; return false;&#125;int getSum(int data)&#123; int sum = 0; while (data != 0) &#123; sum += data % 10; data = data / 10; &#125; return sum;&#125;int dfs(vector &lt;vector&lt;bool&gt;&gt; &amp;visited, int threshold, int row, int col)&#123; int count = 0; if (checkout(threshold, row, col, visited)) &#123; visited[row][col] = true; count = 1 + dfs(visited, threshold, row + 1, col) + dfs(visited, threshold, row - 1, col) + dfs(visited, threshold, row, col + 1) + dfs(visited, threshold, row, col - 1); &#125; return count;&#125;int movingCount(int threshold, int rows, int cols)&#123; vector &lt;vector&lt;bool&gt;&gt; visited; for (int i = 0; i &lt; rows; i++) &#123; vector&lt;bool&gt; temp(cols, false); visited.push_back(temp); &#125; int count = moveCountCore(visited, threshold, 0, 0); return count;&#125; 矩阵中的路径 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 12345678910111213141516171819202122232425262728293031323334353637383940414243bool dfs(char *matrix, int rows, int cols, int row, int col, vector &lt;vector&lt;bool&gt;&gt; &amp;visited, int indexLength, char *str)&#123; if (str[indexLength] == &apos;\\0&apos;) &#123; return true; &#125; bool has = false; if (row &gt;= 0 &amp;&amp; row &lt; rows &amp;&amp; col &gt;= 0 &amp;&amp; col &lt; cols &amp;&amp; !visited[row][col] &amp;&amp; matrix[row * cols + col] == str[indexLength]) &#123; visited[row][col] = true; has = dfs(matrix, rows, cols, row + 1, col, visited, indexLength + 1, str) || dfs(matrix, rows, cols, row - 1, col, visited, indexLength + 1, str) || dfs(matrix, rows, cols, row, col + 1, visited, indexLength + 1, str) || dfs(matrix, rows, cols, row, col - 1, visited, indexLength + 1, str); if (!has) &#123; visited[row][col] = false; &#125; &#125; return has;&#125;bool hasPath(char *matrix, int rows, int cols, char *str)&#123; vector &lt;vector&lt;bool&gt;&gt; visited; for (int i = 0; i &lt; rows; i++) &#123; vector&lt;bool&gt; temp(cols, false); visited.push_back(temp); &#125; for (int i = 0; i &lt; rows; i++) &#123; for (int j = 0; j &lt; cols; j++) &#123; if (findPathCore(matrix, rows, cols, i, j, visited, 0, str)) &#123; return true; &#125; &#125; &#125; return false;&#125; 矩阵中矩阵最长递增路径 一维最长递增子序列12345678910111213141516171819202122232425262728293031vector&lt;vector&lt;int&gt;&gt; directions = &#123; &#123;0,1&#125;,&#123;0,-1&#125;,&#123;1,0&#125;,&#123;-1,0&#125; &#125;;int dfs(vector&lt;vector&lt;int&gt;&gt;&amp; matrix,vector&lt;vector&lt;int&gt;&gt;&amp; dp, int posX, int posY)&#123; if (dp[posX][posY]) return dp[posX][posY]; int maxLength = 0; for (auto direction : directions) &#123; int currentX = posX + direction[0]; int currentY = posY + direction[1]; if (currentX &gt;= 0 &amp;&amp; currentX &lt; matrix.size() &amp;&amp; currentY &gt;= 0 &amp;&amp; currentY &lt; matrix[0].size() &amp;&amp; matrix[currentX][currentY] &gt; matrix[posX][posY]) &#123; maxLength = max(maxLength, dfs(matrix,dp, currentX, currentY)); &#125; &#125; dp[posX][posY] = maxLength+1; return dp[posX][posY];&#125;int longestIncreasingPath(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; if (matrix.empty() || matrix[0].empty()) return 0; int maxLength = 0; vector&lt;vector&lt;int&gt;&gt; dp(matrix.size(),vector&lt;int&gt;(matrix[0].size())); for (int i = 0; i &lt; matrix.size(); i++) &#123; for (int j = 0; j &lt; matrix[0].size(); j++) &#123; maxLength = max(maxLength, dfs(matrix,dp, i, j)); &#125; &#125; return maxLength;&#125; 八皇后123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Solution &#123;public: vector&lt;vector&lt;string&gt;&gt; result;bool judge(vector&lt;int&gt; current,int column)&#123; for (int row = 0; row &lt; current.size(); row++) &#123; if (current[row]==column || abs(column-current[row])==abs(int(current.size())-row)) &#123; return false; &#125; &#125; return true;&#125;void queens(vector&lt;int&gt; current,int n)&#123; if (current.size()==n) &#123; vector&lt;string&gt; temp; for (int row = 0; row &lt; current.size(); row++) &#123; string str = &quot;&quot;; for (int col = 0; col &lt; current.size(); col++) &#123; if (col==current[row]) &#123; str += &quot;Q&quot;; &#125; else &#123; str += &quot;.&quot;; &#125; &#125; temp.push_back(str); &#125; result.push_back(temp); &#125; else &#123; for (int col = 0; col &lt; n; col++) &#123; if (judge(current,col)) &#123; current.push_back(col); queens(current,n); current.pop_back(); &#125; &#125; &#125;&#125;vector&lt;vector&lt;string&gt;&gt; solveNQueens(int n)&#123; vector&lt;int&gt; current; queens(current,n); return result;&#125;&#125;;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://lingyixia.github.io/tags/动态规划/"},{"name":"回溯","slug":"回溯","permalink":"https://lingyixia.github.io/tags/回溯/"},{"name":"dfs","slug":"dfs","permalink":"https://lingyixia.github.io/tags/dfs/"}]},{"title":"NLPInterview","slug":"NLPInterview","date":"2019-05-02T06:20:23.000Z","updated":"2021-09-19T18:01:25.947Z","comments":true,"path":"2019/05/02/NLPInterview/","link":"","permalink":"https://lingyixia.github.io/2019/05/02/NLPInterview/","excerpt":"","text":"总结一些NLP面试必备知识点链接总结:1.https://mp.weixin.qq.com/s/MCIhRmbq2N0HM_0v-nld_w CNN发展史感受野SameAndValidCNN感受野为什么都是奇数:1.从SameAndValid](https://lingyixia.github.io/2019/03/13/CnnSizeCalc/),Padding = Kernel_size-1,当Kernel_size为奇数的时候可以平均到两边2.方便确定卷积核的位置，中心点就可以代表卷积核的位置，但是偶数的话就不方便 Pooling的作用卷积和池化的区别参数计算方法输出层维度计算1*1卷积的作用反卷积和空洞卷积(残差网络)[https://lingyixia.github.io/2019/04/05/transformer/]transformer两个问题:1,为什么用多头:多个attention便于模型学习不同子空间位置的特征表示，然后最终组合起来这些特征，而单头attention直接把这些特征平均，就减少了一些特征的表示可能。2.为什么self-attention要除以一个根号维度:假设Q和K都是独立的随机变量，满足均值为0，方差为1，则点乘后结果均值为0，方差为dk。也即方差会随维度dk的增大而增大，而大的方差导致极小的梯度(我认为大方差导致有的输出单元a（a是softmax的一个输出）很小，softmax反向传播梯度就很小（梯度和a有关））。为了避免这种大方差带来的训练问题，论文中用内积除以维度的开方，使之变为均值为0，方差为1。 batch normalization解决的是梯度消失问题,残差解决的是网络增加深度带来的退化问题目的是学习F(x)=x，但是神经网络学习这个不容易，但是学习F(x)=0更容易(一般初始化的时候都是以0为均值).在假设现在的目的是把5学到5.1，不加残差网络的参数变化是F(5)-&gt;5.1,加残差网络的参数变化是H(5)=F’(5)+5=5.1,也就是F’(5)=0.1，也就是学习的过程变成了学习残差,而残差一般数值较小,神经网络对小数更敏感。平常的前向公式: x_L = \\sum_{i=1}^LF(x_i,w)反向传播公式: \\frac{\\partial L}{\\partial x_l}=\\frac{\\partial L}{\\partial O} \\prod_l \\frac{\\partial O}{\\partial x_l}残差网络的前向公式: x_L = x_l+\\sum_{i=1}^LF(x_i,w)反向传播公式: \\frac{\\partial L}{\\partial x}=\\frac{\\partial L}{\\partial O}(1+ \\prod_l \\frac{\\partial O}{\\partial x_l})残差网络名字由来 CNN复杂度分析RNNRNN梯度消失和爆炸 回答要点:1,所说的梯度消失指的是远距离的消失，近处的不会消失2.所谓的解决是在C的更新路径上解决，而h的路径上，该消失还会消失。 LSTM公式、结构，s和h的作用LSTM和普通的RNN相比,多了三个门结构，用来解决当序列过长造成的梯度爆炸或梯度消失问题(写下三个门结构的公式),这三个门结构都是针对输入信息进行处理的,首先对于输入信息要做一个非线性化(非线性化公式),也就是说f是针对上一步的信息拿过来多少，所以我觉得叫记住门更合适，i是真对当前信息留下多少,最后一个ht，也就是说，h是用来保保持前后联系的状态，c是用来维持信息的状态. LSTM和GRUGRU和LSTM的性能在很多任务上不分伯仲。GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。从结构上来说，GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将hidden state 传给下一个单元，而LSTM则用memory cell 把hidden state 包装起来。 Transform参数计算CNN和RNN反向传播的不同CNN反向传播需要解决的问题是Pool压缩数据、卷积的解决,Pool可以进行上采样，得到原先的size，卷积可以转为矩阵乘矩阵的形式,比如inputsize为x=9*9,卷积核为4*4,stride为1,则输出为5*5,可以将该卷积操作写为25 * 81 乘以81 * 1,然后将得到的25写作5 *5即可,它每一层更新的都是不同的参数.而RNN由于每个步骤都是共享的参数,因此需要每一个步骤都反向传播到最开始，然后把每个步骤传播过来的梯度相加在更新. overfittingBN的作用优化方式激活函数(注意bert的gleu)指标介绍本来很简单的东西解释的一踏糊涂，，还是准备一下措辞吧:语言介绍:精确度率指的是预测为正例中预测正确的比重 准确率指的是所有样本中预测正确的比重,前者针对正例，后者针对预测正确(包括正例预测正确和负例预测正确) ROC曲线:绘制过程横坐标:$FPR=\\frac{FP}{样本标签所有负例}$，也就是所有的负例有多少被预测为正例了.纵坐标:$TPR=\\frac{TP}{样本标签所有正例}$(zh)召回率,也就是所有的正例有多少被预测为正例了. AUC是衡量二分类模型优劣的一种评价指标，表示预测的正例排在负例前面的概率。 机器学习最大熵模型介绍:第一,充分考虑,第二,不做任何假设SVM和LR：相同点:1.都是线性分类器(SVM不考虑核函数)(其实线性的意思是各个特征的线性组合而不是幂次方组合) 2.都是判别模型 3.都是有监督模型不同点:1.损失函数(合页损失核交叉熵损失) 2.LR优化考虑所有的点，SVM其实最终考虑的只有支持向量 3.SVM基于距离分类，而LR基于概率分类，所以SVM最好先对数据进行归一化处理，而LR不受影响 4.SVM的损失函数自带正则，而LR必须另外在损失函数外添加正则项。 5.SVM解决非线性问题用核函数，LR不用核函数(计算所有数据复杂度太高)会在数据处理上下功夫，比如数据离散化.使用场景:1.异常点多，优先使用逻辑回归(使用所有的数据计算loss，会减少异常点的贡献)2.特征维度很大，优先使用逻辑回归，因为特征纬度大用参数模型的逻辑回归表达能力更强,而且速度快3.如果特征维度小，数据少,优先使用svm+核函数,因为特征少容易造成非线性问题,而数据少只要支持向量没变最后的平面也不会变。4.特征小，数据很多很多，先想办法增加维度，然后用lr或线性svm(svm不擅长处理特征维度大核数据多的问题，花费时间会猛增)) 逻辑回归假设服从伯努利分布，线性回归假设服从正太分布 激活函数 Relu/sigmoid /tanhsigmoid: y = 1/(1 + e-x)tanh: y = (ex - e-x)/(ex + e-x)relu: y = max(0, x) 激活函数通常有以下性质： 非线性：如果激活函数都为线性，那么神经网络的最终输出都是和输入呈线性关系；显然这不符合事实。 可导性：神经网络的优化都是基于梯度的，求解梯度时需要确保函数可导。 单调性:激活函数是单调的，否则不能保证神经网络抽象的优化问题为凸优化问题了。 输出范围有限：激活函数的输出值的范围是有限时，基于梯度的方法会更加稳定。输入值范围为 (−∞,+∞) ，如果输出范围不加限制，虽然训练会更加高效，但是learning rate将会更小，且给工程实现带来许多困难。 第一,sigmoid和tanh对比,后者相当于前者的平移版本,取值范围是[-1,1],可以看做一种数据中心化的效果第二,sigmoid最大的优势就是可以输出概率第三,sigmoid反向求导计算量大,容易梯度消失,relu速度快，不会梯度消失 为什么沿梯度方向最快 简单的说就是快不快要用方向导数定义，而方向导数沿梯度方向最大(两向量共线乘积最大) bert点 transform的encoder 预测15%,80%mask,10% 替换 10%保留 其实每次选取15%来predict对于一个句子来说还是比较多的，很容易造成某个词一直都是mask状态，也就是说最后的模型没有该词的信息, 激活函数(在relu中加入了随机正则) L1和L2L1正则项更容易得到稀疏矩阵，用于特征选择参考1(https://blog.csdn.net/jinping_shi/article/details/52433975)参考2L2用于过拟合参考3 调参不理想如果超参数调不好就需要1.考虑参数初始化问题和激活函数2.看是否overfitting Dropout加在哪里word embedding层后、pooling层后、FC层（全联接层）后 GBDT和XGboost大文件小到大排序1.将大文件分割为N个内存可读的小文件,每个小文件从小到大排序2.每个文件取一个指针指向头数据3.将头数据组成小根堆4.将堆顶文件存入文件,将之前堆顶文件的下一个数据放入堆顶，调整成小根堆5.执行步骤4，直到完成. 类别不均衡1.调整class-weight，使对少的类更敏感(把类从小到大数量排序，在倒过来排序，softmax后就是各个权重，即少的大，多的小s)2.比如大类是小类的N倍，则训练N个模型，每个模型都是小类全部和大类的1/N，最后投票。3.少类过采样(复制多分，过拟合)，多类欠采样(没有充分利用样本) SMOTE可以合成少量数据用来训练 GDBT和RF优点缺点维比特算法假设序列长度为,隐含状态数量为m，穷举法的时间复杂度为$O(n^m)$维比特算法时间复杂度为$O(m*n^2)$ 各种normilization","categories":[],"tags":[]},{"title":"CNN发展史","slug":"CNNdevelopment","date":"2019-05-01T10:10:28.000Z","updated":"2021-09-19T18:01:25.944Z","comments":true,"path":"2019/05/01/CNNdevelopment/","link":"","permalink":"https://lingyixia.github.io/2019/05/01/CNNdevelopment/","excerpt":"简单记录一下CNN发展历程","text":"简单记录一下CNN发展历程 LeNet 诞生:1998 作者:LeCun 别称:CNN鼻祖 结构: Input层:$32 \\times 32 \\times 1$ C1层: 输入:$32 \\times 32 \\times 1$ kernel:6个$5 \\times 5$ stride:$1 \\times 1$ 输出:$28 \\times 28 \\times 6$ S2层: 输入:$28 \\times 28 \\times 6$ pooling:$2 \\times 2$ 输出:$14 \\times 14 \\times 6$ C3层: 输入:$14 \\times 14 \\times 6$ kernel:16个$5 \\times 5$ stride:$1 \\times 1$ 输出:$10 \\times 10 \\times 16$ S4层: 输入:$10 \\times 10 \\times 16$ pooling: $2 \\times 2$ 输出:$5 \\times 5 \\times 16$ C5层: 输入:$5 \\times 5 \\times 16$ kernel:120个$5 \\times 5$ stride:$1 \\times 1$ 输出:$1 \\times 1 \\times 120$ 全连接层F6: 输入:$1 \\times 120$ 输出:$120 \\times 84$(10个种类) 全连接输出层: 输入:$1 \\times 84$ 输出:$1 \\times 10$ AlexNet 诞生:2012年ImageNet竞赛 作者:Hinton和他的学生Alex Krizhevsky 别称:CNN王者归来 本质:增加LeNet深度并使用一些小Tips. 特点: ReLU:收敛快,计算量少 数据扩充:对256×256的图片进行随机裁剪到224×224，然后进行水平翻转，相当于将样本数量增加了$((256-224)^2)×2=2048$倍,减少overfitting 重叠池化:减少overfitting dropout:减少overfitting LRN 多GPU 结构: L1:卷积$\\rightarrow$ReLU$\\rightarrow$池化$\\rightarrow$归一化 输入:$224 \\times 224 \\times 3$ kernel:96个$11 \\times 11 \\times 3$ padding:3 stride: 4 卷积输出:$55 \\times 55 \\times 96$ pooling:$3 \\times 3$ stride:2 输出:$27 \\times 27 \\times 96$ L2:卷积$\\rightarrow$ReLU$\\rightarrow$池化$\\rightarrow$归一化 输入:$27 \\times 27 \\times 96$ kernel:256个$5 \\times 5 \\times 96$ padding:两个2 stride: 1 卷积输出:$27 \\times 27 \\times 256$ pooling:$3 \\times 3$ stride:2 输出:$13 \\times 13 \\times 256$ L3:卷积$\\rightarrow$ReLU 输入:$13 \\times 13 \\times 256$ kernel:384个$3 \\times 3$ padding:两个1 stride:1 卷积输出:$13 \\times 13 \\times 384$ L4:卷积$\\rightarrow$ReLU 输入:$13 \\times 13 \\times 384$ kernel:256个$3 \\times 3$ padding:两个1 stride:1 卷积输出:$13 \\times 13 \\times 256$ L5:卷积$\\rightarrow$ReLU$\\rightarrow$池化 输入:$13 \\times 13 \\times 256$ kernel:256个$3 \\times 3$ padding:两个1 stride:1 卷积输出:$13 \\times 13 \\times 256$ pooling:$3 \\times 3$ stride:2 输出:$6 \\times 6 \\times 256$ L6:全连接(全卷积)$\\rightarrow$ReLU$\\rightarrow$Dropout 输入:$6 \\times 6 \\times 256$ 全卷积的意思是4096个$6 \\times 6 \\times 256$的卷积核 输出:$4096 \\times 1 \\times 1$ L7:全连接$\\rightarrow$ReLU$\\rightarrow$Dropout 输入输出都是4096 L8全连接: 输入4096 输出1000 VGGNet 诞生:2014年ImageNet竞赛(定位任务第一名,分类任务第二名) 作者:牛津大学Andrew Zisserman教授的组 本质:比AlexNet使用更小的卷积核和更深的层级 特点: 小卷积核和多卷积子层 小池化核:相比AlexNet的3x3的池化核，VGG全部采用2x2的池化核。 层数更深、特征图更宽:由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，控制了计算量的增加规模。 全连接转卷积（测试阶段）网络结构见链接吧,不写了. GoogLeNet 诞生:2014年ImageNet竞赛(分类任务第一名) 作者:Christian Szegedys 特点:Inception Inception V1 诞生: 2014 创新点:使用$1 \\times 1$卷积核: 1. 全卷积其实就是全连接,$1 \\times 1$卷积不是全连接 2. $1 \\times 1$卷积核前后size不变,通道数可以改变,因此可以用来降维或升维 3. 增加非线性(没明白) 4. 跨通道信息交互 Inception V2 诞生: 2015 创新点:1. 用两个$3 \\times 3$卷积核代替$5 \\times 5$卷积核 2. 提出BatchNormal Inception V3 诞生: 2015 创新点:1.改进拆分卷积 Inception V4 诞生: 2016 创新点:增加ResNet ResNet 诞生:2015 作者:何恺明 别称:里程碑 目的:解决网络太深导致的梯度消失或梯度爆炸,从而使网络退化问题(不是overfitting) 作用:将某些层的输入和输出映射成为恒等对 公式推导参考这里:假设我们要更新第$l$层的某一个参数$w$,网络结构共$L$层,激活函数全部采用$relu$且大于$0$,则有: x_{l+1}=x_l+F(x_l) \\\\ x_{l+2}=x_{l+1}+F(x_{l+1})\\\\ . . .\\\\ x_L=x_l+\\sum_{i=l}^{L-1}F(x_i)则计算反响传播: \\frac{\\partial L}{\\partial x_l}=\\frac{\\partial L}{\\partial x_L} \\times (1+\\frac{\\partial \\sum_{i=l}^{L-1}F(x_i)}{\\partial x_l})可以看出,此时的偏导数不再是连乘，而是连加,因此避免了梯度消失和梯度爆炸。 DenseNet一张图： 两句话： 每一层输入都是前面所有层输出的contact 和resnet唯一的不同就是restnet是add,而densenet用的是contact","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"https://lingyixia.github.io/tags/CNN/"}]},{"title":"迁移学习综述论文笔记","slug":"TransferLearning","date":"2019-04-23T02:32:22.000Z","updated":"2021-09-19T18:01:25.948Z","comments":true,"path":"2019/04/23/TransferLearning/","link":"","permalink":"https://lingyixia.github.io/2019/04/23/TransferLearning/","excerpt":"迁移学习资料。迁移学习与普通机器学习、深度学习最大的不同在于放宽了训练数据和测试数据必须满足独立同分布的假设和解决有标签训练样本匮乏的问题.","text":"迁移学习资料。迁移学习与普通机器学习、深度学习最大的不同在于放宽了训练数据和测试数据必须满足独立同分布的假设和解决有标签训练样本匮乏的问题. 迁移学习研究进展 根据按源领域和目标领域样本是否标注以及任务是否相同划分分类: 目标领域中有少量标注样本的归纳迁移学习 只有源领域中有标签样本的直推式迁移学习 源领域和目标领域都没有标签样本的无监督迁移学习 按采用的技术分类: 半监督学习(半监督学习、直推式学习和主动学习) 基于特征选择方法(选择source domain和 target domain共有特征) 基于特征映射方法(将source domain和target domain映射到高纬度) 基于权重方法(赋予source domain和target domain样本不同的权重) A Survey on Transfer Learning 名词解释:domain和task 三个问题(重点是1和3): 迁移什么:提出了迁移哪部分知识的问题; 何时迁移:提出了哪种情况下迁移手段应当被运用 如何迁移:迁移学习的方式 迁移什么 如何迁移: 两者关系","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"迁移学习","slug":"迁移学习","permalink":"https://lingyixia.github.io/tags/迁移学习/"}]},{"title":"各种API收集","slug":"API","date":"2019-04-22T13:40:46.000Z","updated":"2021-09-19T18:01:25.943Z","comments":true,"path":"2019/04/22/API/","link":"","permalink":"https://lingyixia.github.io/2019/04/22/API/","excerpt":"收集各种Api接口","text":"收集各种Api接口 视频API岩兔站","categories":[],"tags":[]},{"title":"蓄水池采样算法","slug":"PoolSampling","date":"2019-04-14T11:25:54.000Z","updated":"2021-09-19T18:01:25.947Z","comments":true,"path":"2019/04/14/PoolSampling/","link":"","permalink":"https://lingyixia.github.io/2019/04/14/PoolSampling/","excerpt":"蓄水池采样算法","text":"蓄水池采样算法 问题叙述常见采样方式有三种: 从 100000 份调查报告中抽取 1000 份进行统计。 从一本很厚的电话簿中抽取 1000 人进行姓氏统计。 从 Google 搜索 “Ken Thompson”，从中抽取 100 个结果查看哪些是今年的。对于第一种方式我们可以采用从[0,100000-1]中随机采样的方式,如果遇到已经采样的就重新采样,这样的方式如果遇到采样比例很高可能会遇到重新采样次数过多的情况.对于第二种表述的是我们不知道总体是多少,也就无法确定随机范围.对于第三种表述的是总体数据过于庞大,无法直接放到内存中. 蓄水池采样算法为此我们引入蓄水池采样算法,该算法能够保证在$n$个数据中取$k$个,每个数据被采样的概率都是$\\frac{k}{n}$。算法语言描述: 先选取$n$个元素中的前$k$个元素，保存在集合$A$中 从第$i(k+1&lt;=i&lt;=n)$个元素开始，每次先以概率$\\frac{k}{i}$选择是否让第个元素留下。若第$i$个元素存活，则从$A$中随机选择一个元素$j$并用该元素替换它;否则直接淘汰该元素; 重复1和2伪代码:123451. 取前k个数放到用来初始化数组池2. for i= k+1~n:3. j = random(1,i)4. if j&lt;=k:5. swap(j,i) 就这么简答！！！！ 算法证明证明目标:对于这$n$个元素,我们要证明算法结束之后每个元素留下的概率都是$\\frac{k}{n}$,现在把数据分为两部分,$k$之前和$k$之后。证明:对于第$I(I&lt;=k)$个元素,初始状态被留下的概率是1,到了第$k+1$步时,此时第$k+1$个数被留下的概率是$\\frac{k}{k+1}$(代码第3、4行,也就是从(1,k+1)中取到的j是前k个),我们从当前数组池A中随机取一个用来替换第$j$个元素,正好取到$I$的概率是$\\frac{1}{k}$,也就是说,该步中第$j$个数正好替换第$I$个数的概率是$\\frac{k}{k+1} \\times \\frac{1}{k}=\\frac{1}{k+1}$,其实说简单点也就是从(1,k+1)个数中正好取到第$I$个的概率.谨记我们的目标,我们要让第$I$个数留下,也就说第$k+1$步它留下的概率是$1-\\frac{1}{k+1}=\\frac{k}{k+1}$,同理,第$k+2$步它留下的概率是$\\frac{k+1}{k+2}$,则到了第$n$步,它仍然留下的概率是: 1 \\times \\frac{k}{k+1} \\times \\frac{k+1}{k+2} \\times \\frac{k+2}{k+3} \\times ... \\times \\frac{n-1}{n}=\\frac{k}{n}也就是说第$1$到$k$个元素留下的概率是$\\frac{k}{n}$,下面再看第$k+1~n$个元素对于第$J(k+1,n)$个元素,在第$J$步被选中留下的概率是$\\frac{k}{J}$,下面我们要在剩下的步骤中让他不被替换,不被替换的概率在上面我们已经计算过了,则在$J+1$步不被替换的概率是$\\frac{j}{J+1}$,则最终到第$n$步,第$J$个元素留下的概率是: \\frac{k}{j} \\times \\frac{j}{j+1} \\times \\frac{j+1}{j+2}... \\times \\frac{n-1}{n}=\\frac{k}{n}也就是说第$k+1$到$n$个元素留下的概率仍然是$\\frac{k}{n}$证毕!!!!!","categories":[],"tags":[{"name":"采样","slug":"采样","permalink":"https://lingyixia.github.io/tags/采样/"}]},{"title":"动态规划系列","slug":"DynamicProgramming","date":"2019-04-11T07:46:55.000Z","updated":"2021-09-19T18:01:25.945Z","comments":true,"path":"2019/04/11/DynamicProgramming/","link":"","permalink":"https://lingyixia.github.io/2019/04/11/DynamicProgramming/","excerpt":"动态规划常用体型,其实最大的难度是遇到一个问题如何将其归类,当归类正确的时候往往是写出的递推公式”似曾相识”的时候。","text":"动态规划常用体型,其实最大的难度是遇到一个问题如何将其归类,当归类正确的时候往往是写出的递推公式”似曾相识”的时候。 字符串问题最长公共子串 其实很简单,动态规划,记录矩阵dp[i][j]表示以s1[i]和s2[j]结尾的最长公共字串长度,可以看出如果s1[i]!=s2[j]则dp[i][j]一定是0,因为两者不相等,以他们为结尾不可能有公共字串,只有s1[i]==s2[j]的时候才不为0,即: dp[i][j]=\\begin{cases} 0 & s1[i]!=s2[j] \\\\ dp[i-1][j-1]+1 & s1[i]=s2[j] \\\\ \\end{cases} 1234567891011121314151617181920212223string maxLengthSubstr(string s1, string s2)&#123; int maxLength = 0; int index; vector&lt;vector&lt;int&gt;&gt; dp(s1.size() + 1, vector&lt;int&gt;(s2.size() + 1)); for(int i = 0; i &lt; s1.size(); i++) &#123; for(int j = 0; j &lt; s2.size(); j++) &#123; int left_up = i &gt; 0 &amp;&amp; j &gt; 0 ? dp[i - 1][j - 1] : 0; if(s1[i] == s2[j]) &#123; dp[i][j] = left_up + 1; &#125; if(dp[i][j] &gt; maxLength) &#123; maxLength = dp[i][j]; index = i; &#125; &#125; &#125; return s1.substr(index - maxLength, maxLength);&#125; 最长公共子序列 跟上面没有什么不同,动态规划,记录矩阵dp[i][j]表示以s1[i]和s2[j]结尾的最长公共子序列长度,可以看出如果s1[i]!=s2[j]则dp[i][j]= max(dp[i-1][j],dp[i][j-1]),这里是与上面dp公式唯一不同点,而当s1[i]=s2[j]的时候dp[i][j]=dp[i-1][j-1]+1,即: dp[i][j]=\\begin{cases} max(dp[i-1][j],dp[i][j-1]) & s1[i]!=s2[j] \\\\ dp[i-1][j-1]+1 & s1[i]=s2[j] \\\\ \\end{cases}123456789101112131415161718192021222324252627282930313233343536string maxLengthSequence(string s1, string s2)&#123; vector&lt;vector&lt;int&gt;&gt; dp(s1.size(), vector&lt;int&gt;(s2.size())); string result = &quot;&quot;; for(int i = 0; i &lt; s1.size(); ++i) &#123; for(int j = 0; j &lt; s2.size(); ++j) &#123; int left_up = i &gt; 0 &amp;&amp; j &gt; 0 ? dp[i - 1][j - 1] : 0; int left = j &gt; 0 ? dp[i][j - 1] : 0; int up = i &gt; 0 ? dp[i - 1][j] : 0; if(s1[i] == s2[j]) &#123; dp[i][j] = left_up + 1; // dp[i][j] = max(left_up + 1, max(left, up));//本来应该是 这行代码，但是dp[i-1][j]和dp[i][j-1]最多比dp[i][j]大1，因此没必要这样写 &#125; else &#123; dp[i][j] = max(left, up); &#125; &#125; &#125; for(int i = s1.size() - 1, j = s2.size() - 1; i &gt;= 0 &amp;&amp; j &gt;= 0;) &#123; if(s1[i] == s2[j]) &#123; result.push_back(s1[i]); i--; j--; &#125; else if(dp[i][j - 1] &gt;= dp[i - 1][j]) j--; else i--; &#125; std::reverse(result.begin(), result.end()); return result;&#125; 最长回文子串 dp[i][j]是记录从i到j是不是回文串,如果是则dp[i][j]=true,反之为false,因为每一次计算dp[i][j]都需要用到dp[i+1][j-1],而j-1&gt;=i+1,即j-i&gt;=2,即i到j的长度至少为3,也就是说间隔长度为1和2的更新不到,因此要先预处理dp,先把间隔为1和2的提前处理递推公式为: dp[i][j] = \\begin{cases} true & str[i]=str[j] and dp[i+1][j-1]=true\\\\ false & others \\end{cases}或 dp[i][j] = \\begin{cases} str[i+1][j-1] & str[i]=str[j]\\\\ false & others \\end{cases} 12345678910111213141516171819202122232425262728293031323334353637string longestPalindrome(string str)&#123; int index = 0; int maxLength = 1; vector&lt;vector&lt;bool&gt;&gt; dp(str.size(), vector&lt;bool&gt;(str.size(), false)); for (int i = 0; i &lt;= str.size(); i++) &#123; dp[i][i] = true; if (i + 1 &lt; str.size()) &#123; if (str[i] == str[i + 1]) &#123; dp[i][i + 1] = true; index = i; maxLength = 2; &#125; &#125; &#125; for (int l = 3; l &lt;= str.size(); l++) &#123; for (int i = 0; i + l - 1 &lt; str.size(); i++) &#123; int j = i + l - 1; if (str[i] == str[j] &amp;&amp; dp[i + 1][j - 1]) &#123; dp[i][j] = true; if (l &gt; maxLength) &#123; index = i; maxLength = l; &#125; &#125; &#125; &#125; return str.substr(index, maxLength);&#125; 最长回文子序列 跟上诉差不多,公式为: dp[i][j] = \\begin{cases} dp[i+1][j-1]+2 & str[i]=str[j] \\\\ max(dp[i+1][j],dp[i][j-1]) & others \\end{cases} 123456789101112131415161718192021int longestPalindromeSubseq(string str)&#123; vector&lt;vector&lt;int&gt;&gt; dp(str.size(), vector&lt;int&gt;(str.size(), 1)); for (int i = 0; i &lt; str.size()-1; i++) &#123; if (str[i + 1] == str[i]) dp[i][i + 1]++; &#125; for (int l = 3; l &lt;= str.size(); l++) &#123; for (int i = 0; i + l - 1 &lt; str.size(); i++) &#123; int j = i + l - 1; if (str[i] == str[j]) &#123; dp[i][j] = dp[i + 1][j - 1]+2; &#125; else dp[i][j] = max(dp[i + 1][j], dp[i][j - 1]); &#125; &#125; return dp[0][str.size()-1];&#125; 最长递增子序列 dp解法一,dp[i]为以第i个元素结尾的最长递增子序列长度,$O(n^2)$ 123456789101112131415161718int lengthOfLIS(vector&lt;int&gt;&amp; nums) &#123; if(nums.empty()) return 0; int result=1; vector&lt;int&gt; dp(nums.size(),1); for(int i =1;i&lt;nums.size();i++) &#123; for(int j =0;j&lt;i;j++)//不断更新dp[i]的值 &#123; if(nums[i]&gt;nums[j]) &#123; dp[i]=max(dp[j]+1,dp[i]); &#125; &#125; result=max(dp[i],result); &#125; return result; &#125; 背包问题 0-1背包、完全背包、多重背包问题简记.一般而言,背包问题有两种:要求恰好装满和不要求恰好装满,其实两者仅仅初始化数组不同,前者要初始化values[i][0]=0,其他全为-INT_MIN.后者初始化全为0即可。 0-1背包问题常规写法递推公式为:values[i][j] = max(values[i - 1][j], values[i-1][j - woods[i-1].volume] + woods[i-1].value);1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;struct Woods&#123; int volume; int value;&#125;;void backPack(vector&lt;Woods&gt;&amp; woods, int bag);void findItems(vector&lt;vector&lt;int&gt;&gt;&amp; values, vector&lt;Woods&gt;&amp; woods, vector&lt;int&gt;&amp; items, int i, int j);int main()&#123; vector&lt;Woods&gt; woods = &#123; Woods&#123;2,3&#125;,Woods&#123;3,4&#125;,Woods&#123;4,5&#125;,Woods&#123;5,6&#125; &#125;; backPack(woods, 8); return 0;&#125;void backPack(vector&lt;Woods&gt;&amp; woods, int bag)&#123; vector&lt;vector&lt;int&gt;&gt; values(woods.size() + 1, vector&lt;int&gt;(bag + 1)); for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = 1; j &lt;= bag; j++) &#123; if (j &lt; woods[i - 1].volume)//装不进去 &#123; values[i][j] = values[i - 1][j]; &#125; else//能装进去 &#123; values[i][j] = max(values[i - 1][j], values[i - 1][j - woods[i - 1].volume] + woods[i - 1].value); &#125; &#125; &#125; vector&lt;int&gt; items(woods.size() + 1);//保存的是最终放入了哪些物品 findItems(values, woods, items, woods.size(), bag);&#125;void findItems(vector&lt;vector&lt;int&gt;&gt;&amp; values, vector&lt;Woods&gt;&amp; woods, vector&lt;int&gt;&amp; items, int i, int j)&#123; if (i &gt; 0) &#123; if (values[i][j] == values[i - 1][j]) &#123; items[i] = 0; findItems(values, woods, items, i - 1, j); &#125; else &#123; items[i] = 1; findItems(values, woods, items, i - 1, j - woods[i - 1].volume); &#125; &#125;&#125; 上诉方法使用二维数组保存中间值values,values[i][j]表示当背包空间为j,允许使用的物品只有前两个的时候的最大价值，比较消耗空间，而且我们可以看出，对于每一步要求的values[i][j]而言，只依赖于values[i-1][:],即前一行，因此我们只需要一维数组即可，循环保存前一行数据。但是这样貌似就无法找物品组成了，回头在想。 空间优化写法1234567891011121314void backPack(vector&lt;Woods&gt;&amp; woods, int bag)&#123; vector&lt;int&gt; values(bag + 1); for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = bag; j &gt;=1; j--) &#123; if (j &gt;= woods[i - 1].volume)//放不进去 &#123; values[j] = max(values[j], values[j - woods[i - 1].volume] + woods[i - 1].value); &#125; &#125; &#125;&#125; 再次改善1234567891011void backPack(vector&lt;Woods&gt;&amp; woods, int bag)&#123; vector&lt;int&gt; values(bag + 1); for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = bag; j &gt;= woods[i - 1].volume; j--)//倒序 &#123; values[j] = max(values[j], values[j - woods[i - 1].volume] + woods[i - 1].value); &#125; &#125;&#125; 完全背包问题常规写法与0-1背包唯一的不同就是递推公式：values[i][j] = max(values[i - 1][j], values[i][j - woods[i-1].volume] + woods[i-1].value);不在详细说明 空间优化写法1234567891011void backPack(vector&lt;Woods&gt;&amp; woods, int bag)&#123; vector&lt;int&gt; values(bag + 1); for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = woods[i - 1].volume; j &lt;=bag ; j--)//正序 &#123; values[j] = max(values[j], values[j - woods[i - 1].volume] + woods[i - 1].value); &#125; &#125;&#125; 多重背包问题（即每种物品的数量有上限）常规写法在0-1背包的基础上增加数量循环判断,递推公式为:values[i][j] = max(values[i][j], values[i-1][j - k * woods[i - 1].volume] + k * woods[i - 1].value);1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;struct Woods&#123; int volume; int value; int num;&#125;;void backPack(vector&lt;Woods&gt;&amp; woods, int bag);int main()&#123; vector&lt;Woods&gt; woods = &#123; Woods&#123; 80 ,20 ,4 &#125;,Woods&#123; 40 ,50, 9 &#125;,Woods&#123; 30 ,50, 7 &#125;,Woods&#123; 40 ,30 ,6 &#125;,Woods&#123; 20 ,20 ,1 &#125; &#125;; backPack(woods, 1000); return 0;&#125;void backPack(vector&lt;Woods&gt;&amp; woods, int bag)&#123; vector&lt;vector&lt;int&gt;&gt; values(woods.size() + 1, vector&lt;int&gt;(bag + 1)); for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = 1; j &lt;= bag; j++) &#123; for (int k = 1; k &lt;= woods[i - 1].num; k++) &#123; if (j &lt; k * woods[i - 1].volume) break; values[i][j] = max(values[i-1][j], values[i - 1][j - k * woods[i - 1].volume] + k * woods[i - 1].value); &#125; &#125; &#125;&#125; 空间优化写法123456789101112131415void backPack(vector&lt;Woods&gt;&amp; woods, int bag)&#123; vector&lt;int&gt; values(bag + 1); for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = bag; j &gt;= 0; j--) &#123; for (int k = 1; k &lt;= woods[i - 1].num; k++) &#123; if (j &lt; k * woods[i - 1].volume) break; values[j] = max(values[j], values[j - k * woods[i - 1].volume] + k * woods[i - 1].value); &#125; &#125; &#125;&#125; 硬币问题 背包问题变种 硬币问题1最少硬币数量 1234567891011121314151617181920int coinChange(vector&lt;int&gt;&amp; coins, int amount) &#123; vector&lt;vector&lt;int&gt;&gt; values(coins.size() + 1, vector&lt;int&gt;(amount + 1,INT_MAX-1)); for (int i = 1; i &lt;= coins.size(); i++) &#123; values[i][0] = 0; for (int j = 1; j &lt;= amount; j++) &#123; if (j &lt; coins[i - 1]) &#123; values[i][j] = values[i - 1][j]; &#125; else &#123; values[i][j] = min(values[i - 1][j], values[i][j - coins[i - 1]] + 1); &#125; &#125; &#125; return values[coins.size()][amount]==INT_MAX-1?-1:values[coins.size()][amount];&#125; values[i][j]表示使用前i种硬币组成j至少需要多少个硬币 简化空间: 12345678910111213int coinChange(vector&lt;int&gt;&amp; coins, int amount) &#123; vector&lt;int&gt; values(amount + 1,INT_MAX-1); values[0]=0; for (int i = 1; i &lt;= coins.size(); i++) &#123; for (int j = coins[i - 1]; j &lt;= amount; j++) &#123; values[j] = min(values[j], values[j - coins[i - 1]] + 1); &#125; &#125; return values[amount]==INT_MAX-1?-1:values[amount];&#125; 硬币问题2能够组成该数量钱的情况数 123456789101112131415161718192021int change(int amount, vector&lt;int&gt;&amp; coins) &#123; vector&lt;vector&lt;int&gt;&gt; dp(coins.size()+1,vector&lt;int&gt;(amount+1)); dp[0][0]=1; for(int i =1;i&lt;=coins.size();i++) &#123; dp[i][0]=1; for(int j=1;j&lt;=amount;j++) &#123; if(j&lt;coins[i-1]) &#123; dp[i][j]=dp[i-1][j]; &#125; else &#123; dp[i][j]=dp[i-1][j]+dp[i][j-coins[i-1]]; &#125; &#125; &#125; return dp[coins.size()][amount]; &#125; dp[i][j]表示使用前i种硬币组成j的组成数量,dp[i][j]=dp[i-1][j]+dp[i][j-coins[i-1]]的意思，第一项表示不使用币种i组成j的种类数量，第二项表示至少使用一个币种i组成j的种类数，j-coins[i-1]就确保了至少使用一个币种i 简化空间:12345678910111213int change(int amount, vector&lt;int&gt;&amp; coins) &#123; vector&lt;int&gt; dp(amount+1); dp[0]=1; for(int i =1;i&lt;=coins.size();i++) &#123; for(int j=coins[i-1];j&lt;=amount;j++) &#123; dp[j]=dp[j]+dp[j-coins[i-1]]; &#125; &#125; return dp[amount]; &#125; 数组分裂数组分裂一 背包问题变种。其实就是数组中每个数字是一个物品,value=weight=数值,看能不能恰好装满背包的一半 123456789101112131415161718192021bool canPartition(vector&lt;int&gt;&amp; nums) &#123; int sum = 0; for(int i = 0;i&lt;nums.size();i++) &#123; sum+=nums[i]; &#125; if((sum&amp;1)==1) return false; int bag = sum/2; vector&lt;int&gt; values(bag+1,INT_MIN); values[0]=0; for(int i =1;i&lt;=nums.size();i++) &#123; for(int j = bag;j&gt;=nums[i-1];j--) &#123; values[j]=max(values[j],values[j-nums[i-1]]+nums[i-1]); if(values[j]==bag) return true; &#125; &#125; return false; &#125; 数组分裂二 背包问题变种。将数组分为两部分，求各自和的差的绝对值(两部分的和最接近)其实就是数组中每个数字是一个物品,value=weight=数值,target(bag)=sum/2,看这个target(bag)下最大的和是多少 12345678910111213141516171819202122232425int splitArray(vector&lt;int&gt; nums)&#123; int sum = 0; for (int i = 0; i &lt; nums.size(); i++) &#123; sum += nums[i]; &#125; int target = sum/2; vector&lt;vector&lt;int&gt;&gt; dp(nums.size() + 1, vector&lt;int&gt;(target + 1)); for (int i = 1; i &lt;=nums.size(); i++) &#123; for (int j = 1; j &lt;= target; j++) &#123; if (nums[i-1]&gt;j) &#123; dp[i][j] = dp[i-1][j]; &#125; else &#123; dp[i][j] = max(dp[i-1][j],dp[i-1][j-nums[i-1]]+nums[i-1]); &#125; &#125; &#125; return ( sum- target) - dp[nums.size()][target];&#125; 空间优化: 123456789101112131415161718int splitArray(vector&lt;int&gt; nums)&#123; int sum = 0; for (int i = 0; i &lt; nums.size(); i++) &#123; sum += nums[i]; &#125; int target = sum/2; vector&lt;int&gt; dp(target + 1); for (int i = 1; i &lt;=nums.size(); i++) &#123; for (int j = target; j &gt;= nums[i - 1]; j--) &#123; dp[j] = max(dp[j],dp[j-nums[i-1]]+nums[i-1]); &#125; &#125; return ( sum- target) - dp[target];&#125; 零和问题 背包问题变种，难点在于分清资源和目标,此题有两个目标 123456789101112131415161718192021222324int findMaxForm(vector&lt;string&gt; strs, int m, int n) &#123; vector&lt;vector&lt;vector&lt;int&gt;&gt;&gt; dp(strs.size() + 1, vector&lt;vector&lt;int&gt;&gt;(m + 1, vector&lt;int&gt;(n + 1))); for(int k = 1;k&lt;=strs.size();k++) &#123; int zeros = count(strs[k-1].begin(), strs[k-1].end(), &apos;0&apos;); int ones = count(strs[k-1].begin(), strs[k-1].end(), &apos;1&apos;); for (int i = 0; i &lt;= m; i++)//注意从零开始 &#123; for (int j = 0; j &lt;= n; j++)//注意从零开始 &#123; if (i&lt; zeros || j&lt;ones) &#123; dp[k][i][j] = dp[k-1][i][j]; &#125; else &#123; dp[k][i][j] = max(dp[k-1][i][j], dp[k-1][i - zeros][j - ones] + 1); &#125; &#125; &#125; &#125; return dp[strs.size()][m][n];&#125; 空间优化: 1234567891011121314151617int findMaxForm(vector&lt;string&gt;&amp; strs, int m, int n) &#123; vector&lt;vector&lt;int&gt;&gt; dp(m+1,vector&lt;int&gt;(n+1)); for(string str:strs) &#123; int ones = count(str.begin(),str.end(),&apos;1&apos;); int zeros = count(str.begin(),str.end(),&apos;0&apos;); for(int i = m;i&gt;=zeros;i--) &#123; for(int j = n;j&gt;=ones;j--) &#123; dp[i][j]=max(dp[i][j],dp[i-zeros][j-ones]+1); &#125; &#125; &#125; return dp[m][n];&#125; 编辑距离 dp[i][j]表示将word1[1:i]转为word2[1:j]所需要的最少次数 dp[i][j] = \\begin{cases} dp[i-1][j-1] & word1[i]==word2[j] \\\\ min(dp[i-1][j],dp[i][j],dp[i][j-1]) & word1[i]!=word2[j] \\end{cases}dp[i-1][j]、dp[i][j]、dp[i][j-1]表示三种转换方式，取其最小.dp[i-1][j]+1表示把word1[1:i-1]转为word2[1:j],然后删除words[i]dp[i-1][j-1]+1表示把word1[1:i-1]转为word2[1:j-1]然后把word1[i]替换为word2[j]dp[i][j-1]+1表示把word1[1:i]转为dp[1:j-1]然后把word2[j]插入到末尾 12345678910111213141516171819202122232425262728293031323334template &lt;typename T&gt;T min(T a,T b,T c)&#123; if(c&gt;b) c=b; if(c&gt;a) c=a; return c;&#125;int minDistance(string word1, string word2) &#123; vector&lt;vector&lt;int&gt;&gt; dp(word1.size()+1,vector&lt;int&gt;(word2.size()+1)); for(int i=1;i&lt;=word1.size();i++) &#123; dp[i][0]=i; &#125; for(int i=1;i&lt;=word2.size();i++) &#123; dp[0][i]=i; &#125; for(int i =1;i&lt;=word1.size();i++) &#123; for(int j =1;j&lt;=word2.size();j++) &#123; if(word1[i-1]==word2[j-1]) &#123; dp[i][j]=dp[i-1][j-1]; &#125; else &#123; dp[i][j]=min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])+1; &#125; &#125; &#125; return dp[word1.size()][word2.size()];&#125; 矩阵中最大正方形 同编辑距离.dp[i][j]记录以matrix[i][j]结尾的边长(要包含matrix[i][j]) dp[i][j] = \\begin{cases} dp[i+1][j-1]+2 & str[i]=str[j] \\\\ max(dp[i+1][j],dp[i][j-1]) & others \\end{cases}12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849template &lt;typename T&gt;T min(T a, T b, T c)&#123; if (c&gt;a) &#123; c = a; &#125; if (c&gt;b) &#123; c = b; &#125; return c;&#125;int maximalSquare(vector&lt;vector&lt;char&gt;&gt;&amp; matrix)&#123; int maxResult = 0; vector&lt;vector&lt;int&gt;&gt; dp(matrix.size(),vector&lt;int&gt;(matrix[0].size())); for (int i = 0; i &lt; matrix.size(); i++) &#123; if (matrix[i][0]==&apos;1&apos;) &#123; dp[i][0] = 1; maxResult = max(maxResult, 1); &#125; &#125; for (int i = 0; i &lt; matrix[0].size(); i++) &#123; if (matrix[0][i] == &apos;1&apos;) &#123; dp[0][i] = 1; maxResult = max(maxResult, 1); &#125; &#125; for (int i = 1; i &lt; dp.size(); i++) &#123; for (int j = 1; j &lt; dp[0].size(); j++) &#123; if (matrix[i][j]==&apos;0&apos;) &#123; dp[i][j] = 0; &#125; else &#123; dp[i][j] = min(dp[i-1][j-1],dp[i][j-1],dp[i-1][j])+1; maxResult= max(maxResult, dp[i][j]* dp[i][j]); &#125; &#125; &#125;&#125; 鹰蛋问题 问题大概就是一栋楼N层,你有K个一模一样的鸡蛋,问如何找到一个最高层,这个层扔下去刚好鸡蛋不碎,在上一层就碎了。此题变种较多,在编程角度是一个动态规划，在数学角度是一个趣味问答题，下面具体分析.数学角度:比如我们只有两枚蛋,让你给出一个方案，这个方案测试的次数最少，而且一定能找到这个楼层.难度在于思维转换,比如我们现在已经找到了这个次数X，不可能有某个方案的测试次数比X更小，那么X是多少呢?首先我们要转换的思维是:某个楼层,X是它最少的测试次数,意味着这个楼层是X能测试得到的最高楼层.所以我们把题转为这个最高楼层是多少。现在我们想一下,两个鸡蛋最少得有两次测试机会吧，那么如果X是2，这个最高楼层是多少呢?想想方案,如果第一个蛋测试的是5楼，咔，破了,这时候你只剩下一个蛋,只有一次机会了,但是这次机会你只能测试第一层啊,如果没破，机会没了,没测出来.也就是说,只有两次机会,你第一次不能测的太高,最高只能测试第2层.那最低呢?不需要最低，要得到最高楼层只需要考虑每个蛋放的最高楼层是多少才能得到最大效益，也就是说，如果最少次数是X，那第一次放置的最高楼层只能是X,这样才能保证在第一次破的时候剩下的蛋在X-1次内完成测试。那第二次呢?还剩X-1次机会那就在向上走X-1层呗，，，以此类推直到最顶层。也就是说，如果楼层是$F$，那最少次数只需要解方程: X+(X-1)+(X-2)+...+1=\\frac{X(X-1)}{2}≤F也就是说两个鸡蛋,X次机会最多可测试的楼层数是$\\frac{X(X-1)}{2}$那么三个鸡蛋X次的最大楼层是多少呢?那么同样，第一次首先要确定一个最高楼层，这个楼层下如果破了，剩下的2个蛋和X-1次机会能恰好测完.那么就可以这么想，第一次放在第f层，那剩下的2个蛋和X-1次机会最高到$\\frac{X(X-1)}{2}$,所以f就是$\\frac{(X-1)(X-2)}{2}$+1，至于为什么加1稍微想想就知道了，剩下的就一样了.(看到没，这题还可以改成如果n个蛋，X次机会，能测试的最高楼层是多少.)动态规划角度:同样利用上诉思维转换dp[i][j]表示j个鸡蛋i次机会能确定的最高楼层dp[l][i] = dp[l - 1][i - 1] + dp[l - 1][i] + 1这个地推公式的含义dp[l][i]:i个鸡蛋l次机会所能确定的最高楼层dp[l-1][i-1]:第一枚鸡蛋的最高位置dp[l - 1][i]:剩下鸡蛋能在此基础上累加的高度。其实上面这个公式是这个:dp[l][i] = max(dp[l - 1][i - 1], dp[l - 1][i - 1]+ dp[l - 1][i] + 1)前面代表第一个碎了，后面代表第一个没碎，因为前面肯定小于等于后面，所以就不写了. 1234567891011121314 int superEggDrop(int K, int N) &#123; vector&lt;vector&lt;int&gt;&gt; dp(N+1,vector&lt;int&gt;(K+1));//dp[i][j]表示j个鸡蛋i次机会能够确定的最高楼层 int l = 0; while (dp[l][K] &lt; N) &#123; l++; for (int i = 1; i &lt;= K; i++) &#123; dp[l][i] = dp[l - 1][i - 1] + dp[l - 1][i] + 1; &#125; &#125; return l;&#125; 其他DP连续数组最大和 给出一个数组，求连续数组最大和.//递推公式为DP[i] = max{DP[i-1] + A[i],DP[i]},对于当前点i,要么与前面连起来组成和,要么自己组成和 1234567891011int maxSubArray(vector&lt;int&gt;&amp; nums) &#123; int before = 0;//before就是Dp[i-1] int maxSum = INT_MIN; for (int i = 0; i &lt;nums.size(); i++) &#123; before = max(before+nums[i],nums[i]); maxSum = max(maxSum,before); &#125; return maxSum;&#125; 连续数组最大积 与最大和不同点在于数组中的负数会对积有影响，最大和只需要记录当前最大和即可，但是最大积需要记录最大积和最小积,举例说明:当前数字是-1，之前最大积是5,最小积是-6,那么到此时最大积应当是-1 * -6=6而不是-1. 123456789101112131415int maxProduct(vector&lt;int&gt; &amp;nums)&#123; int result = INT_MIN; int beforeMaxProduct = 1; int beforeMinProduct = 1; for (int i = 0; i &lt; nums.size(); ++i) &#123; int tempMax = beforeMaxProduct * nums[i]; int tempMin = beforeMinProduct * nums[i]; beforeMaxProduct = max(max(tempMax, tempMin), nums[i]); beforeMinProduct = min(min(tempMax, tempMin), nums[i]); result = max(beforeMaxProduct, result); &#125; return result;&#125; 数组最大积(不一定连续)123456789101112131415int maxProduct(vector&lt;int&gt; &amp;nums)&#123; int result = INT_MIN; int beforeMaxProduct = 1; int beforeMinProduct = 1; for (int i = 0; i &lt; nums.size(); ++i) &#123; int tempMax = beforeMaxProduct * nums[i]; int tempMin = beforeMinProduct * nums[i]; beforeMaxProduct = max(max(tempMax, tempMin), beforeMaxProduct); beforeMinProduct = min(min(tempMax, tempMin), beforeMinProduct); result = max(beforeMaxProduct, result); &#125; return result;&#125; 最大子矩阵和 和上面基本一样 1234567891011121314151617181920212223242526272829int calclineSum(vector&lt;int&gt; array)//完全和上面一样&#123; int before=0; int maxSum = INT_MIN; for (int i = 0; i &lt; array.size(); i++) &#123; before = max(before+array[i],array[i]); maxSum = max(maxSum,before); &#125; return maxSum;&#125;int maxSumMatrix(vector&lt;vector&lt;int&gt;&gt; matrix)&#123; int maxSum = INT_MIN; for (int i = 0; i &lt; matrix.size(); i++) &#123; vector&lt;int&gt; lineSum(matrix[0].size()); for (int j = i; j &lt; matrix.size(); j++) &#123; for (int k = 0; k &lt; matrix[0].size(); k++) &#123; lineSum[k] += matrix[j][k]; &#125; maxSum = max(maxSum, calclineSum(lineSum)); &#125; &#125; return maxSum;&#125; 小偷1234567891011121314151617181920212223int rob(vector&lt;int&gt; &amp;nums)&#123; if (nums.size() == 0) &#123; return 0; &#125; else if (nums.size() == 1) &#123; return nums.back(); &#125; else if (nums.size() == 2) &#123; return max(nums.front(), nums.back()); &#125; else &#123; vector&lt;int&gt; dp(nums.size()); dp[0] = nums[0]; dp[1] = max(nums[0], nums[1]); for (int i = 2; i &lt; nums.size(); ++i) &#123; dp[i] = max(dp[i - 2] + nums[i], dp[i - 1]); &#125; return dp.back(); &#125;&#125; Jump Game贪心一: 每到一个i,如果i&lt;=reach意味着[0,i-1]的坐标能达到reach,如果i&gt;reach,则意味着根本就到不了这里,无需继续。 12345678910111213bool canJump(vector&lt;int&gt;&amp; nums) &#123; int reach = 0; for (int i =0; i &lt; nums.size(); i++) &#123; if (i &gt; reach || i &gt;= nums.size() - 1) break; else &#123; reach = max(reach, i + nums[i]); &#125; &#125; return reach &gt;= nums.size() - 1;&#125; 贪心二: 和上诉形式不一样,思想差不多 123456789101112bool canJump(vector&lt;int&gt;&amp; nums) &#123; int len = nums.size(); int curMax = nums[0]; for (int i = 0; i &lt;= curMax; i++) &#123; if (nums[i] + i &gt;= len - 1) return true; curMax = max(curMax, nums[i] + i); &#125; return false;&#125; 动态规划: dp[i]表示到达i时候最多还剩下多少步 12345678910bool canJump(vector&lt;int&gt;&amp; nums) &#123; vector&lt;int&gt; dp(nums.size(), 0); for (int i = 1; i &lt; nums.size(); ++i) &#123; dp[i] = max(dp[i - 1], nums[i - 1]) - 1; if (dp[i] &lt; 0) return false; &#125; return true;&#125; 染色问题12345678910111213int numWays(int n, int k)&#123; if (n == 0) return 0; int same = 0; int different = k; for (int i = 2; i &lt;= n; ++i) &#123; int temp = different; different = (same + different) * (k - 1); same = temp; &#125; return same + different;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"字符串","slug":"字符串","permalink":"https://lingyixia.github.io/tags/字符串/"},{"name":"背包","slug":"背包","permalink":"https://lingyixia.github.io/tags/背包/"},{"name":"递归","slug":"递归","permalink":"https://lingyixia.github.io/tags/递归/"},{"name":"硬币","slug":"硬币","permalink":"https://lingyixia.github.io/tags/硬币/"}]},{"title":"树的常见算法","slug":"Trees","date":"2019-04-11T04:20:03.000Z","updated":"2021-09-19T18:01:25.949Z","comments":true,"path":"2019/04/11/Trees/","link":"","permalink":"https://lingyixia.github.io/2019/04/11/Trees/","excerpt":"","text":"二叉树的遍历preOrderTraversal12345678910111213141516171819202122232425struct TreeNode &#123; int val; TreeNode *left; TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; &#125;;vector&lt;int&gt; preorderTraversal(TreeNode* root)&#123; vector&lt;int&gt; result; stack&lt;TreeNode*&gt; treeNodeStack; TreeNode* cursor = root; while (cursor || treeNodeStack.size()&gt;0) &#123; while (cursor) &#123; result.push_back(cursor-&gt;val); treeNodeStack.push(cursor); cursor = cursor-&gt;left; &#125; cursor = treeNodeStack.top(); treeNodeStack.pop(); cursor = cursor-&gt;right; &#125; return result;&#125; 123456789101112131415161718192021222324#先序简单vector&lt;int&gt; preorderTraversal(TreeNode* root) &#123; vector&lt;int&gt; result; if(!root) return result; stack&lt;TreeNode*&gt; s; TreeNode* cursor = root; s.push(root); while(!s.empty()) &#123; cursor=s.top(); result.push_back(cursor-&gt;val); s.pop(); if(cursor-&gt;right) &#123; s.push(cursor-&gt;right); &#125; if(cursor-&gt;left) &#123; s.push(cursor-&gt;left); &#125; &#125; return result;&#125; inOrderTraversal12345678910111213141516171819202122232425struct TreeNode &#123; int val; TreeNode *left; TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; &#125;;vector&lt;int&gt; inorderTraversal(TreeNode* root)&#123; vector&lt;int&gt; result; stack&lt;TreeNode*&gt; treeNodeStack; TreeNode* cursor = root; while (cursor || treeNodeStack.size()&gt;0) &#123; while (cursor) &#123; treeNodeStack.push(cursor); cursor = cursor-&gt;left; &#125; cursor = treeNodeStack.top(); result.push_back(cursor-&gt;val); treeNodeStack.pop(); cursor = cursor-&gt;right; &#125; return result;&#125; 只是简单的把先序遍历28行挪了一下 12345678910111213141516171819202122232425262728293031323334vector&lt;int&gt; postorderTraversal(TreeNode* root)&#123; vector&lt;int&gt; result; if(!root) &#123; return result; &#125; stack&lt;TreeNode*&gt; treeNodeStack; TreeNode* preNode = NULL; TreeNode* cursor = root; treeNodeStack.push(cursor); while (treeNodeStack.size() &gt; 0) &#123; cursor = treeNodeStack.top(); if ((!cursor-&gt;left &amp;&amp; !cursor-&gt;right) || (preNode &amp;&amp; preNode == cursor-&gt;left) || (preNode &amp;&amp; preNode == cursor-&gt;right)) &#123; result.push_back(cursor-&gt;val); preNode = cursor; treeNodeStack.pop(); &#125; else &#123; if (cursor-&gt;right) &#123; treeNodeStack.push(cursor-&gt;right); &#125; if (cursor-&gt;left) &#123; treeNodeStack.push(cursor-&gt;left); &#125; &#125; &#125; return result;&#125; postOrderTraversal12345678910111213141516171819202122232425262728293031323334353637383940struct TreeNode &#123; int val; TreeNode *left; TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; &#125;;vector&lt;int&gt; postorderTraversal(TreeNode* root)&#123; vector&lt;int&gt; result; if(!root) &#123; return result; &#125; stack&lt;TreeNode*&gt; treeNodeStack; TreeNode* preNode = NULL; TreeNode* cursor = root; treeNodeStack.push(cursor); while (treeNodeStack.size() &gt; 0) &#123; cursor = treeNodeStack.top(); if ((!cursor-&gt;left &amp;&amp; !cursor-&gt;right) || (preNode &amp;&amp; preNode == cursor-&gt;left) || (preNode &amp;&amp; preNode == cursor-&gt;right)) &#123; result.push_back(cursor-&gt;val); preNode = cursor; treeNodeStack.pop(); &#125; else &#123; if (cursor-&gt;right) &#123; treeNodeStack.push(cursor-&gt;right); &#125; if (cursor-&gt;left) &#123; treeNodeStack.push(cursor-&gt;left); &#125; &#125; &#125; return result;&#125; 123456789101112131415161718192021222324252627282930#后序拿offer算法struct TreeNode &#123; int val; TreeNode *left; TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; &#125;; vector&lt;int&gt; postorderTraversal(TreeNode* root) &#123; stack&lt;TreeNode*&gt; s; stack&lt;TreeNode*&gt; resultTemp; vector&lt;int&gt; result; if(!root) return result; TreeNode* cursor = root; s.push(cursor); while(!s.empty()) &#123; cursor=s.top(); s.pop(); resultTemp.push(cursor); if(cursor-&gt;left) s.push(cursor-&gt;left); if(cursor-&gt;right) s.push(cursor-&gt;right); &#125; while(!resultTemp.empty()) &#123; result.push_back(resultTemp.top()-&gt;val); resultTemp.pop(); &#125; return result;&#125; 多叉树层序遍历 这个和多叉树的非递归深度重复 12345678910111213141516171819202122232425262728293031323334353637383940414243class Node &#123;public: int val; vector&lt;Node*&gt; children; Node() &#123;&#125; Node(int _val, vector&lt;Node*&gt; _children) &#123; val = _val; children = _children; &#125;&#125;;class Solution&#123;public: vector &lt;vector&lt;int&gt;&gt; levelOrder(Node *root) &#123; vector &lt;vector&lt;int&gt;&gt; result; if (!root) &#123; return result; &#125; queue &lt; Node * &gt; q; q.push(root); Node *cursor = root; vector&lt;int&gt; temp; while (!q.empty()) &#123; temp.clear(); int size = q.size(); for (int i = 0; i &lt; size; i++) &#123; cursor = q.front(); temp.push_back(cursor-&gt;val); q.pop(); for (int j = 0; j &lt; cursor-&gt;children.size(); j++) &#123; q.push(cursor-&gt;children[j]); &#125; &#125; result.push_back(temp); &#125; return result; &#125;&#125;; 如果是二叉树只需要两个循环就可以了，因为最内层循环只需要改成左右子树即可 判断子树 输入两棵二叉树A，B，判断B是不是A的子结构。(ps：我们约定空树不是任意一个树的子结构) 12345678910111213141516171819bool HasSubtree(TreeNode* pRoot1, TreeNode* pRoot2)&#123; if(!pRoot1 || !pRoot2) return false; bool result = false; if(pRoot1-&gt;val==pRoot2-&gt;val) result = isSubTree(pRoot1,pRoot2); if(!result) result = HasSubtree(pRoot1-&gt;left,pRoot2); if(!result) result = HasSubtree(pRoot1-&gt;right,pRoot2); return result;&#125;bool isSubTree(TreeNode* pRoot1,TreeNode* pRoot2)&#123; if(!pRoot2) return true; if(!pRoot1) return false; bool result = false; if(pRoot1-&gt;val==pRoot2-&gt;val) result = true; if(result) result = isSubTree(pRoot1-&gt;left,pRoot2-&gt;left); if(result) result = isSubTree(pRoot1-&gt;right,pRoot2-&gt;right); return result;&#125; 需要注意得地方是返回bool判断左树右树的逻辑 多叉树最大深度 题意为输出多叉树最大深度,最容易想到的方式是递归,该题记录重点为第二种非递归方式,其实也相当于多叉树的层序遍历。 递归123456789101112131415161718192021222324252627282930313233#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;vector&gt;using namespace std;class Node &#123;public: int val; vector&lt;Node*&gt; children; Node() &#123;&#125; Node(int _val, vector&lt;Node*&gt; _children) &#123; val = _val; children = _children; &#125;&#125;;int maxDepth(Node* root);int main()&#123; int maxDepth(Node* root); return 0;&#125;int maxDepth(Node* root) &#123; if(!root)//出口条件 &#123; return 0; &#125; int maxDep=0; for(int i =0;i&lt;root-&gt;children.size();i++) &#123; maxDep=max(maxDep,maxDepth(root-&gt;children[i])); &#125; return maxDep+1;&#125; 非递归12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;vector&gt;using namespace std;class Node &#123;public: int val; vector&lt;Node*&gt; children; Node() &#123;&#125; Node(int _val, vector&lt;Node*&gt; _children) &#123; val = _val; children = _children; &#125;&#125;;int maxDepth(Node* root);int main()&#123; int maxDepth(Node* root); return 0;&#125;int maxDepth(Node* root)&#123; if(!root)//只是当root为空时候的一个判断 &#123; return 0; &#125; int depth = 0; queue&lt;Node*&gt; q; q.push(root); Node* cursor = NULL; while (!q.empty())//这里的三层循环第二层目的是将每一层都组成一个数组,并计算层序,如果单纯为了输出层序遍历的结果完全可以使用两层循环 &#123; int size = q.size(); for (int i = 0; i &lt; size;i++)//注意该队列其实是一层一层从做到右入队列的,切不可用while(!q.empty()) &#123; cursor = q.front(); q.pop(); for (int j = 0; j &lt; cursor-&gt;children.size(); j++) &#123; q.push(cursor-&gt;children[j]); &#125; &#125; depth++;//每层加1 &#125; return depth;&#125; 注意，两个方法中的if(!root)作用并不一样，对于递归而言是跳出条件，而对于非递归而言只是一种情况 二叉树镜像 把一个二叉树变为镜像 1234567891011void Mirror(TreeNode *pRoot) &#123; if(pRoot) &#123; TreeNode* node = pRoot-&gt;left; pRoot-&gt;left=pRoot-&gt;right; pRoot-&gt;right=node; Mirror(pRoot-&gt;left); Mirror(pRoot-&gt;right); &#125;&#125; 对称二叉树1234567891011bool judge(TreeNode* p,TreeNode* q)&#123; if(!p &amp;&amp; !q) return true; if(!p || !q) return false; if(p-&gt;val != q-&gt;val) return false; return judge(p-&gt;left,q-&gt;right) &amp;&amp; judge(p-&gt;right,q-&gt;left);&#125;bool isSymmetrical(TreeNode* pRoot)&#123; return judge(pRoot,pRoot);&#125; 树的子结构(或子树) 给定两棵二叉树A和B，判断A是不是B的子结构或子树，子结构要求A是B的一部分即可，但是子树要求叶子结点也得相同. 12345678910111213141516171819202122bool judgeSubTree(TreeNode *pRoot1, TreeNode *pRoot2)&#123; //判断子结构 if (!pRoot2) return true; if (!pRoot1) return false; //判断子树 //if (!pRoot1 &amp;&amp; !pRoot2) return true; //if (!(pRoot1 &amp;&amp; pRoot2)) return false; bool judge = pRoot1-&gt;val == pRoot2-&gt;val; if (judge) judge = judgeSubTree(pRoot1-&gt;left, pRoot2-&gt;left); if (judge) judge = judgeSubTree(pRoot1-&gt;right, pRoot2-&gt;right); return judge;&#125;bool isSubtree(TreeNode *pRoot1, TreeNode *pRoot2)&#123; if (!pRoot1 || !pRoot2) return false; bool judge = judgeSubTree(pRoot1, pRoot2); if (!judge) judge = isSubtree(pRoot1-&gt;left, pRoot2); if (!judge) judge = isSubtree(pRoot1-&gt;right, pRoot2); return judge;&#125; 二叉树中路径和 输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) 123456789101112131415161718192021222324252627vector&lt;vector&lt;int&gt;&gt; result;void FindPath(TreeNode* root,int expectNumber,vector&lt;int&gt;&amp; path)&#123; if(!root) return; path.push_back(root-&gt;val); expectNumber-=root-&gt;val; if(!root-&gt;left &amp;&amp; !root-&gt;right &amp;&amp; !expectNumber) &#123; result.push_back(path); &#125; if(root-&gt;left) &#123; FindPath(root-&gt;left,expectNumber,path); &#125; if(root-&gt;right) &#123; FindPath(root-&gt;right,expectNumber,path); &#125; path.pop_back();&#125;vector&lt;vector&lt;int&gt; &gt; FindPath(TreeNode* root,int expectNumber) &#123; if(!root) return result; vector&lt;int&gt; path; FindPath(root,expectNumber,path); return result;&#125; 二叉树所有路径中和最大 这里的路径指的是树上所有能联通的两个节点之间的路径 1234567891011121314151617181920int maxSumResult = 0;int maxPathSum(TreeNode *root)&#123; if (!root) return 0; int left = maxPathSum(root-&gt;left); int right = maxPathSum(root-&gt;right); int temp = root-&gt;val; if (left &gt; 0) &#123; temp += left; &#125; if (right &gt; 0) &#123; temp += right; &#125; maxSumResult = max(maxSumResult, temp); return max(root-&gt;val, max(root-&gt;val + left, root-&gt;val + right));&#125; 调用maxPathSum后maxSumResult得到的就是最大路径和 判断是不是平衡二叉树1234567891011121314151617int getHeight(TreeNode* root,bool&amp; ifBalance)&#123; if(!root) return 0; int left = getHeight(root-&gt;left,ifBalance); int right =getHeight(root-&gt;right,ifBalance); if(abs(left-right)&gt;1) &#123; ifBalance=false; &#125; return left&gt;right?left+1:right+1;&#125;bool IsBalanced_Solution(TreeNode* pRoot) &#123; bool ifBalance=true; getHeight(pRoot,ifBalance); return ifBalance;&#125; next指针 完全二叉树增加next指针指向右侧节点 只适用于完全二叉树 12345678910111213141516171819Node *connectCore(Node *root)&#123; if (!root-&gt;left &amp;&amp; !root-&gt;right) return NULL; root-&gt;left-&gt;next = root-&gt;right; connectCore(root-&gt;left); if (root-&gt;next) &#123; root-&gt;right-&gt;next = root-&gt;next-&gt;left; &#125; connectCore(root-&gt;right); return root;&#125;Node *connect(Node *root)&#123; if (!root || (!root-&gt;left &amp;&amp; !root-&gt;right)) return root; return connectCore(root);&#125; 层序遍历，适用于任何形式的二叉树 12345678910111213141516171819202122232425Node *connect(Node *root)&#123; if(!root ||(!root-&gt;left &amp;&amp; !root-&gt;right)) return root; Node *cursor = root; queue&lt;Node *&gt; q; q.push(cursor); while (!q.empty()) &#123; int size = q.size(); for (int i = 0; i &lt; size; ++i) &#123; cursor = q.front(); q.pop(); cursor-&gt;next = (i==size-1)? nullptr:q.front(); if(cursor-&gt;left) &#123; q.push(cursor-&gt;left); &#125; if(cursor-&gt;right) &#123; q.push(cursor-&gt;right); &#125; &#125; &#125; return root; 二叉树从右向左看123456789101112131415161718void fromRight(TreeNode* root,vector&lt;int&gt;&amp; result,int deep) &#123; if(!root) &#123; return; &#125; if(deep==result.size()) &#123; result.push_back(root-&gt;val); &#125; fromRight(root-&gt;right,result,deep+1); fromRight(root-&gt;left,result,deep+1); &#125; vector&lt;int&gt; rightSideView(TreeNode* root) &#123; vector&lt;int&gt; result; fromRight(root,result,0); return result; &#125; 这是用中右左的方式遍历,若从左向右看可以中左右的方式遍历 找到最近公共祖先12345678TreeNode *lowestCommonAncestor(TreeNode *root, TreeNode *p, TreeNode *q)&#123; if (root == NULL || root == p || root == q) return root; TreeNode *left = lowestCommonAncestor(root-&gt;left, p, q); TreeNode *right = lowestCommonAncestor(root-&gt;right, p, q); if (left != NULL &amp;&amp; right != NULL) return root; return left == NULL ? right : left;&#125; 二叉树最大宽度 还不知道咋解决溢出问题…先这样 BFS 1234567891011121314151617181920212223242526272829303132int widthOfBinaryTree(TreeNode *root)&#123; if (!root) return 0; queue&lt;TreeNode *&gt; q; q.push(root); unordered_map&lt;TreeNode *, int&gt; record; record[root] = 1; int result = 1; while (!q.empty()) &#123; TreeNode *leftNode = q.front(); int leftEdge = record[leftNode]; int size = q.size(); for (int i = 0; i &lt; size; ++i) &#123; TreeNode *currentNode = q.front(); q.pop(); result = max(result, record[currentNode] - leftEdge + 1); if (currentNode-&gt;left) &#123; q.push(currentNode-&gt;left); record[currentNode-&gt;left] = 2 * record[currentNode]; &#125; if (currentNode-&gt;right) &#123; q.push(currentNode-&gt;right); record[currentNode-&gt;right] = 2 * record[currentNode] + 1; &#125; &#125; &#125; return result;&#125; DFS 123456789101112131415161718192021int result = 0;void DFS(TreeNode *root, int level, int order, unordered_map&lt;int, int&gt; record)&#123; if (!root) return; if (record.find(level) == record.end()) &#123; record[level] = order; &#125; result = max(result, order - record[level] + 1); DFS(root-&gt;left, level + 1, order * 2, record); DFS(root-&gt;right, level + 1, order * 2 + 1, record);&#125;int widthOfBinaryTree(TreeNode *root)&#123; if (!root) return 0; unordered_map&lt;int, int&gt; record; DFS(root, 0, 1, record); return result;&#125; 判断二叉搜索树12345678910111213141516171819 bool isValidDFS(TreeNode *root, long low, long high)&#123; if (!root) return true; if (root-&gt;val &gt; low &amp;&amp; root-&gt;val &lt; high) &#123; bool result = isValidDFS(root-&gt;left, low, root-&gt;val); if (result) &#123; result = isValidDFS(root-&gt;right, root-&gt;val, high); &#125; return result; &#125; return false;&#125;bool isValidBST(TreeNode *root)&#123; return isValidDFS(root, LONG_MIN, LONG_MAX);&#125; 更大的二茬搜索123456789101112131415161718int currVal = 0;void helper(TreeNode *root)&#123; if (root) &#123; helper(root-&gt;right); root-&gt;val += currVal; currVal = root-&gt;val; helper(root-&gt;left); &#125;&#125;TreeNode *bstToGst(TreeNode *root)&#123; helper(root); return root;&#125; 判断完全二叉树12345678910111213141516171819202122232425262728293031323334 bool isCompleteTree(TreeNode *root)&#123; if (!root) return true; queue&lt;TreeNode *&gt; record; record.push(root); bool flag = true; while (!record.empty()) &#123; TreeNode *front = record.front(); record.pop(); if (!flag &amp;&amp; (front-&gt;left || front-&gt;right)) &#123; return false; &#125; if(front-&gt;left &amp;&amp; front-&gt;right) &#123; record.push(front-&gt;left); record.push(front-&gt;right); &#125; else if (front-&gt;right) &#123; return false; &#125; else if (front-&gt;left) &#123; flag = false; record.push(front-&gt;left); &#125; else &#123; flag = false; &#125; &#125; return true;&#125; 二叉树中序遍历的前驱和后继前驱: 1.如果当前节点有左子树则找左子树的最右2.如果当前节点没有左子树则找其父节点，直到某节点是父节点的右子树，返回该父节点 123456789struct TreeLinkNode &#123; int val; struct TreeLinkNode *left; struct TreeLinkNode *right; struct TreeLinkNode *next; TreeLinkNode(int x) :val(x), left(NULL), right(NULL), next(NULL) &#123; &#125;&#125;; 后继 1.如果当前节点右右子树则找右子树的最左2.如果当前节点没有右子树则找其父节点，直到该某节点是父节点的左子树，返回该父节点. 12345678910111213141516171819202122232425262728struct TreeLinkNode&#123; int val; struct TreeLinkNode *left; struct TreeLinkNode *right; struct TreeLinkNode *next; TreeLinkNode(int x) :val(x), left(NULL), right(NULL), next(NULL) &#123; &#125;&#125; TreeLinkNode* GetNext(TreeLinkNode* pNode) &#123; if(pNode ==nullptr) return nullptr; // 假如有右子树，就去找右子树的最左节点 if(pNode-&gt;right)&#123; TreeLinkNode * res=pNode-&gt;right; while(res-&gt;left) res=res-&gt;left; return res; &#125; //没有右子树时，就一直向上找，找到当前节点是父节点的左孩子为止 TreeLinkNode *father=pNode-&gt;next; TreeLinkNode * cur=pNode; while(father != nullptr &amp;&amp; father-&gt;left != cur)&#123; cur=father; father=father-&gt;next; &#125; return father; &#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"Tree","slug":"Tree","permalink":"https://lingyixia.github.io/tags/Tree/"}]},{"title":"PCA和LDA","slug":"PCA","date":"2019-04-09T03:27:13.000Z","updated":"2021-10-26T03:45:35.626Z","comments":true,"path":"2019/04/09/PCA/","link":"","permalink":"https://lingyixia.github.io/2019/04/09/PCA/","excerpt":"参考1参考2原理就是一句话,把原来的$A$个维度映射到$(B","text":"参考1参考2原理就是一句话,把原来的$A$个维度映射到$(B","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[{"name":"线性代数","slug":"线性代数","permalink":"https://lingyixia.github.io/tags/线性代数/"}]},{"title":"SVM","slug":"SVM","date":"2019-04-08T06:25:07.000Z","updated":"2021-09-19T18:01:25.947Z","comments":true,"path":"2019/04/08/SVM/","link":"","permalink":"https://lingyixia.github.io/2019/04/08/SVM/","excerpt":"","text":"svm总结 拉格朗日对偶性 \\min_x \\quad f(x)\\\\ s.t.\\quad c_i(x)=0}L(x,\\alpha,\\beta)定义拉格朗日极小极大问题为: \\min_x \\theta_P(x)=\\min_x\\max_{\\alpha,\\beta:\\alpha_i>=0}L(x,\\alpha,\\beta)定义原始问题的最优值: p^*=\\min_x \\theta_P(x)对偶问题 \\theta_D(\\alpha,\\beta)=\\min_xL(x,\\alpha,\\beta)定义拉格朗日极大极小问题为: \\max_{\\alpha,\\beta:\\alpha_i>=0}\\theta_D(\\alpha,\\beta)=\\max_{\\alpha,\\beta:\\alpha_i>=0}\\min_xL(x,\\alpha,\\beta)定义对偶问题的最优值: d^*=\\max_{\\alpha,\\beta:\\alpha_i>=0}\\theta_D(\\alpha,\\beta)原始问题和对偶问题的关系很显然: d^*=\\max_{\\alpha,\\beta:\\alpha_i>=0}\\min_xL(x,\\alpha,\\beta)=0}L(x,\\alpha,\\beta)=p^*KTT条件 \\Deta硬间隔最大化假设最终分离超平面表达式为: wx+b=0则任意一点到该超平面的函数间隔为: \\hat{\\gamma_i}=y_i(wx_i+b)SVM的基本思维就是使任意一点到超平面的最小距离最大,但是函数距离不行,需要使用几何距离: \\gamma_i=y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\\\ \\gamma=\\min_{i=1,2,3...N}\\gamma_i现在可以得到优化表达式: \\max_{w,b} \\quad \\gamma \\\\ s.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||})>=\\gamma \\quad i=1,2...N试想,其实$\\gamma$无论是多少对最终值都没有任何影响,毕竟$w$和$b$是可以等比例变化的,现在我们假设$\\gamma$=$\\frac{1}{||w||}$得到: \\max_{w,b} \\quad \\frac{1}{||w||} \\\\ s.t. \\quad y_i(wx_i+b)>=1为了向拉格朗日函数靠拢,改写为: \\min_{w,b} \\quad \\frac{1}{2}||w||^2 \\\\ s.t. \\quad y_i(wx_i+b)-1>=0 \\quad i=1,2,3...N拉格朗日函数: L(w,b,\\alpha)=\\frac{1}{2}||w||^2+\\sum_{i=1}^N \\alpha_i-\\sum_{i=1}^N\\alpha_iy_i(wx_i+b)SVM原始问题: \\min_x \\theta_P(w,b)=\\min_{w,b}\\max_{\\alpha}L(w,b,\\alpha)对偶问题: \\max_{\\alpha}\\min_{w,b}L(w,b,\\alpha)现在要求对偶问题的解:先求$\\min_{w,b}L(w,b,\\alpha)$,由拉格朗日多元函数条件极值可知,极值必然在各个变量导数为0的位置: \\nabla_wL(w,b,\\alpha)=0 \\\\ \\nabla_bL(w,b,\\alpha) = 0\\\\得到: w=\\sum_{i=1}^N\\alpha_iy_ix_i \\\\ \\sum_{i=1}^N\\alpha_iy_i=0从$w$的公式可知模型参数$w$可以完全用训练数据和$\\alpha$计算得出。模型在优化的过程中保存的是参数$\\alpha$,优化完$\\alpha$后可以直接算出$w$.将$w$的公式带回$L(w,b,\\alpha)$,则现在: \\min_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_ix_j)+\\sum_{i=1}^N\\alpha_i现在自然要求的是极大化上诉公式,即$\\max{\\alpha}\\min{w,b}L(w,b,\\alpha)$: \\min_{\\alpha} \\quad \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_ix_j)-\\sum_{i=1}^N\\alpha_i \\\\ s.t. \\quad \\sum_{i=1}^N\\alpha_iy_i=0 \\\\ \\alpha_i>=0 \\quad i=1,2,3...N剩下的就是SMO算法了,不在这里详解. 软间隔最大化对每个点到分离超平面的距离不在硬性要求大于1,而是引入一个松弛变量。 \\min_{w,b} \\quad \\frac{1}{2}||w||^2 +C\\sum_{i=1}^N \\xi_i \\\\ s.t. \\quad y_i(wx_i+b)-1 + \\xi_i>=0 \\quad i=1,2,3...N \\\\ \\xi_i>=0 \\quad i=1,2,3...N计算过程不在详细说明,跟上面差不多,就是多了一个优化参数。参数说明:$C$越大表示对误分类的惩罚越大，即允许分错的样本越少 合页损失函数 \\sum_{i=1}^N[1-y_i(wx_i+b)]_++\\lambda||w||^2下面证明合页损失函数和软间隔最大化优化公式等价.令: [1-y_i(wx_i+b)]_+=\\xi_i必然有$\\xi_i$&gt;=0,可以看出,当$1-y_i(wx_i+b)&gt;0$时,$y_i(wx_i+b)=1-\\xi_i$,当$1-y_i(wx_i+b)&lt;=0$时,$\\xi_i=0$,故$y_i(wx_i+b)&gt;=1-\\xi_i$恒成立。故合页损失函数实际上可以写作: \\min_{w,b} \\sum_{i=1}^N\\xi_i+\\lambda||w||^2取$\\lambda=\\frac{1}{2C}$得: \\min_{w,b}\\frac{1}{C}(\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i) \\\\ s.t. \\quad y_i(wx_i+b)>=1-\\xi_i \\\\ \\xi_i>=0 \\quad i=1,2,3...N故合页损失函数和软间隔最大化其实等价. 核函数核函数是解决非线性支持向量机的方式,在得软间隔的对偶问题: \\min_{\\alpha} \\quad \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_ix_j)-\\sum_{i=1}^N\\alpha_i \\\\ s.t. \\quad \\sum_{i=1}^N\\alpha_iy_i=0 \\\\ 0=","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[]},{"title":"Transformer","slug":"transformer","date":"2019-04-05T05:31:37.000Z","updated":"2021-09-19T18:01:25.966Z","comments":true,"path":"2019/04/05/transformer/","link":"","permalink":"https://lingyixia.github.io/2019/04/05/transformer/","excerpt":"本文章只用于自己记录,详细内容好多博客已经讲的很清楚了。参考博客1参考博客2参考博客3参考平博客(最佳)残差网络我的实现","text":"本文章只用于自己记录,详细内容好多博客已经讲的很清楚了。参考博客1参考博客2参考博客3参考平博客(最佳)残差网络我的实现 整体架构 解释1:左半部分是encoder,方框中是一个encoder cell,右半部分是decoder,方框中是一个decoder cell。解释2:一个encoder cell包含四层:self-Multi-Attention+ResNet And Norm+Feed-Forward+ResNet And Norm,名字和图中不太一样,按照层数数下即可.解释3: 一个decoder cell包含六层:Mask-self-Multi-Attention+ResNet And Norm+encoder-decoder-Multi-Attention+ResNet And Norm+Feed-Forward+ResNet And Norm,名字和图中不太一样,按照层数数下即可.解释4:encoder阶段的self-Multi-Attention和decoder阶段的Mask-self-Multi-Attention,encoder-decoder-Multi-Attention是同一段代码。解释5:以翻译任务为例,假设当前需要翻译t位置的词汇,decoder阶段的mask-self-Attention是对0~t-1,即对已经翻译出来部分的attention,故需要做mask,防止attention未翻译部分,encoder-decoder-Attention是对原文所有的attention。解释6:encoder-decoder-Multi-Attention并不是self-Attention,因为它的Q是原文状态,K和V是译文状态 self-Attention先上图: 其实就是计算句子中每个单词对其他所有单词的关注程度: Attention(Q,K,V)=\\frac{softmax(Q \\times K^T)}{\\sqrt{d_k}} \\times V这里只解释为啥要除以$\\sqrt{dk}$: 解释一已知:$E(Q_n \\times d_k)=0,D(Q_n \\times d_k)=1,E(K_n \\times d_k)=0,D(K_n \\times d_k)=1$,注意，此时的期望和方差都是指的$d_k$维向量的每个分量，且假设这$d_k$个分量独立同分布。目的是保持点积后期望方差不变。如果直接计算点积:$Q \\times K^T$为$n \\times n$维矩阵,注意，后面计算的期望和方差都是按照行或者列，不要想成整个矩阵的期望方差。以某个单词$q$为例:$E(qK^T)=E(\\sum_0^{d_k}q_iK^T)=\\sum_0^{d_k}E(K^T)=d_k \\times 0=0$。其实把$Q$的每个分量看作一个常数即可$D(qK^T)=D(\\sum_0^{d_k}q_iK^T)=\\sum_0^{d_k}D(K^T)=d_k \\times 1=d_k$而$D(\\frac{qK^T}{\\sqrt{d_k}})=D((qKT)^2)\\times D((\\frac{1}{d_k})^2)=d_k/d_k=1$ 解释二很简单的方法，一个$d_k$维的向量$q_i$，去乘以$d_k \\times n$维度的向量，同时这个$d_k \\times n$的向量在$d_k$的每个维度上(设第$i$个维度的随机变量为$X_i$)均值为零，方差为1，现在要求相乘后得到的向量,即一个$n$维向量的均值和方差，其实这和乘不乘$q_i$没关系，只和相乘的时候的加法有关系，最后$n$维度向量的均值$E=E(\\sum_{i=0}^{d_k}X_i)=d_kE(X)=0$,同理$D(\\sum_{i=0}^{d_k}X_i)=\\sum_{i=0}^{d_k}D(X)=d_k$,因此要除以$\\sqrt d_k$. 解释三：复杂度$O(n^2)$(n为序列)三个步骤：1.$Q \\times K^T$，其实就是$n \\times d_k$矩阵乘以$d_k \\times n$维矩阵，复杂度是$O(d_kn^2)$2.$softmax(Q \\times K^T)$,没什么好说的，复杂度$O(n^2)$3.$\\frac{softmax(Q \\times K^T)}{\\sqrt{d_k}}$,就是一个$n\\times d_k$矩阵乘以$d_k\\times n$矩阵,复杂度$O(d_k^2 \\times n)$因此，去除常数$d_k$,复杂度就是$O(n^2)$ ResNet And Norm残差网络的作用见残差网络,Norm的作用自然是加快收敛,防止梯度消失. Feed-Forward这里其实没有什么特殊的东西,源码中用了两个conv1,先把特征维度放大,在把特征维度缩小,其实也就是特征提取的作用. Positional Encoding由于在attention的时候仅仅计算了当前词与其他词的相关度,但是并没有其他词的位置信息,试想,输入一个待翻译的句子1,然后将该句中任意两个单词互换位置形成句子2,在当前结构中其实两个句子并没有什么不同,因此需要对每个单词引如其位置信息.在论文中使用的是这样的方式: PE_{(pos,2i)}=sin(pos/10000^{2i/d_{modle}}) \\\\ PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{modle}})其中pos是当前单词在该句子中的位置,比如句子长20,则pos可以是1,2…20.i是在当前单词的第i个维度,比如每个单词有512个维度,则i可以是1,2…512.$d_{modle}$是单词维度,当前例子中即是512.为什么要用三角函数呢?首先，这种方式能保证各个位置的位置信息各不相同。即绝对位置,其次，由 sin(\\alpha+\\beta)=sin\\alpha cos\\beta + cos\\alpha\\beta \\\\ cos(\\alpha+\\beta)=cos\\alpha cos\\beta-sin\\alpha sin\\beta也就是说，如果单用sin和cos交替使用可以保证PE(pos+k)能用PE(pos)和PE(k)表示出来，也就是相对相对位置,如果仅仅使用sin或cos就没有了相对信息 进一步解释对于一个序列，第$i$个单词和第$j$个单词的$Attention$ $score$为: A_{i,j}=(W_q(E_i+U_i))^T(W_k(E_j+U_j))其中，$W_q$和$W_k$分别是$Q$和$K$的对齐权重,$E_i$和$E_j$分别是第$i$和$j$个单词的词向量,$U_i$和$U_j$分别是第$i$和$j$个单词的位置向量。分解上式: A_{i,j}=\\underbrace{E_i^TW_qW_kE_j}_a+\\underbrace{E_i^TW_qW_kU_j}_b+ \\underbrace{U_i^TW_qW_kE_j}_c+\\underbrace{U_i^TW_qW_kU_j}_d其中，只有$d$包含$i$和$j$的位置相对信息,我们知道: U_t=\\left[ sin(f(0,t)),cos(f(0,t)),sin(f(1,t)),cos(f(1,t))...sin(f(d_{model}/2,t)),cos(f(d_{model}/2,t)) \\right]^T如果没有$W_q$和$W_k$,假设$j-k=i$则有: \\begin{aligned} d&=U_i^TU_{i+k}\\\\ &=\\sum_{j=0}^{d_{model}-1}[sin(f(j,i))sin(f(j,i+k))]+[cos(f(j,i))cos(f(j,i+k))] \\\\ &=\\sum_{j=0}^{d_{model}-1}cos(f(j,(t-(t+k))))\\\\ &=\\sum_{j=0}^{d_{model}-1}cos(f(j,-k)) \\end{aligned}发现如果没有$W_q$和$W_k$,位置距离为$k$的两个单词的同一维度只和相对位置有关，即包含了相对位置信息，但是加上这两个矩阵之后这种相对信息不复存在，但是在学习的过程中可以学出来，比如学到$W_qW_k=E$，这种相对位置信息就完全恢复了。1234567891011121314151617181920212223242526272829303132333435363738394041424344import pandas as pdimport numpy as npimport mathimport matplotlib.pyplot as pltflag = 0def attention_scores(d_model, t, k): def get_angle(pos, i): return pos / math.pow(10000, 2 * i / d_model) angles1 = map(lambda x: get_angle(t, x), range(d_model // 2)) angles2 = map(lambda x: get_angle(t + k, x), range(d_model // 2)) result1 = list() for angle in list(angles1): result1.append(math.sin(angle)) result1.append(math.cos(angle)) result2 = list() for angle in list(angles2): result2.append(math.sin(angle)) result2.append(math.cos(angle)) result1 = np.asarray(result1) result2 = np.asarray(result2) if flag == 0: return sum(result1 * result2) elif flag == 1: W1 = np.random.normal(size=(128, 256)) W2 = np.random.normal(size=(256, 128)) W = np.dot(W1, W2) return sum(np.dot(result1.T, W) * result2) else: W = np.identity(128) return sum(np.dot(result1.T, W) * result2)if __name__ == &apos;__main__&apos;: result = list() for pos in range(-50, 50): result.append(attention_scores(d_model=128, t=64, k=pos)) data = pd.Series(result) data.plot() plt.show() 上诉代码表示第$64$个数据的位置向量和他前后各50个数据范围内的位置向量$attention$值 ,flag=0表示$U_t^T U_{t+k}$,flag=1表示$U_t^TWU_{t+k}$,flag=2表示$U_t^T E U_{t+k}$,$E$表示单位向量,运行代码可以看出flag=0/2图形对称有规律，flag=1图形毫无规律。 其他1.每一个self-Attention都维护一个自己的$W_Q$,$W_K$,$W_V$,也就是生成Q,K,V的全连接神经网络参数,即每个cell的这三个值是不同的.2.在encoder阶段的最后一个encoder cell会将生成的K和V传递给decoder阶段每个decoder cell的encoder-decoder-Multi-Attention使用.而encoder-decoder-Multi-Attention使用的Q是Mask-self-Multi-Attention输出的.3.decoder阶段Mask-self-Multi-Attention用来Attention翻译过的句子,encoder-decoder-Multi-Attention用来Attention原文. 在代码中,的多头 Multi-Attention 的实现实际上每个头把维度给均分了,并不是每个头都 attention 所有的维度。12345678910111213141516171819202122232425262728class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, d_model, num_heads): super(MultiHeadAttention, self).__init__() self.num_heads = num_heads self.d_model = d_model self.depth = d_model // self.num_heads self.wq = tf.keras.layers.Dense(d_model) self.wk = tf.keras.layers.Dense(d_model) self.wv = tf.keras.layers.Dense(d_model) self.dense = tf.keras.layers.Dense(d_model) def split_heads(self, x, batch_size): x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) return tf.transpose(x, perm=[0, 2, 1, 3]) def call(self, v, k, q, mask): batch_size = tf.shape(q)[0] q = self.wq(q) # (batch_size, seq_len, d_model) k = self.wk(k) # (batch_size, seq_len, d_model) v = self.wv(v) # (batch_size, seq_len, d_model) q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth) k = self.split_heads(k, batch_size) # (batch_size, num_heads, seq_len_k, depth) v = self.split_heads(v, batch_size) # (batch_size, num_heads, seq_len_v, depth) scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) output = self.dense(concat_attention) # (batch_size, seq_len_q, d_model) return output, attention_weights","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"Attention","slug":"Attention","permalink":"https://lingyixia.github.io/tags/Attention/"}]},{"title":"Attention小结","slug":"Attention","date":"2019-04-02T08:20:15.000Z","updated":"2021-09-19T18:01:25.943Z","comments":true,"path":"2019/04/02/Attention/","link":"","permalink":"https://lingyixia.github.io/2019/04/02/Attention/","excerpt":"Attention小结","text":"Attention小结符号说明: 当前是第t步 s_t:decoder阶段隐状态 h_t:encoder阶段隐状态 N:encoder步数 BahdanauAttention以encoder-decoder模型为例,假设当前是第t步: y_t=decoder(y_{t-1},s_{t-1},context_t) \\\\ context_t = \\sum_{i=1}^N a_{ti}·h_i \\\\ a_{ti}=softmax(\\frac{e^{e_{ti}}}{\\sum_{j=1}^N e_{e{tj}}}) \\\\ e_{tj}=scores(s_{t-1},h_j)LuongAttention公式和上面基本一样,只有两点不同: BahdanauAttention在计算scores的时候用上一个隐藏状态$s_{t-1}$,而LuongAttention使用当前隐藏状态$s_t$ 在计算scores的时候BahdanauAttention使用的是encoder阶段各层的状态contact后计算,而LuongAttention仅计算最上层。 scores其实各种Attention不同点都在计算scores的时候作文章下面是几种计算scores的方式: 总结其实所有的attention都归结为三个点:Q,K,V,所有的attention通用公式其实就是: context=softmax(scores(Q,K))V而且一般K=V在encoder-decoder翻译模型中,Q指的是译文,K和V指的是原文.当Q=K=V的时候,其实也就是self-attention了,也就是transform模型中的关键点: Attention(Q,K,V)=softmax(\\frac{QK^K}{\\sqrt{d_k}})V其中$\\sqrt d_k$按道理应该不加根号,因为是计算余弦相似度,但其实每个单词维度都相同,除不除单词向量的模就没有什么意义了,随便除个数方式数据太大就行。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"Attention","slug":"Attention","permalink":"https://lingyixia.github.io/tags/Attention/"}]},{"title":"BLMO总结","slug":"BLMO","date":"2019-04-01T09:15:53.000Z","updated":"2021-09-19T18:01:25.943Z","comments":true,"path":"2019/04/01/BLMO/","link":"","permalink":"https://lingyixia.github.io/2019/04/01/BLMO/","excerpt":"BLMO是为了解决word2vec的一词多意而诞生的,比如“苹果”这个词,在word2vec中的词向量是确定的,无论语境是在讲水果还是手机,但是在BLMO中单个词的词向量是随上下文而变化的,即动态的词向量,原理就是在将词转化为此向量的时候让其通过一个网络,得到上下文信息,然后在将其与预训练的静态词向量加权获得真正要在下游任务使用的词向量。","text":"BLMO是为了解决word2vec的一词多意而诞生的,比如“苹果”这个词,在word2vec中的词向量是确定的,无论语境是在讲水果还是手机,但是在BLMO中单个词的词向量是随上下文而变化的,即动态的词向量,原理就是在将词转化为此向量的时候让其通过一个网络,得到上下文信息,然后在将其与预训练的静态词向量加权获得真正要在下游任务使用的词向量。 基本框架 也就是说,在我们想进行一个下游任务比如文本分类,本来需要输入是预训练的词向量,直接将词向量输入模型即可,但是现在我们需要首先将输入词通过BLMO,得到新的词向量,然后在将其输入模型. 使用方式这个教程相当不错 源码部分解析主要模型都在bilm/model中的BidirectionalLanguageModelGraph类. 1.word embedding or word_char embedding:123def __init__(self, options, weight_file, ids_placeholder, use_character_inputs=True, embedding_weight_file=None, max_batch_size=128): 对应_build函数 1234567def _build(self): if self.use_character_inputs: self._build_word_char_embeddings() else: self._build_word_embeddings() self._build_lstms() 也就是说无论输入的是字符还是词,use_character_inputs=True都默认使用char_embedding,一般都这样设置,_build_word_embeddings虽然写了,但是似乎就没什么作用(按道理应该是用该函数得到预训练的词向量然后和上下文词向量加权,但是代码中似乎并没有用它). 2.构建word_char embedding这部分代码的输出要输入LSTM,应该是总体模型中最复杂的一部分,代码较长,只挑重点部分(函数_build_word_char_embeddings): 12345678910... with tf.device(&quot;/cpu:0&quot;): self.embedding_weights = tf.get_variable( &quot;char_embed&quot;, [n_chars, char_embed_dim], dtype=DTYPE, initializer=tf.random_uniform_initializer(-1.0, 1.0) ) # shape (batch_size, unroll_steps, max_chars, embed_dim) self.char_embedding = tf.nn.embedding_lookup(self.embedding_weights, self.ids_placeholder) 这部分是得到char_embedding的代码 下面这部分代码是卷积char_build_word_char_embeddings 输入是[batch_size,n_token, max_char, char_dim],batch_size表示句子数量,n_token表示一个句子单词数,max_char表示一个单词字符数,char_dim表示字符向量维度。卷积核size为[1, n_width, char_dim,char_dim],卷积得到的结果为[batch_size,n_token,max_char-n_width+1,char_dim]。上图是对[1, n_width, char_dim]卷积的结果,然后将m各卷积核得到的结果contact起来,然后下一步经过highway层和project层,这两层可选,且前后维度完全相同。 其他ELMO模型ELMO代码","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lingyixia.github.io/categories/神经网络/"}],"tags":[{"name":"词向量","slug":"词向量","permalink":"https://lingyixia.github.io/tags/词向量/"},{"name":"预训练","slug":"预训练","permalink":"https://lingyixia.github.io/tags/预训练/"}]},{"title":"方差和偏差","slug":"biasAndVariance","date":"2019-03-18T06:36:26.000Z","updated":"2021-09-19T18:01:25.950Z","comments":true,"path":"2019/03/18/biasAndVariance/","link":"","permalink":"https://lingyixia.github.io/2019/03/18/biasAndVariance/","excerpt":"学习算法的预测误差, 或者说泛化误差(generalization error)可以分解为三个部分: 偏差(bias), 方差(variance)和噪声(noise). 在估计学习算法性能的过程中,我们主要关注偏差与方差. 因为噪声属于不可约减的误差 (irreducible error).","text":"学习算法的预测误差, 或者说泛化误差(generalization error)可以分解为三个部分: 偏差(bias), 方差(variance)和噪声(noise). 在估计学习算法性能的过程中,我们主要关注偏差与方差. 因为噪声属于不可约减的误差 (irreducible error). 符号说明 符号 含义 x 测试样本 D 数据集 $y_D$ x在D中的标记 y x的真实标记 f 训练集D上得到的模型 f(x;D) 训练集D上得到的模型在x下输出 $\\bar{f(x)}$ 模型f对x的期望 1.y和$y_D$是有区别的,有些本来就标错了,即噪声。2.这是在不同的训练集D上得到的不同模型,下面的计算也是在不同训练集D下的均值。 误差定义 泛化误差: Err(x)=E[(y-f(x;D))^2] \\tag{1} 均值 \\bar{f(x)}=E_D[f(x;D)] \\tag{2} 注意:方差和真实标记y无关,只和模型计算的实际值有关。 方差 var(x)=E_D[(f(x;D)-\\bar{f(x)})^2] \\tag{3} 噪声 \\epsilon ^2=E_D[(y_D-y)] \\tag{4} 偏差 bias ^2(x) = (\\bar{f(x)}-y)^2 \\tag{5} 对算法的期望泛化误差进行分解: 其中红色部分计算会得0:1. E_D[2(f(x;D)-\\bar{f(x)})(\\bar{f(x)}-y_D)] = E_D(f(x;D)\\bar{f(x)}-\\bar{f(x)^2}+\\bar{f(x)}y_D-f(x;D)y_D)要谨记:其中带D得是变量,不带D的都是常量,也就是说$\\bar{f(x)}$其实就是个数字,故有: E_D(f(x;D)\\bar{f(x)})=\\bar{f(x)}E_D(f(x;D))=\\bar{f(x)}^2前两项得零,后两项等于: \\bar{f(x)}E_D(y_D)-E_D(f(x;D))E_D(y_D)也等于零 $E(XY)=E(X)E(Y)$的必要条件是X和Y相互独立,$E_D(f(x;D))E_D(y_D)$中$f(x;D)$和E_D(y_D)相互独立,因此可以分开。 2.第二部分怎么得零的不太明白,难道是展开后$E_D(y)=E_D(y_D)$?整理如图: 方差与偏差以线性回归为例(训练集(train)、验证集(cv)、测试集(test)比例为6:2:2)我们现在做的是在训练10个模型,次数依次从1到10,对于 多项式回归,当次数选取较低时,我们的 训练集误差 和 交叉验证集误差 都会很大;当次数选择刚好时,训练集误差 和 交叉验证集误差 都很小;当次数过大时会产生过拟合，虽然 训练集误差 很小,但 交叉验证集误差 会很大(关系图如下)。 图像如下: Degree是次数 正则化参数$\\lambda$对于 正则化 参数,使用同样的分析方法,当参数比较小时容易产生过拟合现象,也就是高方差问题。而参数比较大时容易产生欠拟合现象,也就是高偏差问题. 学习曲线学习曲线 的横轴是样本数，纵轴为 训练集 和 交叉验证集 的 误差。所以在一开始，由于样本数很少，Jtrain(θ)Jtrain(θ) 几乎没有,而 Jcv(θ)Jcv(θ) 则非常大。随着样本数的增加,Jtrain(θ)Jtrain(θ) 不断增大,而 Jcv(θ)Jcv(θ) 因为训练数据增加而拟合得更好因此下降。所以 学习曲线 看上去如下图: 在高偏差的情形下,Jtrain(θ) 与 Jcv(θ) 已经十分接近，但是 误差 很大。这时候一味地增加样本数并不能给算法的性能带来提升。 在高方差的情形下,Jtrain(θ) 的 误差 较小,Jcv(θ) 比较大,这时搜集更多的样本很可能带来帮助。 Bagging和Boosting 如何理解bagging是减少variance，而boosting是减少bias? Bagging的意义是从总数据中每次随机取一部分,训练一个模型，这样做N次得到N个模型用投票的方式决定.(下面以N等于2为例)这样得到的N个模型实际上是差不多的，Bagging的偏差表示为:$E(\\frac{X_1+X_2}{2})=\\frac{1}{N}E(X_2+X_2)=E(X_i)$,方差表示为$D(\\frac{X_1+X_2}{2})$,如果$X$之间完全独立则等于$\\frac{1}{4}D(X_1+X_2)=\\frac{D(X_i)}{2}$,若$X$之间完全相同则等于$\\frac{1}{4}D(X_1+X_2)=\\frac{D(X_1)+D(X_2)+Cov(X_1,X_2)}{4}=\\frac{4 \\times D(X_i)}{4}=D{(X_i)}$对于boosting而言每次都是为了降低$L(f(X),y)$,而降低损失函数就意味着降低偏差。由于各子模型之间是强相关的，于是子模型之和并不能显著降低variance(并未理解这句话。。。) 总结 获得更多的训练样本——解决高方差 尝试减少特征的数量——解决高方差 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少正则化程度λ——解决高偏差 尝试增加正则化程度λ——解决高方差","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[{"name":"优化","slug":"优化","permalink":"https://lingyixia.github.io/tags/优化/"}]},{"title":"两种排列对比","slug":"permutation","date":"2019-03-15T12:54:52.000Z","updated":"2021-09-19T18:01:25.963Z","comments":true,"path":"2019/03/15/permutation/","link":"","permalink":"https://lingyixia.github.io/2019/03/15/permutation/","excerpt":"两种排列对比","text":"两种排列对比 可重复1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;vector&lt;char&gt; record = &#123;&apos;a&apos;, &apos;b&apos;, &apos;c&apos;&#125;;vector&lt;vector&lt;char&gt;&gt; result;void Permutation(vector&lt;char&gt; &amp;current)&#123; if (current.size() == record.size()) &#123; result.push_back(current); return; &#125; for (unsigned int i = 0; i &lt; record.size(); i++) &#123; current.push_back(record[i]); Permutation(current); current.pop_back(); &#125;&#125;int main()&#123; vector&lt;char&gt; current; Permutation(current); return 0;&#125; 这里的index表示的是当前current中数据的个数 不可重复12345678910111213141516171819202122232425262728#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;vector&lt;char&gt; record = &#123;&apos;a&apos;, &apos;b&apos;, &apos;c&apos;&#125;;vector&lt;vector&lt;char&gt;&gt; result;void Permutation2(vector&lt;char&gt; &amp;current, int index)&#123; if (index == record.size()) &#123; result.push_back(current); return; &#125; for (unsigned int i = index; i &lt; record.size(); i++) &#123; swap(record[i], record[index]); Permutation2(record, index + 1); swap(record[i], record[index]); &#125;&#125;int main()&#123; Permutation2(record, 0); return 0;&#125; 这里的index表示index之前的保持不变,交换index和他之后的字符","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"递归","slug":"递归","permalink":"https://lingyixia.github.io/tags/递归/"}]},{"title":"数值的整数次幂","slug":"power","date":"2019-03-15T08:17:38.000Z","updated":"2021-09-19T18:01:25.963Z","comments":true,"path":"2019/03/15/power/","link":"","permalink":"https://lingyixia.github.io/2019/03/15/power/","excerpt":"给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。","text":"给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 12345678910111213141516171819202122232425double unsignedPower(double base, unsigned int exponent)&#123; if (exponent == 0) return 1; double result = unsignedPower(base, exponent &gt;&gt; 1); result *= result; if ((exponent &amp; 1) == 1)//无符号数判断奇数偶数,切记前面加括号,因为&quot;==&quot;比&quot;&amp;&quot;优先级高 &#123; result *= base; &#125; return result;&#125;double Power(double base, int exponent)&#123; if (base == 0.0 &amp;&amp; exponent &lt;= 0) &#123; cout &lt;&lt; &quot;无效输入&quot; &lt;&lt; endl; &#125; unsigned int unsignedExponent = abs(exponent); double result = unsignedPower(base, unsignedExponent); if (exponent &gt;= 0) &#123; return result; &#125; return 1.0 / result;&#125; 两个点:一，除2用右移计算，如:9&gt;&gt;1=4,-9&gt;&gt;1=-5 二，判断奇数偶数和1作&amp;运算，其实就是判断其二进制最后一位是不是1。","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"Tip","slug":"Tip","permalink":"https://lingyixia.github.io/tags/Tip/"}]},{"title":"牛顿法和梯度下降法","slug":"niudunanddescent","date":"2019-03-13T13:32:45.000Z","updated":"2021-09-19T18:01:25.962Z","comments":true,"path":"2019/03/13/niudunanddescent/","link":"","permalink":"https://lingyixia.github.io/2019/03/13/niudunanddescent/","excerpt":"","text":"牛顿法和梯度下降对比 牛顿法首先需要确定,牛顿法是为了求解函数值为零的时候变量的取值问题的，具体地，当要求解 f(θ)=0时，如果 f可导，那么可以通过迭代公式初始化参数$\\theta_0$,则$\\theta_0$对应在函数$f(\\theta)$上是$(\\theta_0,f(\\theta_0))$,导数为$f(\\theta_0)\\prime$,则过该点的直线为: f(\\theta)=f(\\theta_0)+f(\\theta_0)\\prime(\\theta-\\theta_0) \\tag{1.1}该直线与$\\theta$轴的交点为: \\theta=\\theta_0-\\frac{f(\\theta_0)}{f(\\theta_0)\\prime} \\tag{1.2}即: \\theta_{n+1}=\\theta_n-\\frac{f(\\theta_n)}{f(\\theta_n)\\prime} \\tag{1.3}这就是牛顿法的参数更新公式.在神经网络更新参数的时候:但是牛顿法中我们的更新目标不是要让$f(x)=0$,而是要让$f(x)\\prime = 0$,因此,更新公式应该是: \\theta_{n+1}=\\theta_n-\\frac{f(\\theta_n)\\prime}{f(\\theta_n)\\prime \\prime} \\tag{1.4}梯度下降法$f(\\theta;x)$是参数为$\\theta$的损失函数,以下就用$f(\\theta)$代替。由泰勒公式: f(\\theta+\\Delta \\theta)≈f(\\theta)+f(\\theta)\\prime \\Delta \\theta \\tag{2.1}我们迭代的目标是让$f(\\theta)$在更新过程中变小,因此,令$\\Delta \\theta = \\eta f(\\theta)\\prime$,其中$\\eta &gt; 0$: f(\\theta-\\eta f(\\theta)\\prime) ≈ f(\\theta)-\\eta f(\\theta)\\prime ^2 \\tag{2.2}也就是说式(2.2)中满足$f(\\theta)&gt;f(\\theta-\\eta f(\\theta)\\prime)$(忽略约等于),这意味着当我们按照下面的公式更新$\\theta$的时候: \\theta_n=\\theta_{n-1}-\\eta f(\\theta)\\prime \\tag{2.3}就能不断的将目标函数$f(\\theta)$缩小。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[]},{"title":"数组中数量超过一半","slug":"halfArray","date":"2019-03-13T06:53:47.000Z","updated":"2021-09-19T18:01:25.959Z","comments":true,"path":"2019/03/13/halfArray/","link":"","permalink":"https://lingyixia.github.io/2019/03/13/halfArray/","excerpt":"数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。","text":"数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 123456789101112131415161718192021int MoreThanHalfNum(vector&lt;int&gt; numbers)&#123; int result = numbers[0];//擂主一开始是第一个数字 int time = 1;//分数是1 for (int i = 1; i &lt; numbers.size(); i++) &#123; if (time == 0)//如果分数掉到0了 &#123; result = numbers[i];//有新的挑战者上台直接当擂主 time=1; &#125; else if (numbers[i] == result) time++; // 如果和擂主同族(数字一样)分数+1,相当于给擂主加HP else time--;//如果不同族(数字不一样) //掉一分,相当于擂主花1HP打掉1HP的挑战者 &#125; int resultNum = 0; for (int i = 0; i &lt; numbers.size(); i++) &#123; if (numbers[i] == result) resultNum++; &#125; return resultNum&gt;=numbers.size()/2?resultNum:0;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[]},{"title":"SameAndValid","slug":"CnnSizeCalc","date":"2019-03-13T06:01:28.000Z","updated":"2021-09-19T18:01:25.945Z","comments":true,"path":"2019/03/13/CnnSizeCalc/","link":"","permalink":"https://lingyixia.github.io/2019/03/13/CnnSizeCalc/","excerpt":"记录Same和Valid两种Padding方式卷积或pooling后的大小","text":"记录Same和Valid两种Padding方式卷积或pooling后的大小 计算公式 (W+padding-K)/stride+1SameSame方式表示当stride为1的时候无论kernel_size是多少,要补充padding的数量需要使前后大小相同,因此,padding的计算方式为: W+X-K+1=W\\\\ X=K-1也就是说需要补充X的padding(X需要平均到两边)eg: ValidValid的方式使不补充padding,当最后不够一个stride的时候直接就不要了. 说明无论哪种方式,计算方式永远不变,只不过Same方式需要先计算padding数量,Valid方式直接padding=0计算即可.","categories":[],"tags":[{"name":"CNN","slug":"CNN","permalink":"https://lingyixia.github.io/tags/CNN/"}]},{"title":"神经网络调优","slug":"neuralNetWorkTips","date":"2019-03-10T02:13:05.000Z","updated":"2021-09-19T18:01:25.962Z","comments":true,"path":"2019/03/10/neuralNetWorkTips/","link":"","permalink":"https://lingyixia.github.io/2019/03/10/neuralNetWorkTips/","excerpt":"","text":"神经网络调优 OverFitting early stop 正则化 Dropout 增加数据量 减少模型参数 tip:为什么正则化防止过拟合? L=L_0+\\frac{\\lambda}{2n}\\sum w^2 \\\\ \\frac{\\partial L}{\\partial w}=\\frac{\\partial L_0}{\\partial w}+ \\frac{\\lambda}{n} w \\\\ \\frac{\\partial L}{\\partial b}=\\frac{\\partial L_0}{\\partial b}可以看出,正则项对b没影响,对w有影响,即: w=(1-\\frac{\\eta \\lambda}{n}) w -\\eta \\frac{\\partial L_0}{\\partial w}也即是说w权重衰减了,w更小了,为什么w更小了就能防止过拟合呢?因为过拟合的网络一般参数较大,w更小了网络就更简单了,所谓奥卡姆剃刀,所以过拟合减少了. 数据预处理中心化\\标准化 权重初始化1.随机初始化为小数据,如均值为0,方差为0.01的高斯分布,初始化小数据目的是使激活函数梯度尽量大,从而加速训练。2.Xavier initialization初始化$w$为: [-\\sqrt{\\frac{6}{m+n}},\\sqrt{\\frac{6}{m+n}}] m和n分别为数据的输入维度和输出维度。该方法的原理使使每一层输出的方差应该尽量相等,也就是每一层的输入和输出方差尽量相等。下面进行公式推导(假设线性激活函数(或者说没有激活函数)),假设任意一层输入为$X=(x_1,x_2,x_3…x_n)$,输出$y=(y_1,y_2,y_3…y_m)$前提:$W$,$X$独立同分布,$E(W)$和$E(X)都为0$。参考 Var(y_i)=Var(W_iX_i)=Var(W_i)Var(X_i) Var(Y)=Var(\\sum_{i=1}^{n_{in}} W_iX_i)=n_{in}Var(W_i)Var(X_i)为使$Var(Y)=Var(X)$则Var(W_i)=\\frac{1}{n_{in}}若考虑反向则:Var(W_i)=\\frac{1}{n_{out}}综合两点: Var(W_i)=\\frac{2}{n_{in}+n_{out}} Batch Normalization参考文章 目的是解决特征分布偏移问题,会造成梯度消失其实就是对每一层的输入做一个Norm,但是直接Norm就损失了学到的特征,因此使用传导公式如下: \\mu = \\frac{1}{m}\\sum_{i=1}^m x_i \\\\ \\sigma^2 = \\frac{1}{m}\\sum_{i=1}^m(x_i-\\mu)^2 \\\\ \\hat{x_i} = \\frac{x_i-\\mu}{\\sigma^2+\\epsilon} \\\\ y_i = \\gamma \\hat{x_i}+\\beta$\\epsilon$是个超参数,m是min-batch的大小,$\\gamma$和$\\beta$是两个需要学习的超参数。可以看出,当$\\gamma$=$\\sigma^2$,$\\epsilon$=0,$\\beta$=$\\mu$的时候就恢复了原特征. 详细解释:已知对于任何一个参数$W$，它在神经网络中的梯度为: \\begin{align} \\frac{\\partial L}{\\partial W}&=\\frac{\\partial L}{\\partial O} \\times W_l \\times \\sigma^{'} \\times W_{l-1} \\times \\sigma^{'}....\\times {x} \\\\ &=\\frac{\\partial L}{\\partial O} \\times \\prod_{i=l}^{1}\\sigma^{'}W_{i} \\end{align}如果没有激活函数的话，则原式等于: \\frac{\\partial L}{\\partial W}=\\frac{\\partial L}{\\partial O} \\times \\prod_{i=l}^{1}W_{i}如果在初始化的时候$W$非常小，那么各层的$W$连乘后也肯定非常小，那上式得到的梯度也一定非常小.但是如果按照BN的方式操作一下，其实按照上诉公式: \\hat{x_i} = \\frac{x_i-\\mu}{\\sigma^2+\\epsilon}即: \\frac{\\partial L}{\\partial W}=\\frac{\\partial L}{\\partial O} \\times \\prod_{i=l}^{1}\\frac{W_i}{\\sigma^2}也就是说，梯度不在仅仅和$W$有关，而是和$\\frac{W}{\\sigma^2}$有关，$W$大的时候一般$\\sigma^2$也大，$W$小的时候一般$\\sigma^2$也小。 加速训练 SGD (训练集做手脚) momentum(在梯度上做手脚) SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定,momentum模拟了惯性,充分考虑历史梯度，并用当前地推加以微调,从而保证稳定性. v_t=\\lambda v_{t-1}+(1-\\lambda)_tg_t \\\\ w_t=w_{t-1}-v_t 初始化$v_0$=0当前梯度是前面所有梯度的加权平均,当前时刻计算的梯度的作用仅仅是在前面梯度加权平均上的调整 Adagard (学习率做手脚)本来应该: w^{t+1} \\leftarrow w^t - \\eta^t g^tAdagard: s_t =s_{t-1}+g_t^2 \\\\ w_t=w_{t-1} - \\frac{\\eta}{\\sqrt{s_t+\\epsilon}} \\times g_t Adagard中对每个参数都有不同的学习率,比如一个参数$W_1$的梯度较大,则经过这个计算之后学习率就较小,反之$w_2$的梯度较小,计算之后学习率较大。 RMSProp(3的改进) s_t=\\lambda s_{t-1}+(1-\\lambda)g_t^2 \\\\ w_t=w_{t-1} - \\frac{\\eta}{\\sqrt{s_t+\\epsilon}} \\times g_t 初始化$s_0$=0 Adadelta(没有学习率) s_t= \\rho s_t+(1-\\rho)g_t^2 \\\\ g_t^{\\prime}=\\sqrt{\\frac{\\Delta w_{t-1}+\\epsilon}{s_t+\\epsilon}}*g_t \\\\ w_t= w_{t-1}-g_t^{\\prime} \\\\ \\Delta w_t=\\rho \\Delta w_{t-1}+(1-\\rho)g_t^{\\prime} * g_t 其实就是维护了一个$\\Delta w$用来代替Adagard中的学习率。 Adam(2和4的结合) v_t=\\beta_1 v_{t-1} +(1-\\beta_1)g_t \\\\ s_t=\\beta_2 s_{t-1} + (1-\\beta_2)g_t^2\\\\ \\hat{v_t}=\\frac{v_t}{1-\\beta_1^t} \\\\ \\hat{s_t}=\\frac{s_t}{1-\\beta_2^t} \\\\ g_t^{\\prime} = \\frac{\\eta \\hat{v_t}}{\\sqrt{\\hat{s_t}}+\\epsilon} \\\\ w_t=w_{t-1}-g_t^{\\prime} 初始化$v_0$=0,$s_0$=0其中3、4两步是参数修正,原因是如果不这样刚开始的时候有可能会使v和g,S和g相差太大.","categories":[],"tags":[{"name":"tips","slug":"tips","permalink":"https://lingyixia.github.io/tags/tips/"}]},{"title":"Summarization-with-Pointer-Generator-Networks","slug":"Summarization-with-Pointer-Generator-Networks","date":"2019-03-05T11:30:48.000Z","updated":"2021-09-19T18:01:25.948Z","comments":true,"path":"2019/03/05/Summarization-with-Pointer-Generator-Networks/","link":"","permalink":"https://lingyixia.github.io/2019/03/05/Summarization-with-Pointer-Generator-Networks/","excerpt":"本论文针对的是摘要生成任务,普遍而言文本生成类的任务都是使用Seq2Seq模型,本论文也不例外,一般而言文本生成有两种模式:抽取式和生成式,顾名思义不在详细描述，本论文分为三个部分:传统Attention,Pointer-Generator Networks和Coverage机制.论文连接: https://arxiv.org/pdf/1704.04368.pdf","text":"本论文针对的是摘要生成任务,普遍而言文本生成类的任务都是使用Seq2Seq模型,本论文也不例外,一般而言文本生成有两种模式:抽取式和生成式,顾名思义不在详细描述，本论文分为三个部分:传统Attention,Pointer-Generator Networks和Coverage机制.论文连接: https://arxiv.org/pdf/1704.04368.pdf 传统Attention简述: \\begin{align} e_i^t&=V^Ttanh(W_hh_i+W_ss_t+b_{attn}) \\tag{1} \\\\ a^t&=sotfmax(e^t) \\tag{2} \\\\ h_t^*&=\\sum_ia_i^th_i \\tag{3} \\\\ \\end{align} \\begin{gather} P_{vocab}=sotfmax(V^{'}(V[o_t;h_t^*]+b)+b^{'}) \\tag{4} \\\\ P(w)=P_{vocab}(w) \\tag{5} \\\\ loss_t = -logP(w_t^*) \\tag{6} \\\\ loss=\\frac{1}{T}\\sum_t^Tloss_t \\tag{7} \\end{gather}其中(4)中$o_t$是LSTM的输出,外面包着的两层实际上是个全连接网络$V^{‘},V,b,b^{‘}$也就是两个全连接网络的参数。需要注意的是代码实现方式,见attention_decoder.py由于h的shape是(batch_size,encode_length,hidden_size)，无法直接加权重W,作者首先增加其维度,得到(batch_size,encode_length,1,hidden_size)利用了conv2d卷积函数实现,得到相同shape,而$s_t$的shape是(batch_size,hidden_size),要加到每个encode_length上,作者首先增加其维度，得到shape为(batch_size,1,1,hidden_size),然后利用广播机制,直接相加即可. Pointer-generator network原理很简单，就是在Attention步骤中,认为(2)中得到的就是词概率,当然，此时的词概率所计算的词只有encoder部分原文的词量，而上面求得的$P_{vocab}$的词量是整个词典的量,但是两者并不是后者包含前者,而是交集关系，因为encoder的原文很可能包含原训练集不包含的词语$(OOV)$,因此，加入$Pointer-generator network$后得到两个词概率,需要把他们合并，方式是通过一个参数做平滑计算. \\begin{gather} P_gen=\\sigma(w_{h^*}h_t^* + w_s^Ts_t+w_x^Ts_t+b_{ptr}) \\tag{8} \\\\ P(w)=p_{gen}P_{vocab}(w)+(1-p_{gen})\\sum_{i:w_i=w}a_i^t \\tag{9} \\end{gather},作者代码的实现方式如:model.py的_calc_final_dist函数12345678#首先计算乘以参数之后:vocab_dists = [p_gen * dist for (p_gen, dist) in zip(self.p_gens, vocab_dists)]attn_dists = [(1 - p_gen) * dist for (p_gen, dist) in zip(self.p_gens, attn_dists)]#然后需要得到原词表加上oov词后的数量:extended_vsize = self._vocab.size() + self._max_art_oovsshape = [self._hps.batch_size, extended_vsize]#将attention的原文词概率插入其中attn_dists_projected = [tf.scatter_nd(indices, copy_dist, shape) for copy_dist in attn_dists] tf.scatter_nd函数介绍12#最后将词典的词概率加入其中final_dists = [vocab_dist + copy_dist for (vocab_dist, copy_dist) inzip(vocab_dists_extended,attn_dists_projected)] Coverage mechanismCoverage 机制的作用是减少重复生成,基本思想是通过每进行一步attention就将前面所有步骤的$a^t$加和: c^t=\\sum_{t{'}=0}^{t-1}a^{t{'}} \\tag{10}这个$c^t$就是decoder第t步开始生成之前原文每个词生成量合成的上下文信息，然后更改公式(1)为: e_i^t=V^Ttanh(W_hh_i+W_s+w_cc_i^t+b_{attn}) \\tag{11}增加一个$covloss_t$: covloss_t = \\sum_i min(a_i^t,c_i^t) \\tag{12}其含义是如果之前该词出现过了，那么它的$c_i^t$就很大，那么为了减少loss，就需要$a_i^t$变小（因为loss是取两者较小值），$a_i^t$小就代表着这个位置被注意的概率减少.最终$loss$为 loss=\\sum_t^T(-logP(w_t^*)+\\lambda\\sum_i min(a_i^t,c_i^t)) \\tag{13}","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lingyixia.github.io/tags/论文/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://lingyixia.github.io/tags/tensorflow/"}]},{"title":"智力题","slug":"Intellectualproblem","date":"2019-03-01T09:28:56.000Z","updated":"2021-09-19T18:01:25.946Z","comments":true,"path":"2019/03/01/Intellectualproblem/","link":"","permalink":"https://lingyixia.github.io/2019/03/01/Intellectualproblem/","excerpt":"","text":"工人和金条 你让工人为你工作7天，给工人的回报是一根金条。金条平分成相连的7段，你必须在每天结束时给他们一段金条，如果只许你两次把金条弄断，你如何给你的工人付费？ 答案:将金条分为7份，先折断7分之1，在折断7分之二；这样金条就变成3块，一份是7分之1一份是7分之二，还有一份是7分之4.第一天的时候给工人7分之1，第二天给工人7分之2，然后工人找回7分之1，第三天付给工人7分之1，第四天付给工人7分之4，工人找回7分之1和7分之2；第五天付给工人7分之1，第6天付给工人7分之2，工人找回7分之1，第7天付给工人7分之1就正好了。","categories":[],"tags":[]},{"title":"Knn和K-means","slug":"KnnAndKmeans","date":"2019-02-28T02:48:15.000Z","updated":"2021-09-19T18:01:25.946Z","comments":true,"path":"2019/02/28/KnnAndKmeans/","link":"","permalink":"https://lingyixia.github.io/2019/02/28/KnnAndKmeans/","excerpt":"","text":"kNN就是k近邻算法,用于监督式分类,kNN中的k代表距离最近的k个样本。K-means是聚类算法,非监督式,k-means中的k代表聚类的个数.两者唯一的相似点时均利用近邻信息来标注类别。 K近邻算法原理输入: 训练数据集: T={(x_1,y_1),(x_2,y_2),...(x_N,y_N)}其中$x_i \\in R^n$为特征向量,$y \\in {c_1,c_2,…c_K}$为类别输出: 实例$x$对应的类$y$ 根据给定的距离度量,在训练集中找出与$x$最近的K个点 在这K个点中多数投票原则决定$x$的类别.当K=1的时候称为最近邻原则,即距离谁最近就分到谁的那一类中.步骤很简单,但是有两个关键问题:距离度量和计算最近点 距离度量一般距离度量使用$L_p$距离： L_p(x_i,x_j)=(\\sum_{l=1}^n|x_i^l-x_j^l|^p)^{\\frac{1}{p}} \\quad (p>=1)当$p=2$时称为欧式距离: L_2(x_i,x_j)=(\\sum_{l=1}^n |x_i^l-x_j^l|^2)^{\\frac{1}{2}}当$p=1$时程为曼哈顿距离: L_1(x_i,x_j)=\\sum_{l=1}^n|x_i^l-x_j^l|当$p=\\infty$时时各个坐标距离的最大值: L_{\\infty}(x_i,x_j)=\\max_l|x_i^l-x_j^l|这个需要证明: \\max_l|x_i^l-x_j^l|","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[{"name":"分类","slug":"分类","permalink":"https://lingyixia.github.io/tags/分类/"},{"name":"聚类","slug":"聚类","permalink":"https://lingyixia.github.io/tags/聚类/"}]},{"title":"数据规范化","slug":"normalization","date":"2019-02-28T02:26:21.000Z","updated":"2021-09-19T18:01:25.962Z","comments":true,"path":"2019/02/28/normalization/","link":"","permalink":"https://lingyixia.github.io/2019/02/28/normalization/","excerpt":"数据标准化及其作用","text":"数据标准化及其作用 标准化作用 消除量纲影响:这篇博客 激活函数对数据在[0,1]之间最为敏感 最小-最大规范化最小-最大规范化也称为离散标准化,是对原始数据的线性变换,将数据值映射到[0, 1]之间。 x^*=\\frac{x-min}{max-min}优点: 保留了原来数据中存在的关系,是消除量纲和数据取值范围影响的最简单方法缺点: 若数值集中且某个数值很大,则规范化后各值接近于0，并且将会相差不大。(如 1， 1.2， 1.3， 1.4， 1.5， 1.6，8.4)这组数据。若将来遇到超过目前属性[min, max]取值范围的时候，会引起系统报错，需要重新确定min和max,即要求测试数据的范围也必须在训练数据的[min, max]范围内. (z-score)规范化(或零均值规范化)经过处理的数据的均值为0,标准差为1: x^* = \\frac{x-\\mu}{\\sigma}优点:只要测试数据偏差训练集的$\\mu,\\sigma$不是太多就可以.几乎都用这个。","categories":[{"name":"预处理","slug":"预处理","permalink":"https://lingyixia.github.io/categories/预处理/"}],"tags":[{"name":"数据","slug":"数据","permalink":"https://lingyixia.github.io/tags/数据/"}]},{"title":"简单模型选择","slug":"validate","date":"2019-02-27T13:27:13.000Z","updated":"2021-09-19T18:01:25.967Z","comments":true,"path":"2019/02/27/validate/","link":"","permalink":"https://lingyixia.github.io/2019/02/27/validate/","excerpt":"模型选择","text":"模型选择 验证集数据量足够大的前提下可以划分为训练集、验证集和测试集，验证集用于对使用训练集训练得到的模型的验证，然后根据这个验证结构调整模型. 简单交叉验证 假设超参数集合为P=\\{P_1,P_2,...P_N\\},谨记，我们的目的是选出P中最合适的超参数。 123456789101112for(i=1;i&lt;=N;i++):&#123; for(j=1;j&lt;=M;j++)//验证M次 &#123; 1. 随机打乱数据，随机取验证和训练集 2. 在上诉训练集上训练 3. 在验证集上验证模型指标 &#125; 指标取平均，就是参数为P[i]下的模型指标&#125;得到N个模型指标，取模型指标最大的参数P[max],为了避免数据浪费，往往会在这个参数集为P[max]下用所有模型在训练一遍取得分最高的模型 S折交叉验证 假设超参数集合为P=\\{P_1,P_2,...P_N\\},谨记，我们的目的是选出P中最合适的超参数。 12345678910111213将所有数据均为为S分for(i=1;i&lt;=N;i++):&#123; for(j=1;j&lt;S;j++) &#123; 1. 第j份用于验证，其他S-1份用于训练 2. 用S-1份数据训练 3. 在第j份数据上验证模型指标 &#125; 指标取平均，就是参数为P[i]下的模型指标&#125;得到N个模型指标，取模型指标最大的参数P[max],为了避免数据浪费，往往会在这个参数集为P[max]下用所有模型在训练一遍取得分最高的模型 留一交叉验证1是S交叉验证中S=N,即只用一个样本作为测试集,其他的都是训练集 简单交叉验证用于初步分析,S折交叉验证用于深入分析,留一交叉验证用于数据量少的时候.","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[]},{"title":"优化问题","slug":"optimize","date":"2019-02-27T08:39:03.000Z","updated":"2021-09-19T18:01:25.962Z","comments":true,"path":"2019/02/27/optimize/","link":"","permalink":"https://lingyixia.github.io/2019/02/27/optimize/","excerpt":"优化问题","text":"优化问题 约束优化问题 \\min_w f(w) \\\\ s.t. \\quad g_i(w)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[{"name":"svm","slug":"svm","permalink":"https://lingyixia.github.io/tags/svm/"},{"name":"数学","slug":"数学","permalink":"https://lingyixia.github.io/tags/数学/"}]},{"title":"softmax","slug":"softmax","date":"2019-02-26T06:48:03.000Z","updated":"2021-09-19T18:01:25.965Z","comments":true,"path":"2019/02/26/softmax/","link":"","permalink":"https://lingyixia.github.io/2019/02/26/softmax/","excerpt":"参考原文","text":"参考原文 softmax优势 计算loss公式方便 反向传播容易计算 计算说明 z_4 = w_{41}*o_1+w_{42}*o_2+w_{43}*o_3 \\\\ z_5 = w_{51}*o_1+w_{52}*o_2+w_{53}*o_3 \\\\ z_6 = w_{61}*o_1+w_{62}*o_2+w_{63}*o_3$z_4,z_5,z_6$分别代表结点4,5,6的输出计算softmax a_4=\\frac{e^{z_4}}{e^{z_4}+e^{z_5}+e^{z_6}} \\\\ a_5=\\frac{e^{z_5}}{e^{z_4}+e^{z_5}+e^{z_6}} \\\\ a_6=\\frac{e^{z_6}}{e^{z_4}+e^{z_5}+e^{z_6}}要求梯度,首先需要定义loss,计算交叉熵损失函数为: Loss= -\\sum_i y_i\\ln a_i其中,$y_i$是上诉$z_4,z_5,z_6$,$a_i$是标签。看起来很复杂,其实实际情况下的$a_i$一般只是一个为1,其他都为0,故: Loss = y_j \\ln a_j当标签为1的时候: Loss = \\ln a_j也就是第一条优势:损失函数简单下面我们计算梯度,如我要求出w_{41},w_{42},w_{43}的偏导，就需要将Loss函数求偏导传到结点4，然后再利用链式法则继续求导即可. 情况一:j=i,比如此时求$w_{41}$的偏导($a_4$对应的标签为1),则: \\frac{\\partial Loss}{\\partial w_{41}}=\\frac{\\partial Loss}{\\partial a_4} \\times \\frac{\\partial a_4}{\\partial z_4} \\times \\frac{\\partial z_4}{\\partial w_{41}}易知: \\frac{\\partial Loss}{\\partial a_4}=-\\frac{1}{a_4} \\\\ \\frac{\\partial z_4}{\\partial w_{41}} =o_1关键点在于求$\\frac{\\partial a_4}{\\partial z_4}$ \\begin{align} \\frac{\\partial a_4}{\\partial z_4} &= \\frac{e^{z_4}(e^{z_5}+e^{z_6})}{e^{z_4}+e^{z_5}+e^{z_6}} \\\\ &= a_4(1-a_4) \\end{align}如图所示: 相乘得到最终公式: \\begin{align} \\frac{\\partial Loss}{\\partial w_{41}} &=-\\frac{1}{a_4} \\times a_4(1-a_4) \\times o_1 \\\\ &=(a_4-1) \\times o_1 \\end{align}即: \\frac{\\partial Loss}{\\partial z_4}=a_4-1 形式非常简单，这说明我只要正向求一次得出结果，然后反向传梯度的时候，只需要将它结果减1即可。 情况二:j!=i,比如此时求$w_{51}$的偏导,$a_4$对应的标签为1,则: \\frac{\\partial Loss}{\\partial w_{51}}=\\frac{\\partial Loss}{\\partial a_4} \\times \\frac{\\partial a_4}{\\partial z_5} \\times \\frac{\\partial z_5}{\\partial w_{51}}关键点在于求$\\frac{\\partial a_4}{\\partial z_5}$ \\begin{align} \\frac{\\partial a_4}{\\partial a_5} &=-\\frac{e^{z_4}e^{z_5}}{(e^{z_4}+e^{z_5}+e^{z_6})^2} \\\\ &= -a_4a_5 \\end{align}如图所示: 相乘得到最终公式: \\begin{align} \\frac{\\partial Loss}{\\partial w_{51}} &=-\\frac{1}{a_4} \\times (-a_4a_5) \\times o_1 \\\\ &=a_5 \\times o_1 \\end{align}即: \\frac{\\partial Loss}{\\partial z_5}=a_5 形式非常简单，这说明我只要正向求一次得出结果，然后反向传梯度的时候，只需要将它结果保存即可 举例说明举个例子，通过若干层的计算，最后得到的某个训练样本的向量的分数是[ 2, 3, 4 ],那么经过softmax函数作用后概率分别就是$[\\frac{e^{2} }{e^{2}+e^{3}+e^{4}},\\frac{e^{3} }{e^{2}+e^{3}+e^{4}} ,\\frac{e^{4} }{e^{2}+e^{3}+e^{4}} ]$,[0.0903,0.2447,0.665],如果这个样本正确的分类是第二个的话，那么计算出来的偏导就是[0.0903,0.2447-1,0.665]=[0.0903,-0.7553,0.665]，是不是非常简单！！然后再根据这个进行back propagation就可以了","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lingyixia.github.io/categories/神经网络/"}],"tags":[{"name":"损失函数","slug":"损失函数","permalink":"https://lingyixia.github.io/tags/损失函数/"}]},{"title":"二分法扩展","slug":"binaryMethod","date":"2019-02-24T08:13:40.000Z","updated":"2021-09-21T04:46:57.362Z","comments":true,"path":"2019/02/24/binaryMethod/","link":"","permalink":"https://lingyixia.github.io/2019/02/24/binaryMethod/","excerpt":"思路很简单，细节最致命，不要看不起看似很简单的二分法，不同的细节决定了它不同的用途!!!","text":"思路很简单，细节最致命，不要看不起看似很简单的二分法，不同的细节决定了它不同的用途!!! 普通二分查找 找到则返回下标，找不到返回-1 1234567891011121314151617181920212223int search(vector&lt;int&gt;&amp; nums,int target)&#123; int low = 0; int high = nums.size()-1; int mid = 0; while(low&lt;=high) &#123; mid = (low+high)/2; if(target&lt;nums[mid]) &#123; high=mid-1; &#125; else if (target&gt;nums[mid]) &#123; low=mid+1; &#125; else &#123; return mid; &#125; &#125; return -1;&#125; 查找插入位置 给出一个升序数组,一个数字，从数组中找出该数字的插入位置,可以这样想:循环结束的条件(假设插入的数字数组中没有)是low=high+1,此时target必然array[high] &lt; target &lt; array[low],因此插入位置一定是low处 12345678910111213141516171819202122 int searchInsert(vector&lt;int&gt;&amp; nums, int target) &#123; int low = 0; int high = nums.size()-1; int middle = 0; while (low &lt;= high) &#123; middle = (low + high) / 2; if (nums[middle] == target) &#123; return middle; &#125; else if (target &gt; nums[middle]) &#123; low = middle + 1; &#125; else &#123; high = middle - 1; &#125; &#125; return low;&#125; 查找上下界 给出一个升序数组(可能有重复),一个数字,从数组中找出该数字的下界 123456789101112131415161718192021int searchRange(vector&lt;int&gt;&amp; nums, int target)&#123; int low = 0; int high = nums.size() - 1; int mid = 0; while (low &lt;= high) &#123; mid = (low + high) / 2; if (target &lt;= nums[mid]) &#123; high = mid - 1; &#125; else &#123; low = mid + 1; &#125; &#125; if(low&gt;=nums.size() || nums[low]!=target) return -1; return low;&#125; 给出一个升序数组(可能有重复),一个数字,从数组中找出该数字的上界 123456789101112131415161718192021int searchRange(vector&lt;int&gt;&amp; nums, int target)&#123; int low = 0; int high = nums.size() - 1; int mid = 0; while (low &lt;= high) &#123; mid = (low + high) / 2; if (target &gt;= nums[mid]) &#123; low = mid + 1; &#125; else &#123; high = mid - 1; &#125; &#125; if(high&lt;0 || nums[high]!=target) return -1; return high;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"二分法","slug":"二分法","permalink":"https://lingyixia.github.io/tags/二分法/"}]},{"title":"ViEmu免费30天破解","slug":"ViEmu","date":"2019-02-23T01:51:14.000Z","updated":"2021-09-19T18:01:25.949Z","comments":true,"path":"2019/02/23/ViEmu/","link":"","permalink":"https://lingyixia.github.io/2019/02/23/ViEmu/","excerpt":"ViEmu","text":"ViEmu 目标删除Viemu在本机上的时间记录文件 总共需要删除两个地方:121. HKEY_CLASSES_ROOT\\Wow6432Node\\CLSID\\&#123;目录ID&#125;的InprocServr322. C:\\Users\\用户名\\AppData\\Local\\Identities\\&#123;ID项&#125; 方法对于要删除的第二个很容易,直接找到删除即可,问题是第一个,该目录下的项有很多,现在需要找的是ViEmu的目录ID 第一步我们需要找到所ViEmu的VSHub.dll，目录ID记录在这个DLL文件里边该文件应该在:C:\\Users\\用户名\\AppData\\Local\\Microsoft\\VisualStudio\\15.0_f45ae071\\Extensions\\h0npgwe4.q4b,也不一定,使用everything查一下即可 第二部1.使用Reflector（.net的反编译器，可以在网上下载）打开该DLL，找到VSHub命名空间下的Hub类，找到Initialize(RegistryKey)方法并点击进入，在对应的代码中，找到ViEmuProt.InitializeLicenseStuff(this.m_productData);这一句代码，如下图所示: 2.点击进入ViEmuProt.InitializeLicenseStuff这个方法，找到其中的vep_WriteTrialPeriodControlItemsIfFirstTime(_productData)函数，如下图所示（这个函数就是写注册表的函数） 3.再次点击进入该函数，如下图: 红色框所示的函数即为写注册表的函数，可以看到，这个CreateSubKey(name)函数中对应的name参数就是我们需要的目录ID，那么这个ID是怎么来的呢？ 可以看到，这个参数是通过函数的第一条语句得到的（图中蓝色框）4.点击进入GenerateTrialControlRegKeyName(_productData)函数（上图蓝框），如下图所示: VS插件对应的product是0，所以，目录ID就是最下边那个{B9CDA4C6-C44F-438B-B5E0-C1B39EA864C4}","categories":[{"name":"Tip","slug":"Tip","permalink":"https://lingyixia.github.io/categories/Tip/"}],"tags":[{"name":"插件","slug":"插件","permalink":"https://lingyixia.github.io/tags/插件/"},{"name":"破解","slug":"破解","permalink":"https://lingyixia.github.io/tags/破解/"}]},{"title":"boosting","slug":"boosting","date":"2019-02-22T11:04:20.000Z","updated":"2021-09-19T18:01:25.951Z","comments":true,"path":"2019/02/22/boosting/","link":"","permalink":"https://lingyixia.github.io/2019/02/22/boosting/","excerpt":"boosting 是一种特殊的集成学习方法。所有的‘基’分类器都是弱学习器，但通过采用特定的方式迭代，每次根据训练过的学习器的预测效果来更新样本权值,用于新的一轮学习,最终提高联合后的学习效果boosting。","text":"boosting 是一种特殊的集成学习方法。所有的‘基’分类器都是弱学习器，但通过采用特定的方式迭代，每次根据训练过的学习器的预测效果来更新样本权值,用于新的一轮学习,最终提高联合后的学习效果boosting。 AdaBoost步骤: 首先，是初始化训练数据的权值分布D1。假设有N个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值:w1=1/N。 然后，训练弱分类器$G_i$.具体训练过程中是:如果某个训练样本点，被弱分类器$G_i$准确地分类,那么在构造下一个训练集中,它对应的权值要减小;相反,如果某个训练样本点被错误分类,那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换而言之,误差率低的弱分类器在最终分类器中占的权重较大,否则较小。 流程输入:训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$输出:分类器G(x) 每个训练样本初始化相同权值,即$w_i=1/N,得到$: D_1=(w_{11},w_{12},...,w_{1N}),w_{1i}=\\frac{1}{N} 开始迭代$m=1$~$M$a. 选取当前权值分布下误差率最低的分类器G为第m个基本分类器$G_m$,并计算误差: e_m=\\sum_{t=1}^NP(G_m(x_i)≠y_i)=\\sum_{i=1}^Nw_{mi}I(G_m(x_i)≠y_i)b. 计算分类器权重: \\alpha_m=\\frac{1}{2}\\ln \\frac{1-e_m}{e_m}此处需要备注:当$e_m$0,且$\\alpha_m$随着$e_m$的增大而减少,即分类误差越小该弱分类器权值越大,意味着在最终的全国分类器中该弱分类器的权值大。 c. 更新样本权值分布$D_{t+1}$: \\begin{gather} D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m+1,N}) \\\\ w_{m+1,i}=\\frac{w_{mi}}{Z_m} e^{-\\alpha y_iG_m(x_i)} \\end{gather}其中,$Z_m$是规范化因子: Z_m=\\sum_i^N w_{mi}e^{-\\alpha_m y_i G_m(x_i)}=2 \\sqrt{e_m(1-e_m)}(可证)此处需要备注:可以看出: w_{m+1,i} = \\begin{cases} \\frac{w_{mi}}{Z_m}e^{-alpha_m} & G_m(x_i) = y_i( G_m(x_i) \\times y_i =1) \\\\ \\frac{w_{mi}}{Z_m}e^{alpha_m} & G_m(x_i) ≠ y_i(G_m(x_i) \\times y_i = -1) \\\\ \\end{cases}意味着某样本分类错误时候该样本权值扩大,即所谓的更加关注该样本. 组合分类器 f(x) = \\sum_{m=1}^M \\alpha_m G_m(x_i)得到最终分类器: G(x) = sign(f(x))=sign(\\sum_{m=1}^M \\alpha_m G_m(x_i)) \\begin{align} f(x) &=sign(F(x)) \\\\ &=\\sum_{k=1}^K \\alpha_kT_k(x;\\beta_k) \\end{align}Boosting Tree(提升树算法)模型采用加法模型,利用前向分布算法,第m步得到的模型为: f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)其中$T(x;\\Theta_m)$表示第m部要得到的决策树,$\\Theta_m$表示其参数,并通过经验风险极小化来确定$\\Theta_m$: \\hat{\\Theta}_m=\\arg \\min_{\\Theta_m} \\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\\Theta_m))学习过程 \\begin{align} f_0(x)&=0 \\\\ f_m(x)&=f_{m-1}(x)+T(x;\\Theta_m) \\\\ f_m(x)&=\\sum_{m=1}^M T(x;\\Theta_m) \\end{align}当使用平方误差损失函数时: L(y,f(x))=(y-f(x))^2即: \\begin{align} L(y,f_{m-1}(x)+T(x;\\Theta_m))&=(y-f_{m-1}(x)-T(x;\\Theta_m))^2 \\\\ &=(r-T(x;\\Theta_m))^2 \\end{align}其中$r=y-f_{m-1}(x)$,即第m-1论求得的树还剩下的残差,现在第m轮的目标是减少这个残差. 梯度提升 主要思想和上诉算法相同,不同点在于使用负梯度(伪残差)代替残差 初始化 f_0(x) = \\arg \\min_c \\sum_{i=1}^N L(y_i,c) 也就是说$f_0(x)$其实是一个常数函数 对m=1,2,3…,Ma. 对i=1,2,3…,N,计算伪残差: r_{mi}=-[\\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)}]_{f(x)=f_{m-1}(x)} b. 使用该残差列表(x_i,r_{mi})计算新的分类器f_m(x)(即基函数,可能是决策树、逻辑回归、SVM等) c. 计算步长(也就是所谓的学习率,在Boosting中称为权重) \\gamma _m=\\arg \\min_{\\gamma} \\sum_{i=1}^N L(y_i,f_{m-1}-\\gamma f_m(x)) 计算步长可以用线性搜索的方式 d. 更新模型: f_m(x) = f_{m-1}(x)+\\gamma_m f(x)GBDT 当上诉梯度提升使用的是基函数是CART的时候就是GBDT,且无论GBDT用来回归还是分类CART均用回归树 伪代码为: XGboost 补充:损失函数：计算的是一个样本的误差代价函数：是整个训练集上所有样本误差的平均目标函数：代价函数 + 正则化项 目标函数为: Obj=\\sum_{i=1}^n l(y_i,\\hat{y_i})+\\sum_t^K \\Omega(f_t)第一项是loss,第二项是正则项: \\Omega(f_t) = \\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^T w_j^2其中$w_j^2$是叶子节点j的权重.由于新生成的树要拟合上次预测的损失,因此有: \\hat{y_i} = \\hat{y_i}^{(t-1)} + f_t(x_i)同时,可以将目标函数改写为: Obj^t=\\sum_{i=1}^n l(y_i,\\hat{y_i}^{(t-1)}+f_t(x_i))+\\Omega(f_t)二阶泰勒展开: Obj^t≈ \\sum_{i=1}^n[l(y_i,\\hat{y}^{(t-1)})+g_if_t(x_i)+\\frac{1}{2} h_i f_t^2(x_i)]+\\Omega(f_t)其中$g_i$和$h_i$分别是一阶和二阶导数.可以直接去掉t-1棵树的残差: g_i = \\frac{\\partial ^ \\prime l(y_i,\\hat{y}^{(t-1)})}{\\partial \\hat{y}^{(t-1)}} \\\\ h_i = \\frac{\\partial ^2 l(y_i,\\hat{y}^{(t-1)})}{\\partial \\hat{y}^{(t-1)}} \\\\ Obj^t≈ \\sum_{i=1}^n[g_if_t(x_i)+\\frac{1}{2}f_t^2(x_i))]+\\Omega(f_t)将目标函数按照叶子节点展开: \\begin{align} Obj^t &≈ \\sum_{i=1}^n[l(y_i,\\hat{y}^{(t-1)})+g_if_t(x_i)+\\frac{1}{2}f_t^2(x_i)]+\\Omega(f_t) \\\\ &=\\sum_{i=1}^n[l(y_i,\\hat{y}^{(t-1)})+g_if_t(x_i)+\\frac{1}{2}f_t^2(x_i)]+\\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^T w_j^2 \\\\ &=\\sum_{j=1}^T[(\\sum_{i \\in I_j} g_i)w_j + \\frac{1}{2}(\\sum_{i \\in I_j}h_i + \\lambda)w_j^2]+\\gamma T \\end{align}最终目标函数为: w_j^* = -\\frac{G_j}{H_j+\\lambda} \\\\ Obj^t = -\\frac{1}{2}\\sum_{j=1}^T \\frac{G_j^2}{H_j+\\lambda}+ \\gamma T其中: G_j = \\sum_{i \\in I} g_i \\\\ H_j = \\sum_{i \\in I} h_i$Obj^t$是每一步的目标函数,我们要让$Obj^t$小,就需要让$ \\frac{G_j^2}{H_j+\\lambda}$大,因此我们在这个地方更改决策树的split方案:对于任意一个划分必能将数据划分为一个二叉树,则我们计算每一个可能的划分: Gain=\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda} -\\gamma 前两项是划分后左右两个子树的得分,第三项是不划分的得分,现在需要计算所有可能的划分,然后选择最大的划分左右子树。 XGboost和GBDT对比 传统GBDT以CART作为基分类器，xgboost还支持线性分类器 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数 xgboost在代价函数里加入了正则项，用于控制模型的复杂度 前向分布算法输入: 训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$,损失函数$L(y_i,f(x_i))$,基函数集${b(x;\\lambda)}$ 这里的基函数集应该指的是未知参数的某种分类器,而在前向分布算法中我们每一步都可以使用不同得分类器(虽然一般是相同得分类器),因此可以是基函数”集”. 输出: 加法模型$f(x)$步骤: 初始化$f_0(x)$ 对$m=1,2,…,M$a. (\\beta_M,\\lambda_m)=arg\\min_{\\beta,\\lambda}\\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\lambda))b. 更新f_m(x)=f_{m-1}(x)+\\beta_m b(x;\\lambda_m) 得到最终加法模型:f(x)=f_M(x)=\\sum_{m=1}^M \\beta_mb(x;\\lambda_m) 从前向分布算法到Adaboost 简单来说,Adaboost就是当损失函数为指数损失函数时的前向分布算法,得到的是二分类模型 对于Adaboost最终分类器是: f(x)=\\sum_{m=1}^M \\alpha_mG_m(x) \\tag{5.1}假设在Adaboost步骤第2步中,已经进行m-1轮迭代,得到: \\begin{align} f_{m-1}(x)&=f_{m-2}(x)+\\alpha_{m-1}G_{m-1} \\\\ &=\\alpha_1G_1(x)+...+\\alpha_{m-1}G_{m-1}(x) \\tag{5.2} \\end{align}在第m轮我们得到: f_m(x)=f_{m-1}(x)+\\alpha_mG_m(x) \\tag{5.3}目标是使前向分布算得到的$\\alpha$和$G_m(x)$在训练数据集$T$上的损失函数最小,即: (\\alpha_m,G_m(x))=arg\\min_{\\alpha,G}\\sum_{i=1}^Ne^{-y_i(f_{m-1}+\\alpha G(x_i))} \\tag{5.4}令其为: (\\alpha_m,G_m(x))=arg\\min_{\\alpha,G}\\sum_{i=1}^N \\overline w_{mi} e^{-y_i\\alpha G(x_i))} \\tag{5.5}其中\\overline w_{mi}=e^{-y_if_{m-1}},可以看出\\overline w_{mi}与\\alpha和G(x)无关,故与最小化无关,但是和f_{m-1}有关,故每一轮都有变化。现在要证明的是使式5.5达到最小的\\alpha^*和G^*(x)就是Adaboost算法所得到的\\alpha_m和G_m(x)现在对两者分别求值:首先,对于G_m^*(x),对任意$\\alpha$&gt;0有: G_m(x)^*=arg\\min_G\\sum_{i=1}^N\\overline w_{mi}I(y_i≠G(x_i)) \\tag{5.6}该分类器G_m^*(x)就是式第m轮分类误差最小的分类器.然后求\\alpha^*_m,参照式5.4和5.5:首先需要知道: \\sum_{y_i=G_m(x_i)}\\overline w_{mi}e^{-\\alpha}=\\sum_i^N \\overline w_{mi}I(y_i=G(x_i)) tag{5.7} \\\\ \\sum_{y_i≠G_m(x_i)}\\overline w_{mi}e^{\\alpha}≠\\sum_i^N \\overline w_{mi}I(y_i≠G(x_i) tag{5.8}因此有: \\begin{align} \\sum_{i=1}^N \\overline w_{mi} e^{-y_i\\alpha G(x_i))}&=\\sum_{y_i=G_m(x_i)}\\overline w_{mi}e^{-\\alpha} +\\sum_{y_i≠G_m(x_i)}\\overline w_{mi}e^{-\\alpha} \\\\ &=(e^\\alpha-e^{-\\alpha}) \\sum_{i=1}^N \\overline w_{mi}I(y_i≠G(x_i))+e^{-\\alpha}\\sum_{i=1}^N \\overline w_{mi} \\tag{5.9} \\end{align}将5.6带入5.9中,并对$\\alpha$求导使导数为0,得到: \\alpha^*_m=\\frac{1}{2}\\log\\frac{1-e_m}{e_m} \\tag{5.10}其中$e_m$使分类误差率: e_m=\\frac{\\sum_i^N \\overline w_{mi}I(y_i≠G_m(x_i))}{\\sum_{i=1}^N \\overline w_{mi}}=\\sum_{i=1}^Nw_{mi}I(y_i≠G_m(x_i))可以看出$\\alpha_m$的更新和adaboost完全一致,最后再看样本权值的更新: f_m(x)=f_{m-1}(x)+\\alpha_mG_m(x) \\tag{5.11} \\overline w_{m+1i}=e^{-y_if_m(x_i)} \\tag{5.12}将5.11带入5.12中得: w_{m+1i}=\\overline w_{mi}e^{-y_i\\alpha_mG_m(x)}与adaboost相比,只少了个规范化因子。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[{"name":"分类","slug":"分类","permalink":"https://lingyixia.github.io/tags/分类/"},{"name":"回归","slug":"回归","permalink":"https://lingyixia.github.io/tags/回归/"}]},{"title":"CART算法","slug":"cart","date":"2019-02-22T06:08:23.000Z","updated":"2021-09-19T18:01:25.951Z","comments":true,"path":"2019/02/22/cart/","link":"","permalink":"https://lingyixia.github.io/2019/02/22/cart/","excerpt":"CART算法详见该博客即可","text":"CART算法详见该博客即可","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[{"name":"分类","slug":"分类","permalink":"https://lingyixia.github.io/tags/分类/"},{"name":"回归","slug":"回归","permalink":"https://lingyixia.github.io/tags/回归/"}]},{"title":"representationMethod","slug":"representationMethod","date":"2019-02-22T01:35:12.000Z","updated":"2021-09-19T18:01:25.964Z","comments":true,"path":"2019/02/22/representationMethod/","link":"","permalink":"https://lingyixia.github.io/2019/02/22/representationMethod/","excerpt":"词表征(word representation)和句子表征(sentence representation)方式总结","text":"词表征(word representation)和句子表征(sentence representation)方式总结 词表征(word representation)one-hot encoding不解释 Word Embedding 主要分为两类:Frequency based Embedding和Prediction based Embedding Frequency based Embedding Count Vector最简单的方式,将train用的N个文本按顺序排列,统计出所有文本中的词频,结果组成一个矩阵,那么每一列就是一个向量，表示这个单词在不同的文档中出现的次数。 TF-IDF Vector 比较好的介绍文章TF是”词频”,IDF是”逆文档频率”.TF-IDF方法基于前者的算法进行了一些改进,它的计算公式如下: tf-idf_{i,j} = tf_{i,j} \\times idf_i其中,$tf_{i,j}$(term-frequence)指的是第i个单词在第j个文档中出现的频次;而$idf_i$(inverse document frequency)的计算公式如下: idf_i = \\log ( \\frac{N}{n+1})其中,1用来防止$n$为0,$N$表示文档的总个数,$n$表示包含该单词的文档的数量。这个公式是什么意思呢？其实就是一个权重，设想一下如果一个单词在各个文档里都出现过，那么$N/n=1$,所以$idf_i=0$,这就意味着这个单词并不重要。这个东西其实很简单,就是在term-frequency的基础上加了一个权重，从而显著降低一些不重要/无意义的单词的frequency,比如a,an,the等,很明显$n$越大$idf_i$越小,即所谓的逆文本。 Co-Occurrence Vector直译过来就是协同出现向量。在解释这个概念之前，我们先定义两个变量: Co-occurrence协同出现指的是两个单词$w_1$和$w_2$在一个Context Window范围内共同出现的次数 Context Window指的是某个单词$w$的上下文范围的大小,也就是前后多少个单词以内的才算是上下文,比如一个Context Window Size = 2的示意图如下: 比如我们有如下的语料库:He is not lazy. He is intelligent. He is smart.我们假设Context Window=2，那么我们就可以得到如下的co-occurrence matrix(共现矩阵): Word2vec Word2vec的直观直觉是两个用两个单词同时出现的概率来区分不同单词。 上面介绍的三种Word Embedding方法都是确定性(deterministic)的方法,而接下来介绍一种非确定性的基于神经网络的预测模型——word2vec。它是只有一个隐含层的神经网络,且激活函数(active function)是线性的,最后一层output采用softmax来计算概率。它包含两种模型:CBOW和Skip-gram. Glove Glove的直观直觉是单词同时出现的概率的比率能够更好地区分单词参考链接。比如，假设我们要表示“冰”和“蒸汽”这两个单词。对于和“冰”相关，和“蒸汽”无关的单词，比如“固体”，我们可以期望P冰-固体/P蒸汽-固体较大。类似地，对于和“冰”无关，和“蒸汽”相关的单词，比如“气体”，我们可以期望P冰-气体/P蒸汽-气体较小。相反，对于像“水”之类同时和“冰”、“蒸汽”相关的单词，以及“时尚”之类同时和“冰”、“蒸汽”无关的单词，我们可以期望P冰-水/P蒸汽-水、P冰-时尚/P蒸汽-时尚应当接近于1. 另一方面，之前我们已经提到过，Word2Vec中隐藏层没有使用激活函数，这就意味着，隐藏层学习的其实是线性关系。既然如此，那么，是否有可能使用比神经网络更简单的模型呢？基于以上两点想法，Glove提出了一个加权最小二乘回归模型,输入为单词-上下文同时出现频次矩阵(共现矩阵): 其中,f是加权函数,定义如下: 文章表征卡方检验信息增益TF-IDF","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"表征","slug":"表征","permalink":"https://lingyixia.github.io/tags/表征/"}]},{"title":"线性回归和逻辑回归对比","slug":"regression","date":"2019-02-20T05:11:08.000Z","updated":"2021-09-19T18:01:25.964Z","comments":true,"path":"2019/02/20/regression/","link":"","permalink":"https://lingyixia.github.io/2019/02/20/regression/","excerpt":"温故而知新啊，今天复习线性回归和逻辑回归，发现了以前没想过的东西,即为什么逻辑回归要用交叉熵函数。","text":"温故而知新啊，今天复习线性回归和逻辑回归，发现了以前没想过的东西,即为什么逻辑回归要用交叉熵函数。 简单对比 最终函数线性回归:f(x)_{w,b}=wx+b逻辑回归: P(Y=1|X)=\\frac{e^{wx+b}}{1+e^{wx+b}}\\\\ P(Y=0|X)=\\frac{1}{1+e^{wx+b}}令:f(x)_{w,b}=P(1|X)=\\frac{e^{wx+b}}{1+e^{wx+b}} 损失函数线性回归(注意谁减谁):L(w,b)=\\frac{1}{2}\\sum_{i=1}^N(y_i-f(x_i))^2逻辑回归(注意谁前谁后):L(w,b)=\\sum_{i=1}^NC(f(x_i),y_i) 梯度计算:线性回归:\\nabla_wL(w,b)=\\sum_{i=1}^N(y_i-f(x_i))(-x_i)\\\\ \\nabla_bL(w,b)=\\sum_{i=1}^N(y_i-f(x_i))逻辑回归:\\begin{align} \\sum_{i=1}^NC(f(x_i),y_i) &= \\sum_{i=1}^N-[y_i\\ln(f(x_i))+(1-y_i)\\ln(1-f(x_i))]\\\\ &= -\\sum_{i=1}^N[y_i\\ln(\\frac{f(x_i)}{1-f(x_i)})+\\ln(1-f(x_i))]\\\\ &= -\\sum_{i=1}^N[y_i(wx_i+b)+\\ln(1-f(x_i))]\\\\ &= -\\sum_{i=1}^N[y_i(wx_i+b)-\\ln(1+e^{wx+b})] \\end{align} \\nabla_wL(w,b)=-\\sum_{i=1}^N(y_i-f(x_i))x_i\\\\ \\nabla_bL(w,b)=-\\sum_{i=1}^N(y_i-f(x_i))至此发现线性回归和逻辑回归的参数偏导公式完全相同，然后梯度上升或下降即可(上升还是下降取决于线性回归谁减谁，逻辑回归交叉熵谁先谁后)。 交叉熵含义对于 T=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}要想让得到回归函数$f(x)$最符合要求,只需使后验概率概率最大即可: \\prod_{i=1}^N[f(x_i)]^{y_i}[1-f(x_i)]^{1-y_i}其中,$y_i$是标签为1的数据，这其实是个似然函数,然后取$\\log$: \\begin{align} L(w,b) &= \\sum_{i=i}^N[y_i\\log(f(x_i))+(1-y_i)\\log(1-f(x_i))]\\\\ &= \\sum_{i=i}^N[y_i(wx_i+b)-\\ln(1+e^{wx+b})] \\end{align}发现$L(w,b)=\\sum_{i=1}^NC(f(x_i),y_i)$,因此，交叉熵的含义其实就是后验概率最大化。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[{"name":"分类","slug":"分类","permalink":"https://lingyixia.github.io/tags/分类/"},{"name":"回归","slug":"回归","permalink":"https://lingyixia.github.io/tags/回归/"},{"name":"有监督","slug":"有监督","permalink":"https://lingyixia.github.io/tags/有监督/"}]},{"title":"Tensorflow中Rnn的实现","slug":"tensorflow_rnn","date":"2019-02-20T02:11:08.000Z","updated":"2021-09-19T18:01:25.966Z","comments":true,"path":"2019/02/20/tensorflow_rnn/","link":"","permalink":"https://lingyixia.github.io/2019/02/20/tensorflow_rnn/","excerpt":"包含TensorFlow中BasicRNNCell,BasicLSTMCell等的实现","text":"包含TensorFlow中BasicRNNCell,BasicLSTMCell等的实现1.BasicRNNCell基本结构如图: 在TensorFlow中，BasicRNNCellm每一步输出的state和output相同，源代码如下: 12345678def call(self, inputs, state): &quot;&quot;&quot;Most basic RNN: output = new_state = act(W * input + U * state + B).&quot;&quot;&quot; gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) output = self._activation(gate_inputs) return output, output 公式如下: h_t=tanh(W_k[x_t,h_{t-1}]+b)或 h_t=tanh(Wx_t+Uh_{t-1}+b)但是我个人认为应该是: h_t=tanh([x_t,h_{t-1}]*W+b) \\tag{1}我也不知道为啥会都写作上面那两种形式.eg:123456789101112131415161718192021222324252627282930import tensorflow as tfimport numpy as npbatch_size = 3input_dim = 2output_dim = 4inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, input_dim))cell = tf.contrib.rnn.BasicRNNCell(num_units=output_dim)previous_state = cell.zero_state(batch_size, dtype=tf.float32)output, state = cell(inputs, previous_state)kernel = cell.variablesX = np.ones(shape=(batch_size, input_dim))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) output, state, kernel = sess.run([output, state,kernel], feed_dict=&#123;inputs: X&#125;) print(X.shape) print(kernel[0].shape) print(kernel[1].shape) print(previous_state.shape) print(state.shape) print(output.shape)结果为:(3, 2)(6, 4)(4,)(3, 4)(3, 4)(3, 4) 分析: (kernel中是所有参数的list，此处是W和bias)根据公式(1),$output=([X,previous_state] * W+bias),即([(3, 2);(3, 4)]*(6, 4)+(4,)) = (3,4)$，代码中也较容易看出.普通RNN梯度消失和梯度爆炸问题原因: 这是一个普通RNN图,其中$W_1$、$W_2$、$W_3$是不同时间步骤下的参数,我们要训练的就是这个参数,输出表达式为: f(w_1)=f_3(w_3f_2(w_2f_1(w_1)))求$W_1$的梯度: \\frac{\\partial f}{\\partial W_1}=\\frac{\\partial f}{\\partial f_3} \\times W_3 \\times \\frac{\\partial f_3}{\\partial f_2} \\times W_2 \\times \\frac{\\partial f_2}{\\partial f_1}这个讲解的相当清楚若使用sigmoid函数,则每一次偏导都是一个(0,1)的数 初始化W全都(0,1),那么上诉公式中每一个因式都是(0,1),因此连乘的多了就会梯度消失 初始化W很大,大到乘以sigmoid函数后还是大于1,那么连乘的多了就会梯度爆炸 2.BasicLSTMCell基本结构如图: 源码如下:12345678910111213141516171819202122232425262728293031323334353637383940414243def call(self, inputs, state): &quot;&quot;&quot;Long short-term memory cell (LSTM). Args: inputs: `2-D` tensor with shape `[batch_size, input_size]`. state: An `LSTMStateTuple` of state tensors, each shaped `[batch_size, self.state_size]`, if `state_is_tuple` has been set to `True`. Otherwise, a `Tensor` shaped `[batch_size, 2 * self.state_size]`. Returns: A pair containing the new hidden state, and the new state (either a `LSTMStateTuple` or a concatenated state, depending on `state_is_tuple`). &quot;&quot;&quot; sigmoid = math_ops.sigmoid one = constant_op.constant(1, dtype=dtypes.int32) # Parameters of gates are concatenated into one multiply for efficiency. if self._state_is_tuple: c, h = state else: c, h = array_ops.split(value=state, num_or_size_splits=2, axis=one) gate_inputs = math_ops.matmul( array_ops.concat([inputs, h], 1), self._kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) # i = input_gate, j = new_input, f = forget_gate, o = output_gate i, j, f, o = array_ops.split( value=gate_inputs, num_or_size_splits=4, axis=one) forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype) # Note that using `add` and `multiply` instead of `+` and `*` gives a # performance improvement. So using those at the cost of readability. add = math_ops.add multiply = math_ops.multiply new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))), multiply(sigmoid(i), self._activation(j))) new_h = multiply(self._activation(new_c), sigmoid(o)) if self._state_is_tuple: new_state = LSTMStateTuple(new_c, new_h) else: new_state = array_ops.concat([new_c, new_h], 1) return new_h, new_state 公式如下: \\begin{gather} f_t = \\sigma(W_f[h_{t-1},x_t]+b_f) \\\\ i_t = \\sigma(W_i[h_{t-1,x_t}]+b_i) \\\\ \\widetilde C_t = \\tanh(W_C[h_{t-1,x_t}]+b_C) \\\\ O_t = \\sigma(W_o[h_{t-1},x_t]+b_o)\\\\ C_t = f_t*C_{t-1}+i_t*\\widetilde C_t \\\\ h_t = O_t * \\tanh(C_t) \\end{gather}同理，我感觉应该是: \\begin{gather} f_t = \\sigma([h_{t-1},x_t]*W_f+b_f)\\\\ i_t = \\sigma([h_{t-1,x_t}]*W_i+b_i) \\\\ \\widetilde C_t = \\tanh([h_{t-1,x_t}]*W_C+b_C) \\\\ O_t = \\sigma([h_{t-1},x_t]*W_o+b_o)\\\\ C_t = f_t*C_{t-1}+i_t*\\widetilde C_t \\\\ h_t = O_t * \\tanh(C_t) \\end{gather}eg:1234567891011121314151617181920212223242526272829303132333435import tensorflow as tfimport numpy as npbatch_size = 3input_dim = 2output_dim = 4inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, input_dim))cell = tf.contrib.rnn.BasicLSTMCell(num_units=output_dim)previous_state = cell.zero_state(batch_size, dtype=tf.float32)output, state = cell(inputs, previous_state)kernel = cell.variablesX = np.ones(shape=(batch_size, input_dim))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) output, state,kernel = sess.run([output, state, kernel], feed_dict=&#123;inputs: X&#125;) print(X.shape) print(previous_state[0].shape) print(previous_state[1].shape) print(kernel[0].shape) print(kernel[1].shape) print(state[0].shape) print(state[1].shape) print(output.shape)结果:(3, 2)(3, 4)(3, 4)(6, 16)(16,)(3, 4)(3, 4)(3, 4) 分析: (kernel是所有参数，即W和bias)根据上诉公式，在源码中求遗忘门:$f_t$,输入门$i_t$和$\\widetilde C_t$s输出门$O_t$的代码为:i, j, f, o = array_ops.split(value=gate_inputs, num_or_size_splits=4, axis=one) 这里也解释了为什么eg中kernel[0]是(6,16),因为代码中是将4个W同时初始化在一起，即（6,16）中其实是有4个W，并在上诉代码中分别计算，kernal[1]同理。这样得到的i,j,f,o应该都是(3,4),在源码中可以看出计算i,j,f,o是矩阵相乘，但是计算$C_t$和$h_t$是各个元素相乘,因此得到的$C_t$和$h_t$都是(3,4). 3.GRU源代码为:12345678910111213141516171819def call(self, inputs, state): &quot;&quot;&quot;Gated recurrent unit (GRU) with nunits cells.&quot;&quot;&quot; gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._gate_kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias) value = math_ops.sigmoid(gate_inputs) r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1) r_state = r * state candidate = math_ops.matmul( array_ops.concat([inputs, r_state], 1), self._candidate_kernel) candidate = nn_ops.bias_add(candidate, self._candidate_bias) c = self._activation(candidate) new_h = u * state + (1 - u) * c return new_h, new_h eg:1234567891011121314151617181920212223242526272829303132333435import tensorflow as tfimport numpy as npbatch_size = 3input_dim = 2output_dim = 4inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, input_dim))cell = tf.contrib.rnn.GRUCell(num_units=output_dim)previous_state = cell.zero_state(batch_size, dtype=tf.float32)output, state = cell(inputs, previous_state)kernel = cell.variablesX = np.ones(shape=(batch_size, input_dim))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) output, state, kernel = sess.run([output, state, kernel], feed_dict=&#123;inputs: X&#125;) print(X.shape) print(previous_state.shape) print(kernel[0].shape) print(kernel[1].shape) print(kernel[2].shape) print(kernel[3].shape) print(state.shape) print(output.shape)结果:(3, 2)(3, 4)(6, 8)(8,)(6, 4)(4,)(3, 4)(3, 4)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"https://lingyixia.github.io/tags/tensorflow/"},{"name":"rnn","slug":"rnn","permalink":"https://lingyixia.github.io/tags/rnn/"}]},{"title":"二进制中一的个数","slug":"binaryOne","date":"2019-02-14T09:11:03.000Z","updated":"2021-09-19T18:01:25.950Z","comments":true,"path":"2019/02/14/binaryOne/","link":"","permalink":"https://lingyixia.github.io/2019/02/14/binaryOne/","excerpt":"计算一个整数的二进制中1的个数","text":"计算一个整数的二进制中1的个数 把1循环左移,同时和原数字做&amp;操作,直到1变成0 1234567891011int numberOfOne(int number)&#123; int count = 0; int flag = 1; while (flag) &#123; if (number &amp; flag) count++; flag &lt;&lt;= 1; &#125; return count;&#125; 一个整数number&amp;(number)结果是将number最右侧的1变为0 12345678910int numberOfOne(int number)&#123; int count=0; while (number) &#123; number &amp;= (number - 1); count++; &#125; return count;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"https://lingyixia.github.io/tags/leetcode/"},{"name":"牛客网","slug":"牛客网","permalink":"https://lingyixia.github.io/tags/牛客网/"}]},{"title":"memset讲解","slug":"memset","date":"2019-02-10T09:11:57.000Z","updated":"2021-09-19T18:01:25.961Z","comments":true,"path":"2019/02/10/memset/","link":"","permalink":"https://lingyixia.github.io/2019/02/10/memset/","excerpt":"原型为:void memset(void s, int ch, size_t n).解释:将以s为起始位置,n字节大小的区域,按字节填充,每个字节填充ch。注意,ch指的是Ascii码的第ch个,也就是说它最大也就是256.一般用来填充0","text":"原型为:void memset(void s, int ch, size_t n).解释:将以s为起始位置,n字节大小的区域,按字节填充,每个字节填充ch。注意,ch指的是Ascii码的第ch个,也就是说它最大也就是256.一般用来填充0eg1:123456789int main()&#123; int* number = new int[10]; memset(number,0,10); cout&lt;&lt;number[0]&lt;&lt;endl; return 0;&#125;输出:0解释:此时的ch是0,也就是ascii码的第0个,也就是00000000,故number全填充为0 eg2:123456789int main()&#123; int* number = new int[10]; memset(number,1,10); cout&lt;&lt;number[0]&lt;&lt;endl; return 0;&#125;输出:16843009解释:此时ch是1,也就是ascii码的第1个,也就是00000001,故number[0]是:00000001000000010000000100000001,计算即可得到","categories":[{"name":"基础","slug":"基础","permalink":"https://lingyixia.github.io/categories/基础/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://lingyixia.github.io/tags/C/"}]},{"title":"栈","slug":"stack","date":"2019-02-08T08:22:29.000Z","updated":"2021-09-19T18:01:25.965Z","comments":true,"path":"2019/02/08/stack/","link":"","permalink":"https://lingyixia.github.io/2019/02/08/stack/","excerpt":"输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）","text":"输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 算法描述: 只需要按照顺序走一遍即可1234567891011121314bool IsPopOrder(vector&lt;int&gt; pushV,vector&lt;int&gt; popV) &#123; stack&lt;int&gt; s; int index=0; for(int i =0;i&lt;pushV.size();i++) &#123; s.push(pushV[i]); while(!s.empty() &amp;&amp; s.top()==popV[index]) &#123; s.pop(); index++; &#125; &#125; return s.empty(); &#125; Tip: 对于一个入栈顺序的弹栈序列必然有这么一个特征:出栈序列中每个数后面的比它小的数必然按照降序排列比如入栈顺序是:1,2,3,4 4,1,2,3不可能是出栈顺序,因为4后面比4小的数1,2,3不是降序排列 3,1,4,2也不合法,3后面比3小的数1,2不是降序排列 1,2,3,4合法,当前每个数后面没有比它小的 删除相邻重复字符串123456789101112131415161718192021string removeDuplicates(string S)&#123; stack&lt;char&gt; s; for (auto ch:S) &#123; if (s.empty() || ch != s.top()) &#123; s.push(ch); &#125; else &#123; s.pop(); &#125; &#125; string result = &quot;&quot;; while (!s.empty()) &#123; result = s.top()+result; s.pop(); &#125; return result;&#125;","categories":[],"tags":[{"name":"栈","slug":"栈","permalink":"https://lingyixia.github.io/tags/栈/"}]},{"title":"CheckPoint的保存和恢复","slug":"modelSaveRestore","date":"2019-02-07T06:33:14.000Z","updated":"2021-09-19T18:01:25.961Z","comments":true,"path":"2019/02/07/modelSaveRestore/","link":"","permalink":"https://lingyixia.github.io/2019/02/07/modelSaveRestore/","excerpt":"本文用于记录tensorflow的tf.train.Saver()函数","text":"本文用于记录tensorflow的tf.train.Saver()函数一个模型保存后会有四个文件: model.ckpt.meta文件: 图结构信息 model.ckpt.data-*+model.ckpt.index:保存变量取值 checkpoint:保存模型名称,即restore的时候的需要传入的名称 模型保存123456789101112131415import tensorflow as tf#save的路径不需要手动创建def main(argv=None): v1 = tf.Variable(initial_value=tf.constant(value=1.0, shape=[1]), name=&apos;v1&apos;) v2 = tf.Variable(initial_value=tf.constant(value=2.0, shape=[1]), name=&apos;v2&apos;) result = tf.add(x=v1, y=v2, name=&apos;add&apos;) saver = tf.train.Saver() with tf.Session() as sess: tf.global_variables_initializer().run() saver.save(sess, &apos;./model/model.ckpt&apos;)if __name__ == &apos;__main__&apos;: tf.app.run() 模型恢复12345678910111213import tensorflow as tfdef main(argv=None): v1 = tf.Variable(initial_value=tf.constant(value=1.0, shape=[1]), name=&apos;v1&apos;) v2 = tf.Variable(initial_value=tf.constant(value=2.0, shape=[1]), name=&apos;v2&apos;) result = tf.add(x=v1, y=v2, name=&apos;add&apos;) saver = tf.train.Saver() with tf.Session() as sess: saver.restore(sess, &apos;./model/model.ckpt&apos;) print(result.eval())if __name__ == &apos;__main__&apos;: tf.app.run() 注意回复模型的时候不需要tf.global_variables_initializer().run(),这种方式是先把原始图构建出来，然后在回复模型,把模型保存的变量恢复到刚刚构建的图中,还可以不定义原图,直接加载保存的图. 12345678910import tensorflow as tfdef main(argv=None): saver = tf.train.import_meta_graph(&apos;./model/model.ckpt.meta&apos;) with tf.Session() as sess: saver.restore(sess, &apos;./model/model.ckpt&apos;) print(sess.run(tf.get_default_graph().get_tensor_by_name(&apos;add:0&apos;)))if __name__ == &apos;__main__&apos;: tf.app.run() 在恢复模型的时候,还可以把保存的变量加载到其他变量中。 1234567891011121314import tensorflow as tfdef main(argv=None): u1 = tf.Variable(initial_value=tf.constant(value=1.0, shape=[1]), name=&apos;other-v1&apos;) u2 = tf.Variable(initial_value=tf.constant(value=2.0, shape=[1]), name=&apos;other-v2&apos;) result = tf.add(x=u1, y=u2, name=&apos;add&apos;) saver = tf.train.Saver(&#123;&apos;v1&apos;: u1, &apos;v2&apos;: u2&#125;) with tf.Session() as sess: saver.restore(sess, &apos;./model/model.ckpt&apos;) print(result.eval())if __name__ == &apos;__main__&apos;: tf.app.run() saver = tf.train.Saver({&#39;v1&#39;: u1, &#39;v2&#39;: u2})的作用是把原来名称name为v1的变量现在加载到变量u1(名称name为other-v1)中,这个方式的作用之一是方便使用滑动平均模型。 123456789101112131415161718192021import tensorflow as tf v = tf.Variable(0, dtype=tf.float32, name=&quot;v&quot;)for variables in tf.global_variables(): print(variables.name) # v:0 ema = tf.train.ExponentialMovingAverage(0.99)maintain_averages_op = ema.apply(tf.global_variables())for variables in tf.global_variables(): print(variables.name) # v:0 # v/ExponentialMovingAverage:0 saver = tf.train.Saver() with tf.Session() as sess: sess.run(tf.global_variables_initializer()) sess.run(tf.assign(v, 10)) sess.run(maintain_averages_op) saver.save(sess, &quot;Model/model_ema.ckpt&quot;)if __name__ == &apos;__main__&apos;: tf.app.run() 12345678910111213#获取影子变量方式1import tensorflow as tfdef main(argv=None): v = tf.Variable(0, dtype=tf.float32, name=&quot;v&quot;) saver = tf.train.Saver(&#123;&quot;v/ExponentialMovingAverage&quot;: v&#125;) with tf.Session() as sess: saver.restore(sess, &quot;./Model/model_ema.ckpt&quot;) print(sess.run(v))if __name__ == &apos;__main__&apos;: tf.app.run() 12345678910111213141516#获取影子变量方式2import tensorflow as tfdef main(argv=None): v = tf.Variable(0, dtype=tf.float32, name=&quot;v&quot;) # 注意此处的变量名称name一定要与已保存的变量名称一致 ema = tf.train.ExponentialMovingAverage(0.99) print(ema.variables_to_restore()) # &#123;&apos;v/ExponentialMovingAverage&apos;: &lt;tf.Variable &apos;v:0&apos; shape=() dtype=float32_ref&gt;&#125; # 此处的v取自上面变量v的名称name=&quot;v&quot; saver = tf.train.Saver(ema.variables_to_restore()) with tf.Session() as sess: saver.restore(sess, &quot;./Model/model_ema.ckpt&quot;) print(sess.run(v))if __name__ == &apos;__main__&apos;: tf.app.run() 常量保存12345678910111213141516import tensorflow as tffrom tensorflow.python.framework import graph_utildef main(argv=None): v1 = tf.Variable(initial_value=tf.constant(value=1.0, shape=[1]), name=&apos;v1&apos;) v2 = tf.Variable(initial_value=tf.constant(value=2.0, shape=[1]), name=&apos;v2&apos;) result = tf.add(x=v1, y=v2, name=&apos;add&apos;) with tf.Session() as sess: tf.global_variables_initializer().run() graph_def = tf.get_default_graph().as_graph_def() output_graph_def = graph_util.convert_variables_to_constants(sess=sess, input_graph_def=graph_def, output_node_names=[&apos;add&apos;]) with tf.gfile.GFile(name=&apos;./model/combined.pd&apos;, mode=&apos;wb&apos;) as f: f.write(output_graph_def.SerializeToString())if __name__ == &apos;__main__&apos;: tf.app.run() 常量恢复1234567891011121314import tensorflow as tffrom tensorflow.python.platform import gfiledef main(argv=None): with tf.Session() as sess: model_filename = &apos;./model/combined.pd&apos; with gfile.FastGFile(model_filename, mode=&apos;rb&apos;) as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) result = tf.import_graph_def(graph_def, return_elements=[&apos;add:0&apos;]) print(sess.run(result))if __name__ == &apos;__main__&apos;: tf.app.run()","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"https://lingyixia.github.io/tags/tensorflow/"}]},{"title":"滑动平均模型","slug":"learningAndMovingAverage","date":"2019-02-03T11:55:20.000Z","updated":"2021-09-19T18:01:25.959Z","comments":true,"path":"2019/02/03/learningAndMovingAverage/","link":"","permalink":"https://lingyixia.github.io/2019/02/03/learningAndMovingAverage/","excerpt":"参考滑动平均(exponential moving average),或者叫做指数加权平均(exponentially weighted moving average)，目的是用历史值和当前值的加权来代替当前值,这样可以使值的变化更加平滑.","text":"参考滑动平均(exponential moving average),或者叫做指数加权平均(exponentially weighted moving average)，目的是用历史值和当前值的加权来代替当前值,这样可以使值的变化更加平滑. 用滑动平均估计局部均值 只用在验证或测试阶段，训练时并不是使用 假设变量$v_t$表示变量$v$在时间$t$处的值,不使用滑动平均模型时的值为$\\theta_t$: v_t = \\beta v_{t-1}+(1-\\beta)\\theta_t \\quad \\beta \\in[0,1] \\tag{1} \\\\eg: \\begin{align} v_1 &= \\theta_1 \\\\ v_2 &=\\beta \\theta_1+(1-\\beta)\\theta_2 \\\\ v_3 &= \\beta v_2 + (1-\\beta)\\theta_3=\\beta^2\\theta_1+\\beta(1-\\beta)\\theta_2+(1-\\beta)\\theta_3\\\\ v_4 &=\\beta^3\\theta_1+ \\beta^2(1-\\beta)\\theta_2+\\beta(1-\\beta)\\theta_3+(1-\\beta)\\theta_4 \\end{align}即: v_t=(1-\\beta)\\theta_t+\\beta(1-\\beta)\\theta_{t-1}+\\beta^2(1-\\beta)\\theta_{t-2}+...+\\beta^k(1-\\beta)\\theta_{t-k}+...+\\beta^n\\theta_1 \\tag{2}上诉实例可以说明$v_t$是所有历史值的加权平均。现在我们要证明:当$\\beta \\rightarrow 1$时,公式(1)计算$v_t$约等于前$\\frac{1}{1-\\beta}$个时间点的加权.比如$\\beta = 0.9$,约等于前10项的加权平均证明:我们假设$k=\\frac{1}{1-\\beta}$,当$\\beta \\rightarrow 1$时,$k \\rightarrow +\\infty$.只要证明$\\beta^k$足够小即可. \\because \\beta^{\\frac{1}{1-\\beta}}=(1-\\frac{1}{k})^k \\\\ \\therefore \\lim_{k\\rightarrow +\\infty}(1-\\frac{1}{k})^n=e^{-1}≈0.3679\\\\ \\therefore \\beta^k ≈ 0.3679我们认为$e^{-1}$足够小,即当$\\beta \\rightarrow 1$时$k=\\frac{1}{1-\\beta}$之后的项都可以忽略,即可以认为公式(1)是前$\\frac{1}{1-\\beta}$项的加权和. 公式(1)中,虽然可以得到上诉结论,但是当$t$比较小的时候,前面不足$\\frac{1}{1-\\beta}$项的时候用公式(1)得到的值和真实值差距较大,所以还要做一个调整: v_t = \\frac{\\beta v_{t-1}+(1-\\beta)}{1-\\beta^t} \\qquad \\beta \\in[0,1] \\tag{2} \\\\当$t$较小的时候分母的调整作用较大,当$t$大到一定程度的时候分母接近1,调整作用越来越小。 tensorflow中的滑动平均eg:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109# coding:utf-8#-------------------------------------------------------------------------------# @Author chenfeiyu01# @Name: Temp.py# @Project TensorflowLearn# @Product PyCharm# @DateTime: 2019-06-16 15:59# @Contact chenfeiyu01@baidu.com# @Version 1.0# @Description:#-------------------------------------------------------------------------------import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataimport osos.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;] = &quot;TRUE&quot;mnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True)BATCH_SIZE = 100INPUT_NODE = 784OUTPUT_NODE = 10LEARNING_RATE_BASE = 0.8LEARNING_RATE_DECAY = 0.99REGULARIZATION_RATE = 0.0001TRAINING_STEPS = 5000MOVING_AVERAGE_DECAY = 0.9999def inference(input_tensor, ema, weights, biases): # 不使用滑动平均类 if ema == None: output = tf.nn.softmax(tf.nn.xw_plus_b(input_tensor, weights, biases)) return output else: # 使用滑动平均类 output = tf.nn.softmax(tf.nn.xw_plus_b(input_tensor, ema.average(weights), ema.average(biases))) return outputdef train(mnist, ifMovingAverage, ifDecayLearnRate): x = tf.placeholder(tf.float32, shape=[None, INPUT_NODE], name=&apos;input_x&apos;) y_ = tf.placeholder(tf.float32, shape=[None, OUTPUT_NODE], name=&apos;input_y&apos;) weights = tf.Variable(tf.truncated_normal(shape=[INPUT_NODE, OUTPUT_NODE], stddev=0.1)) biases = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE])) y = inference(x, None, weights=weights, biases=biases) global_step = tf.Variable(0, trainable=False) ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) ema_op = ema.apply(tf.trainable_variables()) average_y = inference(x, ema, weights, biases) cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.arg_max(y_, 1)) cross_entropy_mean = tf.reduce_mean(cross_entropy) regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE) regularization = regularizer(weights) + regularizer(weights) loss = cross_entropy_mean + regularization learning_rate = LEARNING_RATE_BASE if ifDecayLearnRate: learning_rate = tf.train.exponential_decay( LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY, staircase=True ) train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) with tf.control_dependencies([train_step, ema_op]): train_op = tf.no_op(name=&apos;train&apos;) correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) if ifMovingAverage: correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) with tf.Session() as sess: tf.global_variables_initializer().run() validate_feed = &#123; x: mnist.validation.images, y_: mnist.validation.labels &#125; test_feed = &#123; x: mnist.test.images, y_: mnist.test.labels &#125; for i in range(TRAINING_STEPS): if i % 1000 == 0: validate_acc = sess.run(accuracy, feed_dict=validate_feed) print(f&apos;After &#123;i&#125; training steps, validation accuracy is &#123;validate_acc&#125;&apos;) xs, ys = mnist.train.next_batch(BATCH_SIZE) sess.run(train_op, feed_dict=&#123;x: xs, y_: ys&#125;) test_acc = sess.run(accuracy, feed_dict=test_feed) print(f&apos;after &#123;TRAINING_STEPS&#125; steps, test accuracy is &#123;test_acc&#125;&apos;)if __name__ == &apos;__main__&apos;: train(mnist, ifMovingAverage=True, ifDecayLearnRate=False) 其基本思路是对于一个已经训练好的模型,比如其中有一个参数是v,在验证或测试阶段并不是直接用这个参数去直接计算,而是保存一个影子变量shadow_variable,使用公式:shadow_variable=decay*shadow_variable+(1-decay)*Variable来充当计算时候使用的参数,即在真实变量和影子变量之间做一个平滑运算,一般更接近影子变量.刚开始的时候(还没运行maintain_averages_op)影子变量=原始变量,此后每运行一次maintain_averages_op便计算一次其控制的所有变量的影子变量.然后使用ema.average()函数拿出计算后得到的影子变量。衰减率的计算公式为: min\\left\\{ decay,\\frac{1+num\\_updates}{10+num\\_updates} \\right\\}","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"https://lingyixia.github.io/tags/tensorflow/"}]},{"title":"链表总结","slug":"linkLists","date":"2019-02-02T03:43:08.000Z","updated":"2021-09-19T18:01:25.960Z","comments":true,"path":"2019/02/02/linkLists/","link":"","permalink":"https://lingyixia.github.io/2019/02/02/linkLists/","excerpt":"链表题型总结","text":"链表题型总结 倒数第K个 给出一个单链表,输出该链表倒数第K个节点 三种解法: 解法一遍历一遍,记录长度L,然后重新遍历到第L-K的位置,代码略 解法二123456789101112131415//递归,先递归到链表尾,然后往回走,每走一步K-1,如果K-1！=0则上抛NULL,反之就是找到了,上抛该节点即可ListNode* FindKthToTail(ListNode* pListHead, unsigned int&amp; k) //**注意引用传递的方式**&#123; if (!pListHead) return NULL; ListNode* node = FindKthToTail(pListHead-&gt;next, k); if (node)//找到了,不断上抛该节点 &#123; return node; &#125; if (--k == 0) &#123; return pListHead;//第一次找到,开始上抛 &#125; return NULL;&#125; 解法三12345678910111213141516171819//快慢指针1ListNode* FindKthToTail(ListNode* pListHead, unsigned int k)&#123; int count = k; if (!pListHead || k == 0) return NULL; ListNode* behind = pListHead; ListNode* front = pListHead; bool flag = false; while (front) &#123; if (count-- &lt;= 0) &#123; behind = behind-&gt;next; flag = true; &#125; front = front-&gt;next; &#125; return flag || count == 0 ? behind : NULL;&#125; 判断有无环一般解法有两种 解法一 将遍历过的节点都加入哈希表,每次该点是否在哈希表中: 123456789101112131415bool hasCycle(ListNode *head)&#123; unordered_set&lt;ListNode*&gt; record; ListNode* p = head; while (p) &#123; if (record.count(p)&gt;0) &#123; return true; &#125; record.insert(p); p = p-&gt;next; &#125; return false;&#125; 解法二 快慢指针,快指针一定会在环中与慢指针相遇,下面证明这个结论命题:如图所示: 注意,图中的$m,n$都是移动次数,即箭头个数,不是圆圈的个数!!!其中快指针速度为$s$,慢指针速度为$q$且$s&gt;q$,问为什么两者一定在环内相遇?证明:假设慢指针第一次到达环内移动了$x$次,此时慢指针在环内的位置为$a$,则快指针也移动了$x$次,位置为$b$,则现在题目转化为:必有当继续走$y$步时使得$(a+s \\times y)\\mod n=(b+q \\times y) \\mod n$,即 ((b-a)+(q-s) \\times y)\\mod n =0 \\tag{1}成立.当移动$x$后有: a=(s \\times x - m) \\mod n \\tag{2} b=(q \\times x -m) \\mod n \\tag{3}将$(2)(3)$带入$(1)$中: ((q-s) \\times x) \\mod n +(q-s) \\times y) \\mod n =0由于:$((q-s) \\mod n) \\mod n =(q-s) \\mod n$(多次取模等于一次取模)故 ((q-s) \\times (x+y)) \\mod n=0 \\tag{4}即只需要$(x+y)$是$n$的整数倍即可.因为我们一般取$q=2,s=1$,因为当$q=2,s=1$的时候最快取得$n$得整数倍,即循环次数最少。 1234567891011121314151617bool hasCycle1(ListNode *head)&#123; ListNode* slow = head; ListNode* fast = head; while (fast) &#123; fast = fast-&gt;next; if (!fast) return false; fast = fast-&gt;next; slow = slow-&gt;next; if (fast == slow) &#123; return true; &#125; &#125; return false;&#125; 判断环起始位置 求起始位置也就是求上图中$m$得长度,由于$q=2,s=1,x=m$,相遇位置为$y$,$y$是距离环起始位置的长度,上图(4)式可改为: (m+y) \\mod n =0但是在这里要写成这样的形式: (y+m) \\mod n =0 \\tag{5}即$m$是在$y$的基础上再增加,公式(5)和公(4)意义不同,公式(4)指的是当符合公式(4)的时候前后两指针相遇,公式(5)的意义是当符合该公式的时候对$m$取模为零,即正好在环头位置.eg: 图中在节点5处相遇,此时$m+y$就是$1,2,3,4,5$,此时有$(m+y) \\mod n=0$,现在把这个长度的前$m$放到后面,$3,4,5,6,3$,即$y+m$,此时同样有$(y+m)\\mod n =0$,因此现在要做的就是一个指针放到头部,一个指针放到相遇的地方,依次next一步,当两者相遇的时候就是环的初始位置. 12345678910111213141516171819202122232425ListNode *detectCycle(ListNode *head) &#123; ListNode* slow=head; ListNode* fast = head; while(fast) &#123; fast=fast-&gt;next; if(!fast) return NULL; fast=fast-&gt;next; slow=slow-&gt;next; if(fast==slow) &#123; break; &#125; &#125; if(!fast) return NULL; ListNode* p =head; ListNode* q=fast; while(p!=q) &#123; p=p-&gt;next; q=q-&gt;next; &#125; return p; &#125; 删除链表重复元素重复保留一个12345678910111213ListNode *deleteDuplicates(ListNode *head)&#123; if (!head || !head-&gt;next) &#123; return head; &#125; while (head-&gt;next &amp;&amp; head-&gt;next-&gt;val == head-&gt;val) &#123; head = head-&gt;next; &#125; head-&gt;next = deleteDuplicates(head-&gt;next); return head;&#125; 重复全删解法一:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051struct ListNode&#123; int val; ListNode* next; ListNode(int x) :val(x), next(NULL) &#123;&#125;&#125;;void deleteOne(ListNode* head,ListNode* current)//注意删除单节点方式&#123; if (current-&gt;next) &#123; current-&gt;val = current-&gt;next-&gt;val; current-&gt;next = current-&gt;next-&gt;next; &#125; else &#123; ListNode* p = head;//如果删除的是尾节点则需要从头遍历 while (p-&gt;next!=current) &#123; p = p-&gt;next; &#125; p-&gt;next = NULL; &#125;&#125;ListNode* deleteDuplicates(ListNode* pHead)&#123; ListNode* newHead = new ListNode(-100); newHead-&gt;next = pHead; ListNode* before = newHead; ListNode* after = pHead; bool flag = false; while (after) &#123; while (after &amp;&amp;( before-&gt;val == after-&gt;val)) &#123; flag = true; deleteOne(newHead,after); after = before-&gt;next; &#125; if (flag) &#123; deleteOne(newHead,before); flag = false; &#125; else &#123; before = after; &#125; after = before-&gt;next; &#125; return newHead-&gt;next;&#125; 解法二:1234567891011121314151617ListNode *deleteDuplication(ListNode *pHead)&#123; if (!pHead || !pHead-&gt;next) return pHead; if (pHead-&gt;next-&gt;val == pHead-&gt;val) &#123; while (pHead-&gt;next &amp;&amp; pHead-&gt;next-&gt;val == pHead-&gt;val) &#123; pHead = pHead-&gt;next; &#125; return deleteDuplication(pHead-&gt;next); &#125; else &#123; pHead-&gt;next = deleteDuplication(pHead-&gt;next); return pHead; &#125;&#125; 两个排序链表合并1234567891011121314ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123; if(!l1) return l2; if(!l2) return l1; if(l1-&gt;val&lt;l2-&gt;val) &#123; l1-&gt;next = mergeTwoLists(l1-&gt;next,l2); return l1; &#125; else &#123; l2-&gt;next = mergeTwoLists(l1,l2-&gt;next); return l2; &#125; &#125; 单链表排序归并12345678910111213141516171819202122232425262728293031323334353637383940ListNode* getMidNode(ListNode* head) &#123; ListNode* slow=head; ListNode* fast =head-&gt;next; while(fast) &#123; fast=fast-&gt;next; if(fast) &#123; fast=fast-&gt;next; slow=slow-&gt;next; &#125; &#125; return slow; &#125; ListNode* mergeList(ListNode* l1,ListNode* l2) &#123; if(!l1) return l2; if(!l2) return l1; if(l1-&gt;val&lt;=l2-&gt;val) &#123; l1-&gt;next = mergeList(l1-&gt;next,l2); return l1; &#125; l2-&gt;next = mergeList(l1,l2-&gt;next); return l2; &#125; ListNode* sortList(ListNode* head) &#123; if(head &amp;&amp; head-&gt;next) &#123; ListNode* mid=getMidNode(head); ListNode* left = head; ListNode* right = mid-&gt;next; mid-&gt;next=NULL; left = sortList(left); right = sortList(right); return mergeList(left,right); &#125; return head; &#125; 单链表每k个一组反转百度面试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;iostream&gt;using namespace std;struct ListNode&#123; ListNode* next; int val; ListNode(int val) :next(NULL), val(val) &#123;&#125;;&#125;;ListNode* reverseKGroup(ListNode* head, int k) &#123; ListNode* cursor = head; for (int i = 1; i &lt; k; i++) &#123; if (!cursor-&gt;next) return head; cursor = cursor-&gt;next; &#125; ListNode* tempHead = head; ListNode* tempLast = head; head = cursor; cursor = head; while (tempLast-&gt;next != head) &#123; cursor = tempLast-&gt;next; tempLast-&gt;next = cursor-&gt;next; cursor-&gt;next = tempHead; tempHead = cursor; &#125; tempLast-&gt;next = reverseKGroup(head-&gt;next, k); head-&gt;next = tempHead; return head;&#125;int main()&#123; ListNode* head = new ListNode(1); ListNode* p = head; p-&gt;next = new ListNode(2); p = p-&gt;next; p-&gt;next = new ListNode(3); p = p-&gt;next; p-&gt;next = new ListNode(4); p = p-&gt;next; p-&gt;next = new ListNode(5); ListNode* result = reverseKGroup(head,3); return 0;&#125;","categories":[],"tags":[{"name":"链表","slug":"链表","permalink":"https://lingyixia.github.io/tags/链表/"}]},{"title":"activationFunction","slug":"activationFunction","date":"2019-01-26T13:11:08.000Z","updated":"2021-09-19T18:01:25.949Z","comments":true,"path":"2019/01/26/activationFunction/","link":"","permalink":"https://lingyixia.github.io/2019/01/26/activationFunction/","excerpt":"为什么要使用激活函数","text":"为什么要使用激活函数 看这篇文章即可","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lingyixia.github.io/categories/神经网络/"}],"tags":[{"name":"激活函数","slug":"激活函数","permalink":"https://lingyixia.github.io/tags/激活函数/"}]},{"title":"scrapyAndRedis","slug":"scrapyAndRedis","date":"2019-01-24T06:23:53.000Z","updated":"2021-09-19T18:01:25.965Z","comments":true,"path":"2019/01/24/scrapyAndRedis/","link":"","permalink":"https://lingyixia.github.io/2019/01/24/scrapyAndRedis/","excerpt":"本篇并不是要说scrapy_redis框架,而是要说明另外一个结构:我们有一个需要,需要一个生产者(产生url)和一个消费者(即该爬虫,消费url),消费者监听生产者,当产生url的时候需要将其接过来,并爬取该url的内容。咋一看很符合scrapy_reids结构,但生产者和消费者是约定通过redis发布/订阅的方式交互的,由于对scrapy_reids了解不深,没深想如何改造,便投机取巧的使用了这样一种方式.","text":"本篇并不是要说scrapy_redis框架,而是要说明另外一个结构:我们有一个需要,需要一个生产者(产生url)和一个消费者(即该爬虫,消费url),消费者监听生产者,当产生url的时候需要将其接过来,并爬取该url的内容。咋一看很符合scrapy_reids结构,但生产者和消费者是约定通过redis发布/订阅的方式交互的,由于对scrapy_reids了解不深,没深想如何改造,便投机取巧的使用了这样一种方式. 问题本来其实这样的代码就可以了:12345678910111213def start_requests(self): while True: msg = self.redis_sub.parse_response() if msg[0] != b&apos;message&apos;: continue data = json.loads(msg[2].decode(&apos;utf-8&apos;)) id = data[&apos;id&apos;] styleUrl = data[&apos;styleUrl&apos;] pageCount = data[&apos;pageCount&apos;] self.obi.public(json.dumps(&#123;&apos;id&apos;: id, &apos;isSynchronized&apos;: 1&#125;)) yield SplashRequest(url=styleUrl, callback=self.specHome_parse, args=&#123;&apos;wait&apos;: 5, &apos;timeout&apos;: 60, &apos;images&apos;: 0&#125;, meta=&#123;&apos;pageCount&apos;: pageCount, &apos;id&apos;: id, &apos;dont_redirect&apos;: True&#125;) 即在start_request(self)函数中死循环监听redis,比如生产者那边产生了10个,msg = self.redis_sub.parse_response()这行监听的函数便在第11次循环的时候阻塞,而前面yield的10个SplashRequest会继续执行爬取任务(想的很好),但现实并不成功,当代码阻塞的时候发现原先的10个SplashRequest也不执行了.但是我我以前的经验中已知,下面这样的代码是可以顺利执行的. 1234567def start_requests(self): i=0 while True: yield SplashRequest(url=urls[i], callback=self.specHome_parse, args=&#123;&apos;wait&apos;: 5, &apos;timeout&apos;: 60, &apos;images&apos;: 0&#125;, meta=&#123;&apos;pageCount&apos;: 22, &apos;id&apos;: 1, &apos;dont_redirect&apos;: True&#125;) i++ 当urls中的url很多,比如上百个的时候,该代码是可以执行的,也就是说while True是没问题的,但是为什么阻塞了原先的SplashRequest就不继续了呢?具体还不清楚,但是经验告诉我阻塞了不行,但是让程序动起来就有可能成功,于是改造如下: 解决方法1234567891011121314151617def start_requests(self): while True: try: msg = self.redis_sub.parse_response(block=False, timeout=5) if msg[0] != b&apos;message&apos;: continue data = json.loads(msg[2].decode(&apos;utf-8&apos;)) id = data[&apos;id&apos;] styleUrl = data[&apos;styleUrl&apos;] pageCount = data[&apos;pageCount&apos;] self.obi.public(json.dumps(&#123;&apos;id&apos;: id, &apos;isSynchronized&apos;: 1&#125;)) yield SplashRequest(url=styleUrl, callback=self.specHome_parse, args=&#123;&apos;wait&apos;: 5, &apos;timeout&apos;: 60, &apos;images&apos;: 0&#125;, meta=&#123;&apos;pageCount&apos;: pageCount, &apos;id&apos;: id, &apos;dont_redirect&apos;: True&#125;) except Exception as e: yield SplashRequest() print(e) 即在生产者没有生产url的时候不断yield空的请求,这样让程序动起来,竟然真的可以了!!!","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://lingyixia.github.io/categories/爬虫/"}],"tags":[{"name":"scrapy","slug":"scrapy","permalink":"https://lingyixia.github.io/tags/scrapy/"},{"name":"python","slug":"python","permalink":"https://lingyixia.github.io/tags/python/"},{"name":"redis","slug":"redis","permalink":"https://lingyixia.github.io/tags/redis/"}]},{"title":"CNN理解","slug":"CNN","date":"2019-01-23T12:11:10.000Z","updated":"2021-09-19T18:01:25.944Z","comments":true,"path":"2019/01/23/CNN/","link":"","permalink":"https://lingyixia.github.io/2019/01/23/CNN/","excerpt":"","text":"CNN的卷积层和普通神经网络全连接层对比 为了过渡，首先看一下这篇笔记 举例: 只有一个数据:$6\\times6$,$Filter:3\\times3$,如图所示: 全连接神经网络 参数数量为 $(6\\times 6+1)\\times n$ CNN 参数数量为 $(3\\times 3+1)\\times n$ 对比可以很明显的看出，其实两者相差并不很多，其实feather map数量就是全连接层中的神经元数量，由于每个神经元所含参数只有9个，不能像全连接(每个神经元36个参数)那样按照矩阵相乘想乘得到一个数字，因此是用卷积的方式得到一个feather map，卷积过程中36个数据共享9个参数。 其他 很明显，通过卷积+池化+全连接层也能反向传播 其实能学到东西，关键在于初始化$Weights$，在普通全连接神经网络中: 12345678910def add_layer(inputs, in_size, out_size, activation_function=None): Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases output = tf.nn.dropout(output, keep_prob) if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs 在cnn的卷积层中: 1234567def conv2d(input, shape, activation_function): Weights = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1)) biases = tf.constant(0.1, shape=[1, shape[3]]) convOutput = tf.nn.conv2d(input, filter=Weights, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;) + biases if activation_function: convOutput = activation_function(convOutput) return convOutput 也就是说，同一层中多个神经元的参数初始化是随机的，千万不能初始化为同一个值，这样会导致每个神经元的输出永远相同，也就是每个神经元学到的东西是相同的！！！每个神经元初始化Weights的不同是每个神经元学到不同特征的前提.","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"https://lingyixia.github.io/tags/CNN/"}]},{"title":"C++数组长度","slug":"cpp-array","date":"2019-01-23T03:11:08.000Z","updated":"2021-09-19T18:01:25.957Z","comments":true,"path":"2019/01/23/cpp-array/","link":"","permalink":"https://lingyixia.github.io/2019/01/23/cpp-array/","excerpt":"记录函数内获取数组长度的坑","text":"记录函数内获取数组长度的坑 看代码:12345678910111213141516171819202122#include&lt;iostream&gt;using namespace std;void fun(int* array)&#123; cout &lt;&lt; array &lt;&lt; endl; cout &lt;&lt; sizeof(array);&#125;int main()&#123; int array[] = &#123; 1,2,3,4,5,6 &#125;; cout &lt;&lt; array &lt;&lt; endl; cout &lt;&lt; sizeof(array) &lt;&lt; endl; fun(array); return 0;&#125;结果：00DEFCA02400DEFCA04 可以看出，虽然函数体内外的array所指的地址相同，sizeof后并不一样，前者是实际数组的大小，后者是纯指针的大小（编译器决定），也就是说，当数组传到函数内后，就意味着它是一个纯指针，不再有数组的意义了，要想在函数内获取数组长度，只能以参数的形式传入。总之，想在函数内获取数组长度就用vector吧！！！","categories":[{"name":"基础","slug":"基础","permalink":"https://lingyixia.github.io/categories/基础/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://lingyixia.github.io/tags/C/"},{"name":"坑","slug":"坑","permalink":"https://lingyixia.github.io/tags/坑/"}]},{"title":"开平方","slug":"sqrt","date":"2019-01-22T01:29:55.000Z","updated":"2021-09-19T18:01:25.965Z","comments":true,"path":"2019/01/22/sqrt/","link":"","permalink":"https://lingyixia.github.io/2019/01/22/sqrt/","excerpt":"二分法和牛顿法求一个非负数开平方","text":"二分法和牛顿法求一个非负数开平方 二分法123456789101112131415161718192021222324252627282930double sqrtBinary(double x)&#123; clock_t startTime, endTime; startTime = clock(); double min=0.0; double max=x/2+1; double p = 1e-5; double mid; int interIndex = 0; while ((max-min)&gt;=p) &#123; interIndex++; mid = (min + max) / 2; if (mid*mid&gt;x) &#123; max = mid; &#125; else if (mid*mid&lt;x) &#123; min = mid; &#125; else &#123; return mid; &#125; &#125; endTime = clock(); cout &lt;&lt;&quot;迭代次数:&quot;&lt;&lt;interIndex&lt;&lt;&quot;计算时间:&quot;&lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return mid;&#125; 牛顿法12345678910111213141516171819202122double sqrtNewTon(double n) &#123; clock_t startTime, endTime; startTime = clock();//计时开始 if (n == 0) &#123; return 0; &#125; double last = 0.0; double current = 1.0; double p = 1e-5; int interIndex = 0; while (fabs(current - last) &gt;= p) &#123; interIndex++; last = current; current = (last + n / last) / 2; &#125; endTime = clock();//计时开始 cout &lt;&lt;&quot;迭代次数:&quot;&lt;&lt;interIndex&lt;&lt;&quot;计算时间:&quot;&lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return current;&#125; 牛顿法解释: 对于求$x^2=n$的$x$,首先令$f(x)=x^2-n$,任意取$x_0$,则点为$(x_0,f(x_0))$,则该点的切线方程为: f(x)=f(x_0)+f(x_0)^{\\prime}(x-x_0)即: \\begin{align} f(x)&=x_0^2-n+(2*x_0)(x-x_0) \\\\ &=2x_0x-(x_0^2+n) \\end{align}令$f(x)=0$,得$x=\\frac{(x_0+\\frac{n}{x_0})}{2}$即: x_{i+1}=\\frac{(x_i+\\frac{n}{x_i})}{2}详细见此牛顿法编程一般步骤:1234567891.定义精度p2.定义初始值current3. 定义last保存上次currentwhile (fabs(current-last)&gt;p)&#123; last = current; current = current - f(x)/f&apos;(x)的&#125;cout&lt;&lt;current;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"二分法","slug":"二分法","permalink":"https://lingyixia.github.io/tags/二分法/"},{"name":"牛顿法","slug":"牛顿法","permalink":"https://lingyixia.github.io/tags/牛顿法/"}]},{"title":"明朝那些事儿","slug":"dynastyOfMing","date":"2019-01-21T08:47:01.000Z","updated":"2021-09-19T18:01:25.958Z","comments":true,"path":"2019/01/21/dynastyOfMing/","link":"","permalink":"https://lingyixia.github.io/2019/01/21/dynastyOfMing/","excerpt":"内容太多啦,梳理一下方便系统学习","text":"内容太多啦,梳理一下方便系统学习 皇帝 太祖朱元璋(1328-1398)年号“洪武”1368年称帝，在位31年。 惠帝朱允炆(1377-1402)年号“建文”1398年即位，在位5年。 成祖朱棣(1360-1424)年号“永乐”1402年即位，在位23年。 仁宗朱高炽(1378-1425)年号“洪熙”1424年即位，在位1年。 宣宗朱瞻基(1398-1435)年号“宣德”1425年即位，在位11年。 英宗朱祁镇(1427-1464)年号“正统”;“天顺”(1435-1449;1457-1464)在位，在位23年。 景帝朱祁钰(1428-1457)年号“景泰”1449年即位，在位9年。 宪宗朱见深(1447-1487)年号“成化”1464年即位，在位24年。 孝宗朱佑樘(1470-1505)年号“弘治”1487年即位，在位19年。 武宗朱厚照(1491-1521)年号“正德”1505年即位，在位17年。 世宗朱厚璁(1507-1566)年号“嘉靖”1521年即位，在位46年。 穆宗朱载垢(1537-1572)年号“隆庆”1566年即位，在位7年。 神宗朱翊钧(1563-1620)年号“万历”1572年即位，在位48年。 光宗朱常洛(1582-1620)年号“泰昌”1620年即位，在位1月。 熹宗朱由校(1605-1627)年号“天启”1620年即位，在位8年。 思宗朱由检(1610-1644)年号“崇祯”1627年即位，在位17年。 2019年3月13日,终于读完,mmp的,本来还想着看的慢一点多恶补历史知识，结果看了将近50天，还是没记住多少,目前最大的印象就是要相信所谓的”气数”,”气数”是推动历史前进的一股无形的力量,它觉得你应该受万人景仰,你的道路就会通畅无阻,它觉得你该滚下台了你就会举步维艰,所谓风水轮流转是也.所以,当你觉得时运不济,屋漏偏逢连夜雨,不要一味坚持,有时候要认命。当然,所谓的”气数”的必要条件是”时间”,你一上来遇到难题就趴下了那不行,只有辉煌过一段时间,突然不知道什么时候开始走下坡路,而且各种难题接踵而至,这时候就要思考一下是不是”气数已尽”,是不是应该放弃坚持,另谋出路。可怜的人类在历史面前还是太渺小了。","categories":[{"name":"读书","slug":"读书","permalink":"https://lingyixia.github.io/categories/读书/"}],"tags":[]},{"title":"C++操作符重载","slug":"Operation","date":"2019-01-20T02:14:08.000Z","updated":"2021-09-19T18:01:25.947Z","comments":true,"path":"2019/01/20/Operation/","link":"","permalink":"https://lingyixia.github.io/2019/01/20/Operation/","excerpt":"以平面点为例记录C++中操作符重载写法","text":"以平面点为例记录C++中操作符重载写法 重载规则 除了类属关系运算符 “ . “ 、成员指针运算符 “ .* “ 、作用域运算符 “ :: “ 、sizeof运算符和三目运算符 “ ?: “ 以外，C++ 中的所有运算符都可以重载。 C++不允许用户自己定义新的运算符，只能对已有的C++运算符进行重载。 重载不能改变运算符运算对象（即操作数）的个数。(双目运算符重载还是双目,单目运算符重载还是单目) 重载之后的运算符不能改变运算符的优先级和结合性(如复制运算符”=“是右结合性（自右至左），重载后仍为右结合性)，也不能改变运算符操作数的个数及语法结构。 重载运算符的函数不能有默认的参数 重载的运算符必须和用户定义的自定义类型的对象一起使用，其参数至少应有一个是类对象（或类对象的引用）,也就是说，参数不能全部是C++的标准类型. 用于类对象的运算符一般必须重载，但有两个例外，运算符”=“和运算符”&amp;“不必用户重载。 运算符重载函数可以是类的成员函数，也可以是类的友元函数(可以访问类的私有成员)，还可以是既非类的成员函数也不是友元函数的普通函数。实例代码12345678910111213141516171819202122232425262728293031323334#include&lt;iostream&gt;using namespace std;struct Point&#123; int x, y; Point() :x(0), y(0) &#123;&#125; Point(int _x, int _y) :x(_x), y(_y) &#123;&#125; Point operator+(Point&amp; p) &#123; Point result; result.x = this-&gt;x + p.x; result.y = this-&gt;y + p.y; return result; &#125; friend int operator*(Point &amp;p1, Point &amp;p2); //实现内积 Point operator++(int pos)//有参数后置 &#123; return Point(this-&gt;x++, this-&gt;y++); &#125; Point operator++()//无参数前置 &#123; return Point(++this-&gt;x, ++this-&gt;y); &#125;&#125;;int operator*(Point &amp;p1, Point &amp;p2)&#123; return (p1.x*p2.x) + (p1.y*p2.y);&#125;输出:20p3:(3,9)p4:1p4:(2,3)p4:2","categories":[{"name":"基础","slug":"基础","permalink":"https://lingyixia.github.io/categories/基础/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://lingyixia.github.io/tags/C/"}]},{"title":"为什么使用交叉熵损失函数","slug":"LossFunction","date":"2019-01-19T22:11:08.000Z","updated":"2021-09-19T18:01:25.946Z","comments":true,"path":"2019/01/20/LossFunction/","link":"","permalink":"https://lingyixia.github.io/2019/01/20/LossFunction/","excerpt":"为什么使用交叉熵损失函数而不是二次代价函数","text":"为什么使用交叉熵损失函数而不是二次代价函数本文基本转自这篇文章，感谢作者 前奏 作为神经网络，应当具有自学习的能力，为了更好的模拟人学习的过程，神经网络的学习能力应当能够自我调整，当发现自己犯的错误越大时，改正的力度就越大。比如投篮：当运动员发现自己的投篮方向离正确方向越远，那么他调整的投篮角度就应该越大，篮球就更容易投进篮筐。这所谓的学习能力便体现在损失函数中，常见的损失函数有两种：二次代价函数和交叉熵损失函数，前者主要用在线性回归中，而在神经网络中主要用后者，下面我们来说明为什么。 以一个神经元的二类分类训练为例，进行两次实验,激活函数采用$sigmoid$：输入一个相同的样本数据x=1.0（该样本对应的实际分类y=0）；两次实验各自随机初始化参数，从而在各自的第一次前向传播后得到不同的输出值，形成不同的$Loss$： 在实验1中，随机初始化参数，使得第一次输出值为0.82（该样本对应的实际值为0）;经过300次迭代训练后，输出值由0.82降到0.09，逼近实际值。而在实验2中，第一次输出值为0.98，同样经过300迭代训练，输出值只降到了0.20。从两次实验的代价曲线中可以看出：实验1的代价随着训练次数增加而快速降低，但实验2的代价在一开始下降得非常缓慢；直观上看，初始的误差越大，收敛得越缓慢。 下面计算两种损失函数,eg:$y$是真实值,$\\hat y$是计算值,$z=wx+b$,$\\hat y = \\sigma (z)$ 二次代价损失函数 C=\\frac{1}{2n}\\sum_x|y-\\hat y |^2以一个样本为例: L=\\frac{(y-\\hat y)^2}{2}则有 \\begin{align} \\frac{\\partial L}{\\partial w} &=\\frac{\\partial L}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial z} \\frac{\\partial z}{\\partial w} \\\\ &=(\\hat y-y) \\sigma\\prime(x)x \\end{align} \\begin{align} \\frac{\\partial L}{\\partial w} &=\\frac{\\partial L}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial z} \\\\ &=(\\hat y-y) \\sigma\\prime(x) \\end{align} 其中，z表示神经元的输入，表示激活函数。从以上公式可以看出，w和b的梯度跟激活函数的梯度成正比，激活函数的梯度越大，w和b的大小调整得越快，训练收敛得就越快。而神经网络常用的激活函数为sigmoid函数，该函数的曲线如下所示: 如图所示，实验2的初始输出值（0.98）对应的梯度明显小于实验1的输出值（0.82），因此实验2的参数梯度下降得比实验1慢。这就是初始的代价（误差）越大，导致训练越慢的原因。与我们的期望不符，即：不能像人一样，错误越大，改正的幅度越大，从而学习得越快。 交叉熵损失函数 C=\\frac{1}{n}\\sum_x[y\\ln \\hat y+(1-y)\\ln (1-\\hat y)]以一个样本为例: L=-\\sum_x[y\\ln \\hat y+(1-y)\\ln (1-\\hat y)]则有 \\begin{align} \\frac{\\partial L}{\\partial w}&=-\\sum_x[\\frac{y}{\\hat y}-\\frac{(1-y)}{1-\\hat y}] \\frac{\\partial \\hat y}{\\partial z}\\frac{\\partial z}{\\partial w}\\\\ &=-\\sum_x[\\frac{y}{\\hat y}-\\frac{(1-y)}{1-\\hat y}]\\sigma \\prime(x)x\\\\ &=-\\sum_x[\\frac{y- \\sigma (x)}{\\sigma(x)(1-\\sigma(x))}] \\sigma \\prime(x) x \\\\ &= -\\sum_x[y-\\hat y]x \\end{align} \\begin{align} \\frac{\\partial L}{\\partial b}&=-\\sum_x[y-\\hat y] \\end{align} 因此，$w$的梯度公式中$\\sigma \\prime (z)$原来的被消掉了;另外，该梯度公式中的表示输出值与实际值之间的误差。所以，当误差越大，梯度就越大，参数$w$调整得越快，训练速度也就越快,$b$的梯度同理. 交叉熵函数来源以$w$的偏导为例:在二次代价函数中: \\frac{\\partial L}{\\partial w} = (\\hat y - y) \\sigma \\prime(x) x为了消除$\\sigma \\prime(x)$,我们令要计算的$w$偏导为: \\frac{\\partial L}{\\partial w} = (\\hat y - y) x而$w$偏导实际计算为: \\begin{align} \\frac{\\partial L}{\\partial w} &=\\frac{\\partial L}{\\partial \\hat y}\\sigma \\prime(x)x \\\\ &=\\frac{\\partial L}{\\partial \\hat y}\\hat y(1-\\hat y) x \\\\ \\end{align}则: \\frac{\\partial L}{\\partial \\hat y}\\hat y(1-\\hat y) x= (\\hat y - y) xx被消掉得: \\frac{\\partial L}{\\partial \\hat y} = \\frac{\\hat y-y}{(1-\\hat y)\\hat y}求积分得: L=-[y \\ln \\hat y +(1-y) \\ln (1- \\hat y)]即交叉熵函数.","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"熵","slug":"熵","permalink":"https://lingyixia.github.io/tags/熵/"},{"name":"损失函数","slug":"损失函数","permalink":"https://lingyixia.github.io/tags/损失函数/"}]},{"title":"C++参数传递方式","slug":"parameter_passing","date":"2019-01-19T22:11:08.000Z","updated":"2021-09-19T18:01:25.962Z","comments":true,"path":"2019/01/20/parameter_passing/","link":"","permalink":"https://lingyixia.github.io/2019/01/20/parameter_passing/","excerpt":"C++函数的三种传递方式为：值传递、指针传递和引用传递","text":"C++函数的三种传递方式为：值传递、指针传递和引用传递 值传递(略)指针传递(实质也是值传递)12345678910void fun(int *x)&#123; *x += 5; //修改的是指针x指向的内存单元值&#125;void main(void)&#123; int y = 0; fun(&amp;y);cout&lt;&lt;&lt;&lt;\\&quot;y = \\&quot;&lt;&lt;y&lt;&lt;endl; //y = 5;&#125; 实质是传递地址的值，即地址的‘值传递’ 引用传递12345678910void fun(int &amp;x)&#123; x += 5; //修改的是x引用的对象值 &amp;x = y;&#125;void main(void)&#123;int y = 0;fun(y);cout&lt;&lt;&lt;&lt;\\&quot;y = \\&quot;&lt;&lt;y&lt;&lt;endl; //y = 5;&#125; 实质是取别名，&amp;在C++中有两种作用，取别名和取地址（C中只有后者）,=号左边是引用，=号右边是取址。123int a=3; int &amp;b=a;//引用 int *p=&amp;a; //取地址 引用传递是C++的特性，值传递和指针传递是C语言中本来就有的方式。","categories":[{"name":"基础","slug":"基础","permalink":"https://lingyixia.github.io/categories/基础/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://lingyixia.github.io/tags/C/"}]},{"title":"requests","slug":"requests","date":"2019-01-17T07:03:43.000Z","updated":"2021-09-19T18:01:25.964Z","comments":true,"path":"2019/01/17/requests/","link":"","permalink":"https://lingyixia.github.io/2019/01/17/requests/","excerpt":"requests库的一些记录","text":"requests库的一些记录 1.重试次数和超时,加headers123456789101112131415import requests, timefrom requests.adapters import HTTPAdapterheaders = &#123; &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36&apos;&#125;if __name__ == &apos;__main__&apos;: print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)) sess = requests.Session() sess.mount(&apos;http://&apos;, HTTPAdapter(max_retries=5)) sess.mount(&apos;https://&apos;, HTTPAdapter(max_retries=5)) try: response = sess.get(&apos;http://www.google.com&apos;, timeout=3) except Exception as e: print(e) print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;))","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://lingyixia.github.io/tags/python/"},{"name":"requests","slug":"requests","permalink":"https://lingyixia.github.io/tags/requests/"}]},{"title":"mongo","slug":"mongo","date":"2019-01-17T01:39:16.000Z","updated":"2021-09-19T18:01:25.961Z","comments":true,"path":"2019/01/17/mongo/","link":"","permalink":"https://lingyixia.github.io/2019/01/17/mongo/","excerpt":"我日你妈卖批,手一滑训练集没了,特么的将近两个月的量,汽车之家啊大爷的，，，一定要备份，，一定要备份，，，一定要备份!!!!!!!!!!!!!!最后,祭奠老子第一次误删数据———2019-01-16日!!!!","text":"我日你妈卖批,手一滑训练集没了,特么的将近两个月的量,汽车之家啊大爷的，，，一定要备份，，一定要备份，，，一定要备份!!!!!!!!!!!!!!最后,祭奠老子第一次误删数据———2019-01-16日!!!! mongo数据备份:mongodump -h dbhost -d dbname -o dbdirectory mongo数据恢复:mongorestore -h &lt;hostname&gt;&lt;:port&gt; -d dbname &lt;path&gt;","categories":[{"name":"数据","slug":"数据","permalink":"https://lingyixia.github.io/categories/数据/"}],"tags":[{"name":"mongo","slug":"mongo","permalink":"https://lingyixia.github.io/tags/mongo/"},{"name":"教训","slug":"教训","permalink":"https://lingyixia.github.io/tags/教训/"}]},{"title":"Scrapy","slug":"Scrapy","date":"2019-01-16T02:36:54.000Z","updated":"2021-09-19T18:01:25.948Z","comments":true,"path":"2019/01/16/Scrapy/","link":"","permalink":"https://lingyixia.github.io/2019/01/16/Scrapy/","excerpt":"scrapy爬虫小记","text":"scrapy爬虫小记 setting文件常用配置默认seeting文件位置:Lib\\site-packages\\scrapy\\settings\\default_settings.py ROBOTSTXT_OBEY = False:默认为True,一般都需要改为False,谁会无聊到爬虫还遵循网站协议？ RETRY_HTTP_CODES = [500, 502, 503, 504, 408],默认需要重复的码,可自行添减,比如302 DOWNLOAD_DELAY:下载间隔时间,默认为0 RANDOMIZE_DOWNLOAD_DELAY=True,为True时下载间隔时间为0.5 * DOWNLOAD_DELAY and 1.5 * DOWNLOAD_DELAY DEFAULT_REQUEST_HEADERS中可以添加cookies middlewaresDownloaderMiddleware对于一个DownloaderMiddleware而言的生命周期函数而言,有两个主要函数需要处理1.process_request(self, request, spider)返回值有四种: None: 继续执行下一个DownloaderMiddleware(正常情况) Response对象: 不在调用其他DownloaderMiddleware的process_request函数,返回给scrapy引擎该Response对象 Request对象: Scrapy则停止调用process_request方法并重新调度返回的request raise一个IgnoreRequest异常: 则安装的下载中间件的 process_exception() 方法会被调用 2.process_response(self, request, response, spider)hexo返回值有三种: Response对象:继续执行下一个DownloaderMiddleware(正常情况) Request对象:同上 raise一个IgnoreRequest异常:调用request的errback(Request.errback) RetryMiddlewarescrapy默认:from scrapy.downloadermiddlewares.retry import RetryMiddleware","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://lingyixia.github.io/categories/爬虫/"}],"tags":[{"name":"scrapy","slug":"scrapy","permalink":"https://lingyixia.github.io/tags/scrapy/"},{"name":"python","slug":"python","permalink":"https://lingyixia.github.io/tags/python/"}]},{"title":"生活常识","slug":"commonSense","date":"2019-01-15T07:36:17.000Z","updated":"2021-09-19T18:01:25.957Z","comments":true,"path":"2019/01/15/commonSense/","link":"","permalink":"https://lingyixia.github.io/2019/01/15/commonSense/","excerpt":"The article has been encrypted, please enter your password to view.","text":"Nothing Valuable Incorrect Password! No content to display! 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","categories":[],"tags":[{"name":"生活常识","slug":"生活常识","permalink":"https://lingyixia.github.io/tags/生活常识/"}]},{"title":"PatchArray","slug":"leetcode_330","date":"2019-01-14T13:52:31.000Z","updated":"2021-09-19T18:01:25.959Z","comments":true,"path":"2019/01/14/leetcode_330/","link":"","permalink":"https://lingyixia.github.io/2019/01/14/leetcode_330/","excerpt":"Given a sorted positive integer array nums and an integer n, add/patch elements to the array such that any number in range [1, n] inclusive can be formed by the sum of some elements in the array. Return the minimum number of patches required.","text":"Given a sorted positive integer array nums and an integer n, add/patch elements to the array such that any number in range [1, n] inclusive can be formed by the sum of some elements in the array. Return the minimum number of patches required. 举例:nums = [1, 2, 4, 13, 43],n = 100,当已经将[1,2,4]加入临时数组时,此时有子数组[1],[2],[1,2],[4],[1,4],[2,4],[1,2,4],即连续的1,2,3,4,5,6,7,最大是7,下一个加入临时数组的数字需要将8加进去,但是nums中下一个是13,就算加进去也没有8(中间有裂口),那么要想中间没有裂口且数组和增加最大,下一个加入临时数组的数字是就得是8(贪心),此时子数组最大和是7+8=15,而下一个miss就是16。 实例代码:12345678910111213141516171819int minPatches(vector&lt;int&gt;&amp; nums, int n)&#123; long long miss = 1; int res = 0; int pos = 0; while (miss&lt;=n) &#123; if (pos&lt;nums.size()&amp;&amp;nums[pos]&lt;=miss) &#123; miss += nums[pos++]; &#125; else &#123; miss += miss; res++; &#125; &#125; return res;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"https://lingyixia.github.io/tags/leetcode/"},{"name":"贪心算法","slug":"贪心算法","permalink":"https://lingyixia.github.io/tags/贪心算法/"}]},{"title":"Tampermonkey","slug":"Tampermonkey","date":"2019-01-14T10:25:14.000Z","updated":"2021-09-19T18:01:25.948Z","comments":true,"path":"2019/01/14/Tampermonkey/","link":"","permalink":"https://lingyixia.github.io/2019/01/14/Tampermonkey/","excerpt":"Tampermonkey编程小记","text":"Tampermonkey编程小记eg:(针对知音漫客付费漫画)12345678910111213// ==UserScript==// @name 知音漫客免费// @version 0.1// @description 知音漫客付费漫画免费看// @author 陈飞宇// @match https://www.zymk.cn/*// @require http://code.jquery.com/jquery-1.11.0.min.js// ==/UserScript==$(document).ready(function()&#123; $(document.body).css(&#123; &quot;overflow-y&quot;: &quot;auto&quot; &#125;); $(&quot;#layui-layer1&quot;).remove(); $(&quot;div.layui-layer-shade&quot;).remove();&#125;); 文件头部必须包裹 123// ==UserScript==// ==/UserScript== 要使用Jquery只需加入 1// @require http://code.jquery.com/jquery-1.11.0.min.js","categories":[{"name":"前端","slug":"前端","permalink":"https://lingyixia.github.io/categories/前端/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://lingyixia.github.io/tags/javascript/"},{"name":"jquery","slug":"jquery","permalink":"https://lingyixia.github.io/tags/jquery/"}]},{"title":"进制","slug":"jinzhi","date":"2019-01-14T08:30:32.000Z","updated":"2021-09-19T18:01:25.959Z","comments":true,"path":"2019/01/14/jinzhi/","link":"","permalink":"https://lingyixia.github.io/2019/01/14/jinzhi/","excerpt":"C++输出数字二进制、十进制、八进制、十六进制","text":"C++输出数字二进制、十进制、八进制、十六进制123456789101112131415161718#include&lt;iostream&gt;#include &lt;bitset&gt;using namespace std;int main() &#123; int num = -42; cout &lt;&lt; bitset&lt;sizeof(num) * 8&gt;(num) &lt;&lt; endl; cout &lt;&lt; num &lt;&lt; endl; //默认十进制输出 cout &lt;&lt; hex &lt;&lt; num &lt;&lt; endl; cout &lt;&lt; oct &lt;&lt; num &lt;&lt; endl; return 0;&#125;输出:11111111111111111111111111010110-42ffffffd637777777726","categories":[{"name":"基础","slug":"基础","permalink":"https://lingyixia.github.io/categories/基础/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://lingyixia.github.io/tags/C/"}]},{"title":"Hexo中Latex公式显示","slug":"LatexInHexo","date":"2019-01-14T03:05:00.000Z","updated":"2021-09-19T18:01:25.946Z","comments":true,"path":"2019/01/14/LatexInHexo/","link":"","permalink":"https://lingyixia.github.io/2019/01/14/LatexInHexo/","excerpt":"Hexo中Latex公式显示问题解决","text":"Hexo中Latex公式显示问题解决 卸载: npm uninstall hexo-renderer-marked —save 安装 npm install hexo-renderer-kramed —save 解决冲突node_modules\\kramed\\lib\\rules\\inline.js中:需要转义的字符太多了,直接看这里吧 开关每次写博客头部加上mathjax: trueeg: ---title: index.htmldate: 2018-2-8 21:01:30tags:mathjax: true---","categories":[{"name":"博客","slug":"博客","permalink":"https://lingyixia.github.io/categories/博客/"}],"tags":[{"name":"Latex","slug":"Latex","permalink":"https://lingyixia.github.io/tags/Latex/"},{"name":"Markdown","slug":"Markdown","permalink":"https://lingyixia.github.io/tags/Markdown/"},{"name":"Hexo","slug":"Hexo","permalink":"https://lingyixia.github.io/tags/Hexo/"}]},{"title":"Tools","slug":"Tools","date":"2019-01-12T09:46:47.000Z","updated":"2021-09-19T18:01:25.948Z","comments":true,"path":"2019/01/12/Tools/","link":"","permalink":"https://lingyixia.github.io/2019/01/12/Tools/","excerpt":"本博客用来记录个人电脑常用的一些小工具，虽然看起来很不起眼，但是实际使用起来会有很大的帮助","text":"本博客用来记录个人电脑常用的一些小工具，虽然看起来很不起眼，但是实际使用起来会有很大的帮助 DeskPins: windows上用来将窗口显示在最高层的工具 Cmder: windows上代替cmd自带命令窗口的工具，自带git bash Everything: windows上用来快速查询文件的工具，比windows自带查询要快一万倍","categories":[{"name":"Tip","slug":"Tip","permalink":"https://lingyixia.github.io/categories/Tip/"}],"tags":[]},{"title":"RobinsonCrusoe","slug":"RobinsonCrusoe","date":"2019-01-12T09:42:31.000Z","updated":"2021-09-19T18:01:25.947Z","comments":true,"path":"2019/01/12/RobinsonCrusoe/","link":"","permalink":"https://lingyixia.github.io/2019/01/12/RobinsonCrusoe/","excerpt":"立下flag,今天开始读英文版《鲁滨逊漂流记》,看什么时候能读完,特么的上一本《哈利波特与魔法石》读了多半年。。。。。","text":"立下flag,今天开始读英文版《鲁滨逊漂流记》,看什么时候能读完,特么的上一本《哈利波特与魔法石》读了多半年。。。。。 今天2019年1月15日,读到11页,争取到18号读完第一章。重复单词老忘… …难受,而且完全感受不到作者烘托的氛围,垃圾英语水平. 2019年1月21日记,18日已经读完第一章当时没记录,下面开始第二章，感觉纸质的书真的好难查,生词实在太多，接下来可能用微信读书比较多. 2019年1月21日记下,做个调整,感觉这本书生词还是有点多,而且比较厚,下面先读动物庄园找找感觉. 2019年2月16日记录,动物庄园读完，感觉整体不难，不查字典的话能看明白整体大意，细节需要查单词.句法上难度不大，只有极个别长难句.","categories":[{"name":"读书","slug":"读书","permalink":"https://lingyixia.github.io/categories/读书/"}],"tags":[]},{"title":"C++优先队列","slug":"priority_queue","date":"2019-01-11T11:11:11.000Z","updated":"2021-09-19T18:01:25.963Z","comments":true,"path":"2019/01/11/priority_queue/","link":"","permalink":"https://lingyixia.github.io/2019/01/11/priority_queue/","excerpt":"优先队列是队列和栈结合形成的一种数据结构，c++中使用堆来构建。","text":"优先队列是队列和栈结合形成的一种数据结构，c++中使用堆来构建。首先函数在头文件中，归属于命名空间std，使用的时候需要注意。有两种声明方式:12std::priority_queue&lt;T&gt; pq;std::priority_queue&lt;T, std::vector&lt;T&gt;, cmp&gt; pq; 第一种方式较为简单，当T是基本数据类型的时候使用当T是自定义数据结构的时候(一般是struct)需要使用第二中声明方式.三个参数从左到右第一个指的是优先队列中的数据类型，第二个表示存储堆的数据结构，一般是vector即可，第三个是比较结构，是一个struct，默认是less，即小的在前，但是默认声明方式只针对基本数据结构，自定义的需要重写该结构体，less也可以改为greater。 greater和less是std实现的两个仿函数（就是使一个类的使用看上去像一个函数。其实现就是类中实现一个operator()，这个类就有了类似函数的行为，就是一个仿函数类了） eg:123456789101112131415161718192021222324#include&lt;iostream&gt;#include&lt;queue&gt;using namespace std;struct Node&#123; int val; Node(int val) :val(val) &#123;&#125;&#125;;int main()&#123; priority_queue&lt;Node&gt; A; //大根堆 priority_queue&lt;Node, vector&lt;Node&gt;, greater&lt;Node&gt; &gt; B; //小根堆 A.push(Node(1)); A.push(Node(9)); A.push(Node(4)); cout &lt;&lt; A.top().val &lt;&lt; endl; A.pop(); cout &lt;&lt; A.top().val &lt;&lt; endl; A.pop(); cout &lt;&lt; A.top().val &lt;&lt; endl; A.pop(); return 0;&#125; 这样运行出错，因为priority_queue A; 默认使用的是less，而less的源代码是: 1234567891011struct less&#123; // functor for operator&lt; _CXX17_DEPRECATE_ADAPTOR_TYPEDEFS typedef _Ty first_argument_type; _CXX17_DEPRECATE_ADAPTOR_TYPEDEFS typedef _Ty second_argument_type; _CXX17_DEPRECATE_ADAPTOR_TYPEDEFS typedef bool result_type; constexpr bool operator()(const _Ty&amp;_Left, const _Ty&amp; _Right) const &#123; // apply operator&lt; to operands return (_Left &lt; _Right); &#125;&#125;; return (_Left &lt; _Right)对于一个自定义结构体而言程序不知道&lt;是什么操作，因此需要要么在结构体中重载&lt;符号，要么重写less函数，因此有两种写法。第一种: 12345678910struct Node&#123; int val; Node(int val) :val(val) &#123;&#125; bool operator &lt;(Node a) const &#123; return val &lt; a.val; &#125; bool operator &gt;(Node a) const &#123; return val &gt; a.val; &#125;&#125;;``` 第二种: includeincludeusing namespace std;struct Node{ int val; Node(int val) :val(val) {} };struct cmp{ bool operator()(Node a, Node b) { return a.val &lt; b.val; }};int main(){ priority_queue","categories":[{"name":"基础","slug":"基础","permalink":"https://lingyixia.github.io/categories/基础/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://lingyixia.github.io/tags/C/"}]},{"title":"gradientDescent","slug":"gradientDescent","date":"2019-01-11T08:01:48.000Z","updated":"2021-09-19T18:01:25.958Z","comments":true,"path":"2019/01/11/gradientDescent/","link":"","permalink":"https://lingyixia.github.io/2019/01/11/gradientDescent/","excerpt":"梯度下降一般常提到的有三种方式:批量梯度下降法BGD、随机梯度下降法SGD和小批量梯度下降法MBGD,下面依次介绍.","text":"梯度下降一般常提到的有三种方式:批量梯度下降法BGD、随机梯度下降法SGD和小批量梯度下降法MBGD,下面依次介绍. 假设有$m$个训练数据,则参数$\\theta$更新公式为: \\theta_j=\\theta_j-\\alpha \\times loss批量梯度下降(Batch Gradient Descent,BGD)参数更新公式为: \\theta_{j+1}=\\theta_j-\\frac{1}{m}\\sum_{i=1}^m loss_i \\tag{1}伪代码为:1234repeat&#123; 公式(1)&#125; 即每次迭代都将$m$个训练数据全部计算一遍,用以更新参数. 优点:1.一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行2.由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优. 缺点:速度慢从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下: 随机梯度下降(Stochastic Gradient Descent,SGD)参数更新公式为: \\theta_j=\\theta_j + loss^i \\tag{2}伪代码为:12345678Random shuffle dataset;repeat&#123; for i=1,2,3...m &#123; 公式(2) &#125;&#125; 也就是不是同时使用所有训练数据,而是逐个使用每个训练数据。 优点:1.由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。 缺点:1.准确度下降。由于即使在目标函数为强凸函数的情况下,SGD仍旧无法做到线性收敛。2.可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势3.不易于并行实现。 从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下: 小批量梯度下降(Mini-Batch Gradient Descent, MBGD)思想为综合BGD和SGD两种方式取中,伪代码为: \\theta_{j+1}=\\theta_j-\\sum_{i=0}^{m-batchsize}\\frac{1}{batchsize}\\sum_i^{i+batchsize} loss_i \\tag{3}123456repeat&#123; for i=0,11,21,...991&#123; 公式(3) &#125;&#125; 上诉代码中batchsize为10,即每10个算一个并行更新一次优点:1.通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。2.每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次)3.可实现并行化.缺点:1.batch_size的不当选择可能会带来一些问题 三者对比图 参考文章","categories":[{"name":"神经网络","slug":"神经网络","permalink":"https://lingyixia.github.io/categories/神经网络/"}],"tags":[{"name":"梯度","slug":"梯度","permalink":"https://lingyixia.github.io/tags/梯度/"}]},{"title":"word2vec","slug":"word2vec","date":"2019-01-10T10:00:04.000Z","updated":"2021-09-19T18:01:25.967Z","comments":true,"path":"2019/01/10/word2vec/","link":"","permalink":"https://lingyixia.github.io/2019/01/10/word2vec/","excerpt":"本文不作为详细教程，知识自己容易遗忘知识点的记录。","text":"本文不作为详细教程，知识自己容易遗忘知识点的记录。 语言模型通俗来说就是用来量化哪个句子更像人话的模型,目前可分为统计语言模型和神经网络模型。 统计语言模型由Bayes公式,一个句子组成的概率公式为: p(w_k|w_1^{k-1})=\\frac{p(w_1^k)}{p(w_1^{k-1})} \\tag{1}根据大数定理: p(w_k|w_1^{k-1})≈\\frac{count(w_1^k)}{count(w_1^{k-1})} \\tag{2}由马尔科夫假设:任意一个词出现的概率只是他前面出现的有限的一个或者几个词相关(未来的事件，只取决于有限的历史)基于马尔可夫假设，N-gram 语言模型认为一个词出现的概率只与它前面的n-1个词相关,即: p(w_k|w_{k-n+1}^{k-1})=\\frac{p(w_{k-n+1}^k)}{w_{k-n+1}^{k-1}}≈\\frac{count(w_{k-n+1}^k)}{count(w_{k-n+1}^{k-1})} \\tag{3}eg:n=2 p(w_k|w_{k-1})≈\\frac{count(w_{k-1}^{k})}{count(w_{k-1})} \\tag{4}一般而言,语言模型利用最大似然确定目标函数(即最大化某句子的概率): Loss=\\sum_w \\log p(w|context(w)) \\tag{5}其中: p(w|context(w))=F(w,context(w),\\theta) \\tag{6}比如早期的智能ABC据说就是用的N-gram,还有搜索引擎,输入一个词后面多个选项也是N-gram. 神经网络语言模型神经概率语言模型(NPLM)该结构是词向量模型的先驱网络结构: Input Layer是n-1个词的词向量首尾相连,得到$x_w$,长度为:$(n-1)m$,$m$是词向量长度. Projectjon Layer是:$z_w=tanh(Wx_w+p)$,长度依然为$(n-1)m$. Hjdden Layer是$y_w=Uz_w+q$,得到的$y_w=(y_1,y_2…y_N)$,N是词表大小. Output Layer是$softmax$,即将$y_w$的各个分量做$softmax$，使其加和为1. 由式(6)$F(w,context(w),\\theta)$,此时$\\theta$包含: 词向量: $v_w\\in R^m$ 神经网络参数: $W \\in R^{n_h \\times (n-1)m},p \\in R^{n_h},Y \\in R^{n_h},q \\in R^{N}$其中$n_h$是batchsjze,N是词典大小。 词向量模型基于Hierarchical SoftMaxCBOW由上下文词推断当前词的词向量模型 Input Layer: $2c$个词的随机初始化词向量 Projectjon Layer是:$x_w=\\sum_{j=1}^{2c}v(context(w)_j)$ Output Layer:是一颗由训练语料构成的Huffman树 与NPML相比,他们的$x_w$不同,NPML是收尾相接,而$CBOW$是向量加和.他们的输出层不同,NPML是输出层是线性结构,$CBOW$输出层是$Huffman$树.损失函数计算:引入一些符号: $p^w$:从根节点到达$w$节点的路径 $l^w$:路径$p^w$中节点的个数 $p^w1…p^w{l^w}$:依次代表路径中的节点,根节点-中间节点-叶子节点 $d2^w…d^w{l^w} \\in {0,1}$:词$w$的$Huffman$编码,由$l^w-1$位构成,根节点无需编码 $\\theta1^w…\\theta^w{l^w-1}$: 路径中非叶子节点对应的向量,用于辅助计算。 单词$w$是足球,对应的上下文词汇为$c_w$,上下文词向量和为$x_w$举例说明: 约定编码1为负类,0为正类,每一个节点就是一个二分类器，是逻辑回归(sjgmojd)。其中$\\theta$是对应的非叶子节点的向量，一个节点被分为正类和负类的概率分别如下: \\sigma(x^T_w\\theta)=\\frac{1}{1+e^{x^T_w\\theta}},1-\\sigma(x^T_w\\theta)那么从根节点到足球的概率为: 第一次: $p(d_2^w|x_w,\\theta_1^w)=1-\\sigma(x_w^T\\theta_1^w)$ 第二次: $p(d_3^w|x_w,\\theta_2^w)=\\sigma(x_w^T\\theta_2^w)$ 第三次: $p(d_4^w|x_w,\\theta_3^w)=\\sigma(x_w^T\\theta_3^w)$ 第四次: $p(d_5^w|x_w,\\theta_4^w)=1-\\sigma(x_w^T\\theta_4^w)$ p(足球|c_{足球})=\\prod_{j=2}^5p(d_j^w|x_w,\\theta_{j-1}^w)该公式即为目标函数,重新整理: p(w|context(w))=\\prod_{j=2}^{l^w}p(d_j^w|x_w,\\theta_{j-1}^w)其中 p(d_j^w|x_w,\\theta_{j-1}^w) = \\begin{cases} \\sigma(x_w^T\\theta_{j-1}^w) & d_j^w=0\\\\ 1-\\sigma(x_w^T\\theta_{j-1}^w) & d_j^w=1\\\\ \\end{cases}写成整体形式为: p(d_j^w|x_w,\\theta_{-1}^w)=[\\sigma(x_w^T\\theta_{j-1}^w)]^{1-d_j^w}·[1-\\sigma(x_w^T\\theta_{j-1}^w)]^{d_j^w}最大似然损失函数 \\begin{align} L &= \\sum_{w \\in C} \\log \\prod_{j=2}^{l^w}\\{[\\sigma(x_w^T\\theta_{j-1}^w)]^{1-d_j^w}·[1-\\sigma(x_w^T\\theta_{j-1}^w)]^{d_j^w}\\}\\\\ &= \\sum_{w \\in C}\\sum_{j=2}^{l^w}\\{(1-d_j^w)·log[\\sigma(x_w^T\\theta_{j-1}^w)]+d_j^w·log[1-\\theta(x_w^T\\theta_{j-1}^w)]\\} \\end{align}令$L(w,j)=\\sum{w \\in C}\\sum{j=2}^{l^w}{(1-dj^w)·log[\\sigma(x_w^T\\theta{j-1}^w)]+dj^w·log[1-\\theta(x_w^T\\theta{j-1}^w)]}$则 L=\\sum_{w \\in C}L(w,j)对于具体求导见参考1和参考2Skip-Gram由上下文词推断当前词的词向量模型 Input Layer:中心词w的词向量$v_w$ Projectjon Layer:其实在这里是多余的，只是为了和CBOW做对比 Output Layer:是一颗由训练语料构成的Huffman树由于需要预测中心词左右共2c个词，因此每次预测都需要走2c遍该Huffman树(CBOW)只需要走一遍，因此条件概率应为: p(Context(w)|w)=\\prod_{u \\in Context(w)} p(u|w)而 p(u|w)=\\prod_{j=2}^{l^u}p(d_j^u|v_w,\\theta_{j-1}^u)上诉公式仿照CBOW即可 p(d_j^u|v_w,\\theta_{-1}^u)=[\\sigma(v_w^T\\theta_{j-1}^u)]^{1-d_j^u}·[1-\\sigma(v_w^T\\theta_{j-1}^u)]^{d_j^u}根据最大似然估计得: \\begin{align} L &=\\sum_{w \\in c} log \\prod_{u \\in Context(w)} \\prod_{j=2}^{l^u}\\{[\\sigma(v_w^T\\theta_{j-1}^u)]^{1-d_j^u}·[1-\\sigma(v_w)^T\\theta_{j-1}^u]\\} \\\\ &= \\sum_{w \\in C} \\sum_{u \\in Context(w)} \\sum_{j=2}^{l^u}\\{(1-d_j^u)·log[\\sigma(v_w)^T\\theta_{j-1}^u]+d_j^u·log[1-\\sigma(v_w)^T\\theta_{j-1}^u]\\} \\end{align}令 L(w,u,j)=(1-d_j^u)·log[\\sigma(v_w)^T\\theta_{j-1}^u]+d_j^u·log[1-\\sigma(v_w)^T\\theta_{j-1}^u]求梯度同上基于Negative Sampling 个人想法对于cbow的hierarchical softmax伪代码: Negative Sampling伪代码: 我觉得大多数博客说的hierarchical softmax一次更新所有参数的说法简直误人子弟,看看伪代码即可,hierarchical softmax一次性更新的权重是该路径上所有非叶子节点的权值,因此如果huffman数很大,则更新的权值也就非常多,而Negative Sampling的方式只更新正样本对应的一个权值和负样本对应各自的权值,是固定个数的,所以它更新的权值少。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"词向量","slug":"词向量","permalink":"https://lingyixia.github.io/tags/词向量/"}]},{"title":"矩阵可逆充要条件","slug":"inverse","date":"2019-01-10T07:38:41.000Z","updated":"2021-09-19T18:01:25.959Z","comments":true,"path":"2019/01/10/inverse/","link":"","permalink":"https://lingyixia.github.io/2019/01/10/inverse/","excerpt":"方阵可逆充要条件(可逆矩阵=非奇异矩阵)","text":"方阵可逆充要条件(可逆矩阵=非奇异矩阵) 行列式不为0 $Ax=0$只有0解 满秩 特征值全不为0 行/列向量组均线性无关","categories":[{"name":"数学","slug":"数学","permalink":"https://lingyixia.github.io/categories/数学/"}],"tags":[{"name":"矩阵","slug":"矩阵","permalink":"https://lingyixia.github.io/tags/矩阵/"}]},{"title":"不同特征值的特征向量线性无关","slug":"characteristic-value","date":"2019-01-10T06:18:16.000Z","updated":"2021-09-19T18:01:25.957Z","comments":true,"path":"2019/01/10/characteristic-value/","link":"","permalink":"https://lingyixia.github.io/2019/01/10/characteristic-value/","excerpt":"证明n阶方针不同特征值的特征向量线性无关","text":"证明n阶方针不同特征值的特征向量线性无关 命题假设$n$阶方阵$A$,有$s$个不同特征值$\\lambda_1,\\lambda_2,\\lambda_3…\\lambda_s$,对应于$s$个特征向量$\\alpha_1,\\alpha_2,\\alpha_3…\\alpha_s$,试证明这$s$个向量线性无关. 证明假设$\\lambda_1&lt;\\lambda_2&lt;\\lambda_3…\\lambda_s$,若线行无关则必然存在:k_1\\alpha_1+k_2\\alpha_2+k_3\\alpha_3+...+k_s\\alpha_s=0 \\tag{1}其中$k_1,k_2,k_3…k_s$不全为0.左乘$A$得: k_1A\\alpha_1+k_2A\\alpha_2+k_3A\\alpha_3+...+k_sA\\alpha_s=0 \\tag{2}即: k_1\\lambda_1\\alpha_1+k_2\\lambda_2\\alpha_2+k_3\\lambda_3\\alpha_3hexo+...+k_s\\lambda_s\\alpha_s=0 \\tag{3}由(1)得: k_1\\alpha_1=-(k_2\\alpha_2+k_3\\alpha_3+...+k_s\\alpha_s)\\tag{4}带入(3)中得: k_2\\alpha_2(\\lambda_2-\\lambda_1)+k_3\\alpha_3(\\lambda_3-\\lambda_1)+...+k_s\\alpha_s(\\lambda_s-\\lambda_1)=0 \\tag{5}由于:$\\lambda_1&lt;\\lambda_2&lt;\\lambda_3…\\lambda_s$且其中$k_1,k_2,k_3…k_s$不全为0.$\\alpha_1,\\alpha_2,\\alpha_3…\\alpha_s$为非0向量,故(5)式不可能成立,由反证法得(1)式也不可能成立.","categories":[{"name":"数学","slug":"数学","permalink":"https://lingyixia.github.io/categories/数学/"}],"tags":[{"name":"矩阵","slug":"矩阵","permalink":"https://lingyixia.github.io/tags/矩阵/"}]},{"title":"两种交叉熵损失函数对比","slug":"TwoCrossEntropy","date":"2019-01-08T14:00:12.000Z","updated":"2021-09-19T18:01:25.949Z","comments":true,"path":"2019/01/08/TwoCrossEntropy/","link":"","permalink":"https://lingyixia.github.io/2019/01/08/TwoCrossEntropy/","excerpt":"两种交叉熵损失函数对比","text":"两种交叉熵损失函数对比 第一种 C=\\sum_{i=0}^N p(x)\\log q(x)该形式交叉熵损失函数对应神经网络的输出为$softmax$,即这N个$p(x)$加和为1. 第二种 C=\\sum_{i=0}^N p(x)\\log q(x)+(1-p(x))\\log(1-q(x))该形式交叉熵损失函数对应神经网络的输出为$sigmod$,即这N个$p(x)$加和不是1.","categories":[],"tags":[{"name":"熵","slug":"熵","permalink":"https://lingyixia.github.io/tags/熵/"}]},{"title":"VimLearn","slug":"VimLearn","date":"2019-01-02T11:29:51.000Z","updated":"2021-09-19T18:01:25.949Z","comments":true,"path":"2019/01/02/VimLearn/","link":"","permalink":"https://lingyixia.github.io/2019/01/02/VimLearn/","excerpt":"试过那么多次,这次一定要把vim用熟!!!!!2019年1月19日,记第一次爱上Vim(Visual Studio中的viemu)","text":"试过那么多次,这次一定要把vim用熟!!!!!2019年1月19日,记第一次爱上Vim(Visual Studio中的viemu) 六种基本模式 基本模式(6):普通模式、插入模式、可视模式、选择模式、命令行模式、Ex模式。派生模式(5):操作符等待模式、插入普通模式、插入可视模式、插入选择模式、替换模式 模式名称 英文名 描述 命令模式 Command-Mode 用于输入指令,如:保存、运行、切换标签、切割屏幕等 插入模式 Insert-Mode 也即编辑模式,用于编辑文本,其他编辑器大多默认该模式,大多数新用户希望文本编辑器编辑过程中一直保持这个模式 可视模式 Visual-Mode 相当于高亮选取文本后的普通模式 普通模式 Normal-Mode 用于查看文本，也可复制、粘贴、撤销、重做等 其它模式-&gt;正常模式Esc 正常模式-&gt;插入模式a\\A,s\\S,o\\O,i\\I...... 正常模式-&gt;命令模式: 正常模式-&gt;可视模式 指令 描述 v 可视模式 Ctrl+v 可视块模式 Shift+v 可视行模式 常用功能多行注释1.Ctrl+v进入可视模式2.选择行3.I4.输入注释符(“//“或”#”等等)5.Esc以上步骤不可缺(尤其最后一步)!!!! 删除多行注释1.Ctrl+v进入可视模式2.选定要取消注释的多行(只选择注释符列)3.按下”x”或者”d”. 删除、复制、剪切1.normal模式下,yy复制整行,nyy为复制n行,10,15y是复制10到15行,10,15 move 20把10到15行移动到20行后面2.dd剪切整行,ndd为剪切n行,10,15y是剪切10到15行,10,15d删除10到15行,10,15 move 20把10到15行复制到20行后面3.vim中d表示删除,实际上是剪切,要想真正删除使用_d即可。 查找在normal模式下按下/即可进入查找模式，输入要查找的字符串并按下回车。 Vim会跳转到第一个匹配。按下n查找下一个，按下N查找上一个。1.\\c表示大小写不敏感查找,\\C表示大小写敏感查找。例如:/foo\\c2.查找当前单词:normal模式下按下*即可查找光标所在单词(word),要求每次出现的前后为空白字符或标点符号。例如当前为foo,可以匹配foo bar中的foo,但不可匹配foobar中的foo。 这在查找函数名、变量名时非常有用。 按下g*即可查找光标所在单词的字符序列，每次出现前后字符无要求。 即foo bar和foobar中的foo均可被匹配到。3.查找并替换:s(substitute)命令用来查找和替换字符串。语法如下:{作用范围}s/{目标}/{替换}/{替换标志}例如:%s/foo/bar/g会在全局范围(%)查找foo并替换为bar,所有出现都会被替换(g)4.作用范围:作用范围分为当前行、全文、选区等等。当前行:s/foo/bar/g全文:%s/foo/bar/g选区,在Visual模式下选择区域后输入: Vim即可自动补全为 :&#39;&lt;,&#39;&gt;。:&#39;&lt;,&#39;&gt;s/foo/bar/g2-11行:5,12s/foo/bar/g当前行.与接下来两行+2:.,+2s/foo/bar/g5.替换标志:i表示大小写不敏感查找,I表示大小写敏感:%s/foo/bar/i(等效于\\C和\\c).c表示需要确认:%s/foo/bar/gc参考这里 其他1.普通模式下,I是行首,A是行尾,a是从光标后插入,i是光标前插入,O从该行上一行新增一行,o从该行下一行新增一行2.d剪切,y复制,p粘贴3.全选:普通模式下:ggVG4.全部复制:普通模式下:ggyG5.全部删除:普通模式下:dG解释:gg是让光标移到首行，在vim才有效，vi中无效v:是进入Visual(可视）模式G:光标移到最后一行选中内容以后就可以其他的操作了，比如d删除选中内容y复制选中内容到0号寄存器6.字符串次数统计:普通模式下:%s/需要统计的字符串(无需引号)//gn7.代码移动:普通模式下:60,70 &gt; &lt;四个空格&gt;表示60到70行右移4个空格","categories":[{"name":"基础","slug":"基础","permalink":"https://lingyixia.github.io/categories/基础/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://lingyixia.github.io/tags/vim/"}]},{"title":"convex","slug":"convex","date":"2018-12-27T12:20:55.000Z","updated":"2021-09-19T18:01:25.957Z","comments":true,"path":"2018/12/27/convex/","link":"","permalink":"https://lingyixia.github.io/2018/12/27/convex/","excerpt":"There are some trees, where each tree is represented by (x,y) coordinate in a two-dimensional garden. Your job is to fence the entire garden using the minimum length of rope as it is expensive. The garden is well fenced only if all the trees are enclosed. Your task is to help find the coordinates of trees which are exactly located on the fence perimeter.","text":"There are some trees, where each tree is represented by (x,y) coordinate in a two-dimensional garden. Your job is to fence the entire garden using the minimum length of rope as it is expensive. The garden is well fenced only if all the trees are enclosed. Your task is to help find the coordinates of trees which are exactly located on the fence perimeter. Graham扫描法步骤: 将所有的点排序,取$y$轴最小的一个点记为$P_0$(如果多个则取$x$最小的点) 平移所有的点,使$P_0$为原点 计算各个点相对于$P_0$的夹角$α$,按从小到大的顺序对各个点排序。当$α$相同时，距离$P_0$比较近的排在前面。例如图中得到的结果为$P_1,P_2,P_3,P_4,P_5,P_6,P_7,P_8$(此处排序是个关键点)。我们由几何知识可以知道，结果中第一个点$P_1$和最后一个点$P_8$一定是凸包上的点。然后将$P_0$和$P_1$依次入栈。 栈顶出栈,即$P_1$然后判断下一个点$P_2$在栈顶点$P_0$和刚刚出栈的点$P_1$组成的直线的哪边，如果在左边或者同一直线上则$P_1$再次入栈,$P_2$跟随入栈,如果在右边则$P_1$扔掉,扔掉，重复步骤4,直到$P_2$在该直线的左边或同一直线此处判断是个关键点。 直到最后一个$P_8$为止:代码如下(需要简化,以后在说):123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;stack&gt;#include&lt;vector&gt;using namespace std;struct Point&#123; int x, y; Point() :x(0), y(0) &#123;&#125; Point(int a, int b) :x(a), y(b) &#123;&#125;&#125;;Point operator -(const Point&amp; pointA, const Point&amp; pointB)&#123; Point temp; temp.x = pointA.x - pointB.x; temp.y = pointA.y - pointB.y; return temp;&#125;Point operator +(const Point&amp; pointA, const Point&amp; pointB)&#123; Point temp; temp.x = pointA.x + pointB.x; temp.y = pointA.y + pointB.y; return temp;&#125;bool compareLess(const Point&amp; A, const Point&amp; B)&#123; if (A.y == B.y) &#123; return A.x &lt; B.x; &#125; return A.y &lt; B.y;&#125;bool compareAngle(const Point&amp; A, const Point&amp; B)&#123; float a = A.x / sqrt(A.x*A.x + A.y* A.y); float b = B.x / sqrt(B.x*B.x + B.y * B.y); if (a == b) &#123; if (A.x == B.x) &#123; return A.y &gt; B.y; &#125; return A.x &lt; B.x; &#125; return a &gt; b;&#125;vector&lt;Point&gt; outerTrees(vector&lt;Point&gt;&amp; points);int calcSquare(Point&amp; A, Point&amp; B, Point&amp; C);int main()&#123; vector&lt;Point&gt; points = &#123; Point(0, 2), Point(0, 4), Point(0, 5), Point(0, 9), Point(2, 1), Point(2, 2), Point(2, 3), Point(2, 5), Point(3, 1), Point(3, 2), Point(3, 6), Point(3, 9), Point(4, 2), Point(4, 5), Point(5, 8), Point(5, 9), Point(6, 3), Point(7, 9), Point(8, 1), Point(8, 2), Point(8, 5), Point(8, 7), Point(9, 0), Point(9, 1), Point(9, 6) &#125;; vector&lt;Point&gt; p = outerTrees(points); return 0;&#125;vector&lt;Point&gt; outerTrees(vector&lt;Point&gt;&amp; points)&#123; if (points.size() &lt; 4) &#123; return points; &#125; sort(points.begin(), points.end(), compareLess); Point zero = points.front(); for (vector&lt;Point&gt;::iterator iter = points.begin(); iter != points.end(); iter++) &#123; *iter = *iter - zero; &#125; sort(points.begin() + 1, points.end(), compareAngle); vector&lt;Point&gt;::iterator iter; for (iter = points.begin(); iter-&gt;x == 0; iter++); sort(points.begin(), iter, compareLess); stack&lt;Point&gt; outer; vector&lt;Point&gt; result; outer.push(points.front()); outer.push(*(points.begin() + 1)); for (vector&lt;Point&gt;::iterator iter = points.begin() + 2; iter != points.end(); iter++) &#123; Point tempTop = outer.top(); outer.pop(); while (calcSquare(outer.top(), tempTop, *iter) &lt; 0) &#123; tempTop = outer.top(); outer.pop(); &#125; outer.push(tempTop); outer.push(*iter); &#125; while (!outer.empty()) &#123; result.push_back(outer.top() + zero); outer.pop(); &#125; return result;&#125;int calcSquare(Point&amp; A, Point&amp; B, Point&amp; C)&#123; return A.x * B.y + A.y*C.x + B.x*C.y - B.y*C.x - A.y*B.x - A.x*C.y;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://lingyixia.github.io/tags/C/"},{"name":"leetcode","slug":"leetcode","permalink":"https://lingyixia.github.io/tags/leetcode/"}]},{"title":"各种熵","slug":"entropy","date":"2018-12-26T05:11:08.000Z","updated":"2021-09-19T18:01:25.958Z","comments":true,"path":"2018/12/26/entropy/","link":"","permalink":"https://lingyixia.github.io/2018/12/26/entropy/","excerpt":"熵(信息熵)、交叉熵和相对熵(KL散度)、条件熵","text":"熵(信息熵)、交叉熵和相对熵(KL散度)、条件熵 熵(信息熵) 信息熵有两个含义:1.系统包含的信息量的期望 2.定量描述该系统所需的编码长度的期望 公式推导定性推导:设$h(x)$为$x$包含的信息量,如果我们有俩个不相关的事件$x$和$y$,那么我们观察到的俩个事件同时发生时获得的信息量应该等于观察到的事件各自发生时获得的信息量之和,即:$h(x,y) = h(x) + h(y)$,由于$x$,$y$是俩个不相关的事件，则满足$p(x,y) = p(x) \\times p(y)$.那么要想让$h(x,y) = h(x) + h(y)$,$h(x)$就只能是$\\log p(x)$,为了让信息量为非负,我们在其前面加负号，得到信息量公式: h(x) = -\\log p(x)定量推导:参考该博客这个很牛逼！！！！！开始推导：首先说明信息量的定义，谨记x是个概率值，不是事件: 信息量是概率的递减函数，记为$f(x)$,$x\\in[0,1]$ $f(1)=0,f(0)=+∞$ 独立事件(概率之积)的信息量等于各自信息量之和:$f (x_1 \\times x_2)=f(x_1)+f(x_2),x_1,x_2\\in[0,1]$ \\begin{align} f(x)^\\prime &= \\lim_{\\Delta x \\to 0}\\frac{f(x+\\Delta x)-f(x)}{\\Delta x} \\\\ &= \\lim_{\\Delta x \\to 0}\\frac{f(\\frac{x+\\Delta x}{x} \\times x)-f(x)}{\\Delta x} \\\\ &= \\lim_{\\Delta x \\to 0}\\frac{f(x+\\Delta x)+f(x)-f(x)}{\\Delta x} \\\\ &=\\lim_{\\Delta x \\to 0}\\frac{f(\\frac{x+\\Delta x}{x})}{\\Delta x} \\\\ &= \\frac{1}{x}\\lim_{\\Delta x \\to 0}\\frac{f(1+\\frac{\\Delta x}{x})}{\\frac{\\Delta x}{x}} \\\\ &=\\frac{1}{x}f(1)^\\prime \\end{align}积分得: f(x) = f(1)^\\prime\\ln|x|+C \\qquad x\\in[0,1]令$x=1$,得$C=0$,故 \\begin{align} f(x) &= f(1)^\\prime\\ln x \\\\ &=f(1)^\\prime\\frac{\\log_ax}{\\log_ae} \\\\ &= \\frac{f(1)^\\prime}{\\log_ae} ] \\times \\log_ax \\end{align}而$\\frac{f(1)^\\prime}{\\log_ae}$是个常数，令为1，则 \\begin{align} f(x) &= \\log_ax \\\\ &= -\\log_a\\frac{1}{x} \\end{align}证毕!!! 编码推导:参考此处首先需要知道的是Karft不等式:$\\sum r^{-l_i}\\le1$,其中$r$是进制数，一般取二进制,$l_i$表示第$i$个信息的码长.问题可以转化为: \\min_{l_i}\\sum p_il_i \\\\ s.t.\\quad \\sum r^{-l_i}\\le1由拉格朗日乘数法: L(l_i,\\lambda)=\\sum p_il_i+\\lambda(\\sum r^{-l_i}-1)根据拉格朗日法则求极值得: \\begin{cases} p_i-\\lambda \\times r^{-l_i}\\ln r =0 \\\\ \\sum r^{-l_i}-1=0 \\\\ \\end{cases}由上面公式得: \\begin{cases} r^{-l_i}= \\frac{p_i}{\\lambda \\times \\ln r} \\\\ \\sum r^{-l_i}=1 \\\\ \\end{cases}一式带入二式得:$\\sum r^{-l_i} = \\sum \\frac{p_i}{\\lambda \\times \\ln r} = \\frac{1}{\\lambda \\times \\ln r} = 1$ 因此:$\\lambda = \\frac{1}{\\ln r}$ 最终得:$l_i = \\log r \\frac{\\lambda \\times \\ln r}{p_i}=\\log r\\frac{1}{p_i}$，正好是熵!!! 则公式信息熵: H(x) = -p(x)\\log p(x)自然是系统信息量的期望，或称为编码长度的期望. 交叉熵和相对熵(KL散度)现在有两个分布，真实分布p和非真实分布q，我们的样本来自真实分布p,按照真实分布p来编码样本所需的编码长度的期望为(信息熵): H(p) = \\sum -p\\log p按照不真实分布q来编码样本所需的编码长度的期望为(交叉熵): H(p,q)= \\sum -p\\log q注意:$H(p,q)≠H(q,p)!!!$而KL散度(相对熵)为: H(p||q)=H(p,q)-H(p)它表示两个分布的差异，差异越大，相对熵越大。 条件熵条件熵针对得是某个特征范围内来计算，而不是整个样本，即熵是整个样本的不确定性，而条件熵是特定特征条件下样本的不确定性。案例见此,公式表达为: \\begin{align} H(Y|X) &=\\sum_{x\\in X}p(X)H(Y|X=x) \\\\ &=\\sum _{x,y}-p(x,y)\\log p(y|x) \\end{align}X表示总体样本的某个特征，条件熵和其他熵最大的不同就是条件熵和特征有关，其他熵只和Label有关，和特征无关。 联合熵表示X和Y是同一个分布中的两个特征，没有特征和Label之分. H(X,Y) = \\sum_{x,y}-p(x,y)\\log p(x,y)联合熵和条件熵的关系: \\begin{align} H(X,Y) - H(Y|X) &= -\\sum_{x,y}p(x,y)\\log p(x,y) + \\sum_{x,y}p(x,y)\\log p(y|x) \\\\ &= -\\sum_{x,y}p(x,y)\\log p(x) \\\\ &=-\\sum_xp(x)\\log p(x) \\\\ &=H(X) \\end{align}注意，要和条件熵的符号区分。 互信息(信息增益) \\begin{align} I(X,Y) &= H(X)-H(X|Y) \\\\ &= H(Y)-H(Y|X) \\\\ &= H(X)+H(Y) - H(X,Y) \\\\ &= H(X,Y) - H(X|Y) -H(Y|X) \\end{align}","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[{"name":"熵","slug":"熵","permalink":"https://lingyixia.github.io/tags/熵/"}]},{"title":"tensorflowFunctions","slug":"tensorflowFunctions","date":"2018-12-25T10:17:37.000Z","updated":"2021-09-19T18:01:25.966Z","comments":true,"path":"2018/12/25/tensorflowFunctions/","link":"","permalink":"https://lingyixia.github.io/2018/12/25/tensorflowFunctions/","excerpt":"tensorflow中常用函数小记(先暂记,以后整理)","text":"tensorflow中常用函数小记(先暂记,以后整理) tf.tile()用于向量扩张，参数说明:1234567891011121314151617181920212223242526272829tile(input,multiples,name=None)input: 一个tensormultiples: 一个tensor,维度的数量和input维度的数量必然相同eg:import tensorflow as tfraw = tf.Variable(tf.random_normal(shape=(1, 3, 2)))multi = tf.tile(raw, multiples=[2, 1, 3])with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(raw.eval()) print(&apos;-----------------------------&apos;) print(sess.run(multi))输出:[[[-1.19627476 -0.66122055] [ 1.45084798 -0.87026799] [ 0.60792369 0.39918834]]]-----------------------------[[[-1.19627476 -0.66122055 -1.19627476 -0.66122055 -1.19627476 -0.66122055] [ 1.45084798 -0.87026799 1.45084798 -0.87026799 1.45084798 -0.87026799] [ 0.60792369 0.39918834 0.60792369 0.39918834 0.60792369 0.39918834]] [[-1.19627476 -0.66122055 -1.19627476 -0.66122055 -1.19627476 -0.66122055] [ 1.45084798 -0.87026799 1.45084798 -0.87026799 1.45084798 -0.87026799] [ 0.60792369 0.39918834 0.60792369 0.39918834 0.60792369 0.39918834]]]解释: multiples各个维度表示对input相应维度重复多少倍,如该实例中表示对input的第0个维度重复2次，第1个维度重复1次,第2个维度重复3次，因此得到的shape必然是(2,3,6) 拼接tf.concat 原型:tf.concat(values, axis, name=&#39;concat&#39;):按照指定的已经存在的轴进行拼接 eg:略 tf.stack() 原型:tf.stack(values, axis=0, name=&#39;&#39;):按照指定的新建的轴进行拼接,用于连接tensor，与concat不同的是stack增加维度数量,concat不增加维度数量。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374eg 1:import tensorflow as tfa = tf.constant([[1,1,1],[2,2,2],[3,3,3],[4,4,4]]) # shape (4,3)b = tf.constant([[5,5,5],[6,6,6],[7,7,7],[8,8,8]]) # shape (4,3)ab = tf.stack([a,b], axis=0)with tf.Session() as sess: print(ab.shape) print(sess.run(ab))输出:(2, 4, 3)[[[1 1 1] [2 2 2] [3 3 3] [4 4 4]] [[5 5 5] [6 6 6] [7 7 7] [8 8 8]]]eg 2:import tensorflow as tfa = tf.constant([[1,1,1],[2,2,2],[3,3,3],[4,4,4]]) # shape (4,3)b = tf.constant([[5,5,5],[6,6,6],[7,7,7],[8,8,8]]) # shape (4,3)ab = tf.stack([a,b], axis=1)with tf.Session() as sess: print(ab.shape) print(sess.run(ab))输出:(4, 2, 3)[[[1 1 1] [5 5 5]] [[2 2 2] [6 6 6]] [[3 3 3] [7 7 7]] [[4 4 4] [8 8 8]]]eg 3:import tensorflow as tfa = tf.constant([[1,1,1],[2,2,2],[3,3,3],[4,4,4]]) # shape (4,3)b = tf.constant([[5,5,5],[6,6,6],[7,7,7],[8,8,8]]) # shape (4,3)ab = tf.stack([a,b], axis=2)with tf.Session() as sess: print(ab.shape) print(sess.run(ab))输出:(4, 3, 2)[[[1 5] [1 5] [1 5]] [[2 6] [2 6] [2 6]] [[3 7] [3 7] [3 7]] [[4 8] [4 8] [4 8]]] 也就是说把两个tensor相应维度合并成一个新维度，如eg2,把第1维度的[1,1,1]和[5,5,5]合并为[[1,1,1],[5,5,5]],同理tf.unstack().与conact不同的一点是conact中tensor维度有多少axis就最大是多少,stack()的axis可以大1个维度。 tf.scatter_nd()给全为0的tensor插入数据，参数说明:123indices: shape的坐标updates: 实际数据，根据indices中的坐标插入shape中shape: 一个该shape的全为0的tensor eg:(简单例子直接百度)1234567891011121314151617181920212223242526272829import tensorflow as tfindices = tf.constant([[0, 1], [2, 3]])updates = tf.constant([[5, 5, 5, 5], [8, 8, 8, 8]])shape = tf.constant([4, 4, 4])scatter = tf.scatter_nd(indices, updates, shape)with tf.Session() as sess: print(sess.run(scatter))输出:[[[0 0 0 0] [5 5 5 5] [0 0 0 0] [0 0 0 0]] [[0 0 0 0] [0 0 0 0] [0 0 0 0] [0 0 0 0]] [[0 0 0 0] [0 0 0 0] [0 0 0 0] [8 8 8 8]] [[0 0 0 0] [0 0 0 0] [0 0 0 0] [0 0 0 0]]] 解释: indices包含两个:[0,1]和[2,3],分别表示updates中[5, 5, 5, 5]和[8, 8, 8, 8]在shape为(4,4,4)的全为0的tensor中的位置,即[5,5,5,5]的位置是(0,1,),[8,8,8,8]的位置是(2,3,) 判断不合法值 nan:not a number,inf:infinity只有这两个!!!其他的:np.NAN和np.NaN就是nan,np.NINF就是-inf np.isfinite 判断是否是nan或inf eg:1234567import numpy as npprint(np.isfinite([np.inf, np.log(0), 1, np.nan]))输出:/opt/home/chenfeiyu/PythonWorkSpace/MyPointerGeneratorDataset/ttttt.py:9: RuntimeWarning: divide by zero encountered in log print(np.isfinite([np.inf, np.log(0), 1, np.nan]))[False False True False] 注意:此时系统对于$np.log(0)$给出的是warning,但是对于1/0就没办法判断是否是nan或inf了,因为对于分母为零系统给出的是error,程序会直接中断。 np.isinf 判断是否为infinity。 eg:1234567import numpy as npprint(np.isinf([np.inf, np.log(0), 1, np.nan]))输出:/opt/home/chenfeiyu/PythonWorkSpace/MyPointerGeneratorDataset/ttttt.py:9: RuntimeWarning: divide by zero encountered in log print(np.isinf([np.inf, np.log(0), 1, np.nan]))[ True True False False] np.nan 判断是否为not a number 1234567import numpy as npprint(np.isnan([np.inf, np.log(0), 1, np.nan]))输出:/opt/home/chenfeiyu/PythonWorkSpace/MyPointerGeneratorDataset/ttttt.py:9: RuntimeWarning: divide by zero encountered in log print(np.isnan([np.inf, np.log(0), 1, np.nan]))[False False False True] 抽取tf.slice() 原型:tf.slice(inputs,begin,size,name=’’)从inputs的指定位置连续的取,大小为size 123456789import tensorflow as tfinputs = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])data = tf.slice(inputs, [1, 0, 0], [1, 2, 2])with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(data.eval())输出:[[[3 3] [4 4]]] 说明:begin和size必和inputs的shape同型,上例中指的是从inputs的[1,0,0]开始,即第一个3处,取第一个维度size为1,第二个维度size为2,第三个维度size为2,即得到上述输出. tf.gather() 原型:tf.gather(params, indices, validate_indices=None, name=None):按照指定的下标集合从params的axis=0中抽取子集,适合抽取不连续区域的子集. 1234567891011121314import tensorflow as tfparams = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])indices=tf.constant([1,2])with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(tf.gather(params, indices)))输出:[[[3 3 3] [4 4 4]] [[5 5 5] [6 6 6]]] 说明:上诉是一般用法,实际的操作为[params[1],params[2]],无论indices是多少维,params直接找最里面的整形数字。 tf.gather_nd() 原型同上,唯一不同点是该函数取得不是axis=0,下面的例子阐述了两个函数的区别 123456789101112131415161718192021222324252627282930313233import tensorflow as tfparams = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])indices = tf.constant([[[0,1],[1,1]]])with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(tf.gather(params, indices)))输出:[[[[[1 1 1] [2 2 2]] [[3 3 3] [4 4 4]]] [[[3 3 3] [4 4 4]] [[3 3 3] [4 4 4]]]]]import tensorflow as tfparams = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]])indices = tf.constant([[[0,1],[1,1]]])with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(tf.gather_nd(params, indices)))输出:[[[2 2 2] [4 4 4]]] 说明:对于indices = tf.constant([[[0,1],[1,1]]]),gather函数的操作是[[[params[0],params[1]],[params[1],params[1]]]],即直接找整形.而gather_nd函数的操作是[[params[0,1],params[1,1]]],即找整形外面一层. tf.split() 原型为:split(value, num_or_size_splits, axis=0, num=None, name=&quot;split&quot;):将value分裂,若num_or_size_splits为一个整形,则把value的第axis维分为num_or_size_splits个Tensor(此时的value的axis必须满足整除num_or_size_splits),若num_or_size_splits为一个一维Tensor,则分成该Tensor的shape个Tensor(此时该Tensor各个维度相加必须等于该value的axis) 12345678910111213141516171819202122232425262728eg1:import tensorflow as tfvalue = tf.Variable(initial_value=tf.truncated_normal(shape=[6, 30]))split0, split1, split2 = tf.split(value, 3, 0)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(split0.shape) print(split1.shape) print(split2.shape)输出:(2, 30)(2, 30)(2, 30)import tensorflow as tfvalue = tf.Variable(initial_value=tf.truncated_normal(shape=[6, 30]))split0, split1, split2 = tf.split(value, [15,4,11], 1)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(split0.shape) print(split1.shape) print(split2.shape)输出:(6, 15)(6, 4)(6, 11) 损失函数sparse_softmax_cross_entropy_with_logitseg:12345678910import tensorflow as tfif __name__ == &apos;__main__&apos;: logits=tf.constant(value=[[0.00827806,-0.03050169],[-0.01209893,-0.03642108],[-0.0045999,-0.01193358],[-0.00983661,-0.04756571],[0.00212166,-0.05041311]]) labels = tf.constant(value=[1,1,1,0,0]) losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=labels) with tf.Session() as sess: tf.global_variables_initializer().run() print(losses.eval())#输出:[0.71272504 0.70538217 0.6968208 0.6744606 0.6672247 ] sparse_softmax_cross_entropy_with_logits的作用是首先将logits每一行做softmax,然后对labels每一个label转为one_hot,最后计算$\\sum_i^n y\\prime ln y$(e为底数) softmax_cross_entropy_with_logitseg:12345678910import tensorflow as tfif __name__ == &apos;__main__&apos;: logits=tf.constant(value=[[0.00827806,-0.03050169],[-0.01209893,-0.03642108],[-0.0045999,-0.01193358],[-0.00983661,-0.04756571],[0.00212166,-0.05041311]]) labels = tf.constant(value=[[0,1],[0,1],[0,1],[1,0],[1,0]]) losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels) with tf.Session() as sess: tf.global_variables_initializer().run() print(losses.eval())#输出:[0.71272504 0.70538217 0.6968208 0.6744606 0.6672247 ] 注意，下面的函数的labels本来就是one_hot类型,下面函数和上面唯一的不同就是这个,即下面的无需将labels转为one_hot,而是直接输入one_hot类型。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://lingyixia.github.io/categories/深度学习/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"https://lingyixia.github.io/tags/tensorflow/"}]},{"title":"朴素贝叶斯","slug":"NaiveBayes","date":"2018-12-20T07:36:51.000Z","updated":"2021-09-19T18:01:25.947Z","comments":true,"path":"2018/12/20/NaiveBayes/","link":"","permalink":"https://lingyixia.github.io/2018/12/20/NaiveBayes/","excerpt":"朴素贝叶斯简单推导","text":"朴素贝叶斯简单推导 问题定义设输入空间$\\chi\\subseteq R^n$，为n维空间的集合，输出空间为类标记集合$\\gamma={c_1,c_2,…,c_K}$，输入为特征向量$x\\subset\\chi$，输出为类标记$y\\subset\\gamma$，$X$是输入控件$\\chi$上的随机向量,$Y$是定义在输出空间$\\gamma$,$P(X,Y)$是$X$和$Y$的联合分布，训练数据集: T=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}由$P(X,Y)$独立同分布产生(谨记：$(x_1,y_1)$中的$x_1\\subset\\chi$，即$x_1$是n维)。现在需要用T训练贝叶斯模型，并判断$x’$的标签。 问题探究显然，$x’$的标签可以$y_1$~$y_N$中任何一个，若一定要判断标签也等同于哪个标签的概率最高，即计算: y=arg\\max_{c_k}P(Y=c_k|X=x) \\qquad k=1,2,3,...,K \\tag{1}已知: P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\Sigma_kP(X=x|Y=c_k)P(Y=c_k)} \\quad k=1,2,3,...K \\tag{2}其中，分母对于每个$c_k$都是相同的，略去不计算，只需计算:$P(X=x|Y=c_k)P(Y=c_k)$，即: y=arg\\max_{c_k}P(X=x|Y=c_k)P(Y=c_k) \\qquad k=1,2,3,...K \\tag{3}对于$P(Y=c_k)$只需要在给出的$T$中数出来即可，因此现在只需要计算$P(X=x|Y=c_k)$，这也是整个朴素贝叶斯过程最困难的一步，为什么说困难呢？$P(X=x|y=c_k)$完整表示为$P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},X^{(3)}=x^{(3)},….,X^{(N)}=x^{(N)}|Y=c_k)$若和$P(Y=c_k)$一样只有一个维度，很容易数出来，但这里需要同时考虑N个维度，数的过程是相当复杂的，因此，朴素贝叶斯在此处做了条件独立性假设： P(X=x|y=c_k)=\\prod_{i=1}^n(X^{(i)}=x^{(i)}|Y=c_k) \\qquad k=1,2,3,...K \\tag{4}这样就只需要同时考虑一个维度，简单了许多，最后将$(4)$带入$(3)$中得: y=arg\\max_{c_k}\\prod_{i=1}^n(X^{(i)}=x^{(i)}|Y=c_k)P(Y=c_k) \\qquad k=1,2,3,...K \\tag{5}至此朴素贝叶斯过程结束。 名词谨记 先验概率:$P(Y=c_k)$，即不管$X$，直接在$T$中数出来的概率 后验概率:$arg\\max_{c_k}P(Y=c_k|X=x)$，即考虑$X$的约束下，数出来的概率（注意：两个概率都是针对$Y$的概率） 原理其实前面已经可以说是整个朴素贝叶斯的过程，但是还要继续说明一个问题：$(1)$式是整个朴素贝叶斯要计算的基本公式，我们可以看出，其实该公式就是后验概率，也就是说，朴素贝叶斯是一个后验概率最大化的过程。 后验概率最大化的含义先说结论：后验概率最大化的含义就是选择$0-1$损失函数时的期望风险最小化，下面证明这个结论：首先$0-1$损失函数： L(Y,f(X))= \\begin{cases} 1 & Y ≠ f(X)\\\\ 0 & Y=f(X) \\\\ \\end{cases}其中$Y$和$f(X)$分别是$X$的实际标签和计算所的标签，则期望风险函数为： \\begin{align} R_{exp}(f(X)) &= E[L(Y,f(X))] \\\\ &=\\sum_{x=1}^N\\sum_{k=1}^K[L(Y=c_k,f(X)]P(Y=c_k,X=x_i)\\\\ &=E_X(\\sum_{k=1}^K[L(Y=c_k,f(X)]P(Y=c_k|X)) \\end{align}意思就是先对确定$X=x$求条件期望,其实就是两层而来来算。这样，针对确定的$X=x$而言： \\begin{align} f(x) &= arg\\min_{y\\subset\\chi}\\sum_{k=1}^KL(Y=c_k,f(X)=y)P(Y=c_k,|X=x) \\\\ &= arg\\min_{y\\subset\\chi}\\sum_{k=1}^KP(Y≠c_k,|X=x) \\\\\\\\ &= arg\\min_{y\\subset\\chi}\\sum_{k=1}^K(1-P(Y=c_k,|X=x)) \\\\ &= arg\\max_{y\\subset\\chi}\\sum_{k=1}^KP(Y=c_k,|X=x) \\end{align}由此，我们从期望风险最小化入手，最终得到了后验概率最小化，即朴素贝叶斯的原理。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lingyixia.github.io/categories/机器学习/"}],"tags":[{"name":"分类","slug":"分类","permalink":"https://lingyixia.github.io/tags/分类/"},{"name":"无监督","slug":"无监督","permalink":"https://lingyixia.github.io/tags/无监督/"}]},{"title":"排序算法","slug":"sort","date":"2018-11-20T02:11:08.000Z","updated":"2021-09-19T18:01:25.965Z","comments":true,"path":"2018/11/20/sort/","link":"","permalink":"https://lingyixia.github.io/2018/11/20/sort/","excerpt":"排序算法大概分为:交换类(快速排序和冒泡排序),插入类(简单插入排序和希尔排序),选择类(简单选择排序和堆排序),归并类(二路归并排序和多路归并排序),以下算法全部以增序为例","text":"排序算法大概分为:交换类(快速排序和冒泡排序),插入类(简单插入排序和希尔排序),选择类(简单选择排序和堆排序),归并类(二路归并排序和多路归并排序),以下算法全部以增序为例 交换类排序:冒泡排序 一共进行$n-1$轮次(最后一轮没必要,因为就剩一个了顺序已经排好),每一轮把第$[0,1,2,3…,i]$小的上浮。 实例代码:1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;void bubbleSort(vector&lt;int&gt;&amp; array);int main()&#123; clock_t startTime, endTime; startTime = clock();//计时开始 vector&lt;int&gt; array = &#123;5, 1, 9, 3, 7, 4, 8, 6, 2 &#125;; bubbleSort(arrray); endTime = clock();//计时结束 cout &lt;&lt; &quot;总计时长&quot; &lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return 0;&#125;void bubbleSort(vector&lt;int&gt;&amp; array)&#123; bool flag = true;//flag为false的意义为i之后的已经不需要排序了 for (int i = 0; i &lt; array.size() &amp;&amp; flag; i++) &#123; flag = false; for (int j = array.size() - 2; j &gt;= i; j--) &#123; if (array[j] &gt; array[j + 1]) &#123; swap(array[j], array[j + 1]); flag = true; &#125; &#125; &#125;&#125; 总结分析: 名称 时间 最好 最坏 空间 稳定 备注 冒泡排序 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ 是 每一轮都有一个元素到最终位置上 快速排序 把一个序列,选一个数(第一个数)进行划分。左边小于x,中间x,右边大于x。再依次递归划分左右两边。 实例代码:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;void Qsort(vector&lt;int&gt;&amp; array,int low,int high);//快速排序int Partition(vector&lt;int&gt;&amp; array, int low, int high);int main()&#123; clock_t startTime, endTime; startTime = clock();//计时开始 vector&lt;int&gt; array = &#123;5, 1, 9, 3, 7, 4, 8, 6, 2 &#125;; Qsort(array,0,array.size()-1); endTime = clock();//计时结束 cout &lt;&lt; &quot;总计时长&quot; &lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return 0;&#125;void Qsort(vector&lt;int&gt;&amp; array, int low, int high)&#123; int privot; if (low&lt;high) &#123; privot = Partition(array, low, high); Qsort(array,low,privot-1); Qsort(array, privot + 1, high); &#125;&#125;int Partition(vector&lt;int&gt;&amp; array, int low, int high)&#123; int privotKey = array[low]; while (low&lt;high) &#123; while (low&lt;high&amp;&amp;array[high]&gt;=privotKey)//改为&gt;则会使结果相同的数字在最终枢轴坐标左侧 &#123; high--; &#125; array[low] = array[high]; while (low &lt; high&amp;&amp;array[low] &lt;= privotKey)//改为&lt;则会使结果相同的数字在最终枢轴坐标右侧 &#123; low++; &#125; array[high] = array[low]; &#125; array[low] = privotKey; return low;&#125; 总结分析: 名称 时间 最好 最坏 空间 稳定 备注 快速排序 $O(nlogn)$ $O(nlogn)$ $O(n^2)$ 栈的深度$O(log_2n)$ 否 基本有序或者基本逆序，效果最差 二路快速排序 目的是处理数组中重复数据多,容易使时间复杂度退化到$n^2$的问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;cstdlib&gt;#include&lt;ctime&gt;#include&lt;ctime&gt;using namespace std;void swap(vector&lt;int&gt;&amp; array, int p, int q);void Qsort(vector&lt;int&gt;&amp; array, int low, int high);//二路快速排序+随机化int Partition2(vector&lt;int&gt;&amp; array, int low, int high);int main()&#123; clock_t startTime, endTime; srand((unsigned)time(NULL)); startTime = clock();//计时开始 vector&lt;int&gt; array = &#123; 5, 1, 9, 3, 7, 4, 8, 6, 2 &#125;; Qsort(array, 0, array.size() - 1); endTime = clock();//计时结束 cout &lt;&lt; &quot;总计时长&quot; &lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return 0;&#125;void Qsort(vector&lt;int&gt;&amp; array, int low, int high)&#123; int privot; if (low&lt;high) &#123; privot = Partition2(array, low, high); Qsort(array, low, privot - 1); Qsort(array, privot + 1, high); &#125;&#125;int Partition2(vector&lt;int&gt;&amp; array, int low, int high)&#123; swap(array,low,low + rand()%(high-low+1)); int privotKey = array[low]; int lowP = low+1; int highP = high; while (true) &#123; while (lowP&lt;=highP&amp;&amp;array[lowP] &lt; privotKey) &#123; lowP++; &#125; while (highP&gt;= lowP+1&amp;&amp;array[highP] &gt; privotKey) &#123; highP--; &#125; if (lowP &gt; highP) break; swap(array,lowP,highP); highP--; lowP++; &#125; swap(array,low,highP); return highP;&#125;void swap(vector&lt;int&gt;&amp; array, int p, int q)&#123; int temp = array[p]; array[p] = array[q]; array[q] = temp;&#125; 三路快速排序 目的是处理数组中重复数据多,容易使时间复杂度退化到$n^2$的问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;cstdlib&gt;#include&lt;ctime&gt;#include&lt;ctime&gt;using namespace std;void swap(vector&lt;int&gt;&amp; array, int p, int q);void Qsort3(vector&lt;int&gt;&amp; array, int low, int high);vector&lt;int&gt; Partition3(vector&lt;int&gt;&amp; array, int low, int high);int main()&#123; clock_t startTime, endTime; srand((unsigned)time(NULL)); startTime = clock();//计时开始 vector&lt;int&gt; array = &#123; 5, 1, 9, 3, 7, 4, 8, 6, 2 &#125;; Qsort3(array, 0, array.size() - 1); endTime = clock();//计时结束 cout &lt;&lt; &quot;总计时长&quot; &lt;&lt; (double)(endTime - startTime); // CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return 0;&#125;void Qsort3(vector&lt;int&gt;&amp; array, int low, int high)&#123; vector&lt;int&gt; privots; if (low&lt;high) &#123; privots = Partition3(array, low, high); Qsort3(array, low, privots[0]); Qsort3(array, privots[1], high); &#125;&#125;vector&lt;int&gt; Partition3(vector&lt;int&gt;&amp; array, int low, int high)&#123; vector&lt;int&gt; privots; swap(array, low, low + rand() % (high - low + 1)); int privotKey = array[low]; int lowP = low; int highP = high+1; int pos = low + 1; while (pos&lt;highP) &#123; if (array[pos]&lt;privotKey) &#123; swap(array,lowP+1,pos); pos++; lowP++; &#125; else if (array[pos] &gt; privotKey) &#123; swap(array,highP-1,pos); highP--; &#125; else &#123; pos++; &#125; &#125; swap(array, low, lowP); privots.push_back(lowP-1); privots.push_back(highP); return privots;&#125;void swap(vector&lt;int&gt;&amp; array, int p, int q)&#123; int temp = array[p]; array[p] = array[q]; array[q] = temp;&#125; 单链表快速排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include &lt;iostream&gt;using namespace std;struct ListNode&#123; ListNode *next; int val; ListNode(int val) : val(val), next(nullptr) &#123;&#125;&#125;;//个函数很重要需要特别主意，它可以用一次遍历把一个单链表分为&#123;小于key&#125;key&#123;大于key&#125;三部分ListNode *partition(ListNode *start, ListNode *end)&#123; int key = start-&gt;val; ListNode *slow = start; ListNode *fast = slow-&gt;next; while (fast != end) &#123; if (fast-&gt;val &lt;= key) &#123; slow=slow-&gt;next; swap(fast-&gt;val, slow-&gt;val); &#125; fast = fast-&gt;next; &#125; swap(start-&gt;val, slow-&gt;val); return slow;&#125;void quickSort(ListNode *start, ListNode *end)&#123; if (start != end) &#123; ListNode *index = partition(start, end); quickSort(start, index); quickSort(index-&gt;next, end); &#125;&#125;int main()&#123; ListNode *head = new ListNode(3); ListNode *work = head; ListNode *p = new ListNode(1); work-&gt;next = p; work = work-&gt;next; p = new ListNode(3); work-&gt;next = p; work = work-&gt;next; p = new ListNode(5); work-&gt;next = p; work = work-&gt;next; p = new ListNode(4); work-&gt;next = p; work = work-&gt;next; quickSort(head,NULL); while (head) &#123; cout&lt;&lt;head-&gt;val&lt;&lt;&quot; &quot;; head=head-&gt;next; &#125; return 0;&#125; 插入排序直接插入排序 前面的已经有序,把后面的插入到前面有序的元素中,不断增长有序序列。1.找到待插入位置2.给插入元素腾出空间,边比较边移动 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;void directInsertSort(vector&lt;int&gt;&amp; array);int main()&#123; clock_t startTime, endTime; startTime = clock();//计时开始 vector&lt;int&gt; array = &#123;5, 1, 9, 3, 7, 4, 8, 6, 2 &#125;; directInsertSort(array); endTime = clock();//计时结束 cout &lt;&lt; &quot;总计时长&quot; &lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return 0;&#125;void directInsertSort(vector&lt;int&gt;&amp; array)&#123; for (int i = 1; i &lt; array.size(); i++) &#123; int temp = array[i]; int j = i - 1; while (j &gt;= 0 &amp;&amp; temp &lt; array[j]) &#123; array[j + 1] = array[j]; j--; &#125; array[j + 1] = temp; &#125;&#125; 总结分析: 名称 时间 最好 最坏 空间 稳定 备注 直接插入排序 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ 是 适用于顺序存储 希尔排序 希尔排序又称为缩小增量排序,把整个列表分成多个$L[i,i+d,i+2d…i+kd]$这样的列表，每个进行直接插入排序。每一轮不断缩小d的值,直到全部有序(最后的d是1)。 实例代码:123456789101112131415161718192021222324252627282930313233343536#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;void shellSort(vector&lt;int&gt;&amp; array);//其实就相当于直接插入排序int main()&#123; clock_t startTime, endTime; startTime = clock();//计时开始 vector&lt;int&gt; array = &#123;5, 1, 9, 3, 7, 4, 8, 6, 2 &#125;; shellSort(array); endTime = clock();//计时结束 cout &lt;&lt; &quot;总计时长&quot; &lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return 0;&#125;void shellSort(vector&lt;int&gt;&amp; array)&#123; int increment = array.size(); do &#123; increment = increment / 3 + 1; for (int i = increment; i&lt;array.size(); i++) &#123; int temp = array[i]; int j = i - increment; while (j&gt;=0 &amp;&amp; temp&lt;array[j]) &#123; array[j + increment] = array[j]; j -= increment; &#125; array[j + increment] = temp; &#125; &#125; while (increment&gt;1);&#125; 总结分析: 名称 时间 最好 最坏 空间 稳定 备注 希尔排序 $O(n^{1.3})$ $O(n^2)$ $O(1)$ 否 选择类排序简单选择排序 前面已经有序,后面选择最小的与前面交换,从有序序列尾不断增长有序序列 实例代码:1234567891011121314151617#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;void easySelectSort(vector&lt;int&gt;&amp; array);//简单选择排序(适用于数组，链表涉及到交换)int main()&#123; clock_t startTime, endTime; startTime = clock();//计时开始 vector&lt;int&gt; array = &#123;5, 1, 9, 3, 7, 4, 8, 6, 2 &#125;; easySelectSort(array); endTime = clock();//计时结束 cout &lt;&lt; &quot;总计时长&quot; &lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return 0;&#125; 总结分析: 名称 时间 最好 最坏 空间 稳定 备注 简单选择排序 $O(n^2)$ $O(n^2)$ $O(n^2)$ $O(1)$ 否 堆排序 最好索引从1开始!!!!设当前节点是号是n,当索引从1开始时2*n正处于子树中,而索引从0开始时不然。 实例代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;void heapSort(vector&lt;int&gt;&amp; array);//堆排序必须从1开始，因为若是从0开始当前下标乘以2后的位置就不是当前节点的孩子节点了void heapAdjust(vector&lt;int&gt;&amp; array, int start, int end);int main()&#123; clock_t startTime, endTime; startTime = clock();//计时开始 vector&lt;int&gt; array = &#123;5, 1, 9, 3, 7, 4, 8, 6, 2 &#125;; heapSort(array); endTime = clock();//计时结束 cout &lt;&lt; &quot;总计时长&quot; &lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return 0;&#125;void heapSort(vector&lt;int&gt;&amp; array)&#123; for (int i = (array.size()-1)/2; i &gt;=1; i--) &#123; heapAdjust(array, i, array.size()-1); &#125; for (int i = array.size()-1; i &gt; 1; i--)//之所以是&gt;1而不是&gt;=1是因为只剩一个没必要了 &#123; swap(array[1],array[i]); heapAdjust(array,1,i-1); &#125;&#125;void heapAdjust(vector&lt;int&gt;&amp; array,int start,int end)&#123; int temp = array[start]; for (int i = start*2; i &lt;= end ; i*=2) &#123; if (i+1&lt;=end &amp;&amp; array[i]&lt;array[i+1])//判断+1后是否越界 &#123; i++; &#125; if (temp&gt;=array[i]) &#123; break;//注意！！！！！无需continue,因为开始建堆的时候就是从下向上调整的，上面不变动下面就肯定不需要变动 &#125; array[start] = array[i]; start = i; &#125; array[start] = temp;&#125; 分析总结: 名称 时间 最好 最坏 空间 稳定 备注 堆排序 $O(nlogn)$ $O(nlogn)$ $O(nlogn)$ $O(1)$ 否 归并排序(二路)由上向下-递归 归并排序的形式是一颗二叉树，遍历的次数就是二叉树的深度$O(logn)$,一共n个数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128递归:#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;void mergeSort(vector&lt;int&gt;&amp; array, int left, int right);//归并排序void merge(vector&lt;int&gt;&amp; array, int left, int mid, int right);int main()&#123; clock_t startTime, endTime; startTime = clock();//计时开始 vector&lt;int&gt; array = &#123;5, 1, 9, 3, 7, 4, 8, 6, 2 &#125;; mergeSort(array,0,array.size()-1); endTime = clock();//计时结束 cout &lt;&lt; &quot;总计时长&quot; &lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return 0;&#125;void mergeSort(vector&lt;int&gt;&amp; array,int left,int right)&#123; if (left&lt;right) &#123; int mid = (left + right) / 2; mergeSort(array,left,mid); mergeSort(array, mid+1, right); merge(array,left,mid,right); &#125;&#125;void merge(vector&lt;int&gt;&amp; array,int left,int mid,int right)&#123; int i = left; int j = mid + 1; vector&lt;int&gt; temp; while (i&lt;=mid &amp;&amp; j&lt;=right) &#123; if (array[i]&lt;array[j]) &#123; temp.push_back(array[i++]); &#125; else &#123; temp.push_back(array[j++]); &#125; &#125; while (i&lt;=mid) &#123; temp.push_back(array[i++]); &#125; while (j&lt;=right) &#123; temp.push_back(array[j++]); &#125; for (int i = 0; i &lt; temp.size(); i++) &#123; array[left+i] = temp[i]; &#125;&#125;非递归:#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;void merge(vector&lt;int&gt;&amp; array, int left, int mid, int right);void merge_sort_down2up(vector&lt;int&gt; &amp;array);void merge_groups(vector&lt;int&gt;&amp; array, int gap);int main()&#123; clock_t startTime, endTime; startTime = clock();//计时开始 vector&lt;int&gt; array = &#123;5, 1, 9, 3, 7, 4, 8, 6, 2 &#125;; merge_sort_down2up(array); endTime = clock();//计时结束 cout &lt;&lt; &quot;总计时长&quot; &lt;&lt; (double)(endTime - startTime) / CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl; return 0;&#125;void merge(vector&lt;int&gt;&amp; array,int left,int mid,int right)&#123; int i = left; int j = mid + 1; vector&lt;int&gt; temp; while (i&lt;=mid &amp;&amp; j&lt;=right) &#123; if (array[i]&lt;array[j]) &#123; temp.push_back(array[i++]); &#125; else &#123; temp.push_back(array[j++]); &#125; &#125; while (i&lt;=mid) &#123; temp.push_back(array[i++]); &#125; while (j&lt;=right) &#123; temp.push_back(array[j++]); &#125; for (int i = 0; i &lt; temp.size(); i++) &#123; array[left+i] = temp[i]; &#125;&#125;void merge_sort_down2up(vector&lt;int&gt;&amp; array) &#123; for (int i = 1; i &lt; array.size(); i = i * 2) &#123; merge_groups(array, i); &#125;&#125;void merge_groups(vector&lt;int&gt;&amp; array, int gap)&#123; int twolen = 2 * gap; int i; for (i = 0; i + twolen - 1 &lt; array.size(); i += twolen) &#123; int start = i; int mid = i + gap - 1; int end = i + twolen - 1; merge(array, start, mid, end); &#125; // 最后还有一个gap if (i + gap - 1 &lt; array.size() - 1) &#123; merge(array, i, i + gap - 1, array.size() - 1); &#125;&#125; 单链表归并排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#include &lt;iostream&gt;using namespace std;struct ListNode&#123; ListNode *next; int val; ListNode(int val) : val(val), next(nullptr) &#123;&#125;&#125;;ListNode *getMidNode(ListNode *head)&#123; ListNode *slow = head; ListNode *fast = head-&gt;next; while (fast) &#123; fast = fast-&gt;next; if (fast) &#123; fast = fast-&gt;next; slow = slow-&gt;next; &#125; &#125; return slow;&#125;//归并方式合并两个单链表需要记住ListNode *mergeList(ListNode *l1, ListNode *l2)&#123; if (!l1) return l2; if (!l2) return l1; if (l1-&gt;val &lt;= l2-&gt;val) &#123; l1-&gt;next = mergeList(l1-&gt;next, l2); return l1; &#125; l2-&gt;next = mergeList(l1, l2-&gt;next); return l2;&#125;ListNode *sortList(ListNode *head)&#123; if (head &amp;&amp; head-&gt;next) &#123; ListNode *mid = getMidNode(head); ListNode *left = head; ListNode *right = mid-&gt;next; mid-&gt;next = NULL; left = sortList(left); right = sortList(right); return mergeList(left, right); &#125; return head;&#125;int main()&#123; ListNode *head = new ListNode(3); ListNode *work = head; ListNode *p = new ListNode(1); work-&gt;next = p; work = work-&gt;next; p = new ListNode(3); work-&gt;next = p; work = work-&gt;next; p = new ListNode(5); work-&gt;next = p; work = work-&gt;next; p = new ListNode(4); work-&gt;next = p; work = work-&gt;next; head = sortList(head); while (head) &#123; cout &lt;&lt; head-&gt;val &lt;&lt; &quot; &quot;; head = head-&gt;next; &#125; return 0;&#125; 分析总结: 名称 时间 最好 最坏 空间 稳定 备注 归并排序 $O(nlogn)$ $O(nlogn)$ $O(nlogn)$ $O(n)$ 是 计数排序 其实就是计算待排序数组中每个数字的个数,用下标来代替原数，因此只适用于整型,计算max-min是为了减少countArray的size 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;void countingSort(vector&lt;int&gt; &amp;array)&#123; int max = INT_MIN; int min = INT_MAX; for (auto num:array) &#123; if (num &gt; max) &#123; max = num; &#125; if (num &lt; min) &#123; min = max; &#125; &#125; vector&lt;int&gt; countArray(max - min + 1, 0); for (auto num:array) &#123; countArray[num - min]++; &#125; for (int i = 0; i &lt; countArray.size(); ++i) &#123; for (int j = 0; j &lt; countArray[i]; ++j) &#123; cout &lt;&lt; min + i &lt;&lt; &quot; &quot;; &#125; &#125;&#125;int main()&#123; vector&lt;int&gt; array = &#123;1, 5, 3, 7, 6, 2, 8, 9, 4, 3, 3&#125;; countingSort(array); return 0;&#125; 分析总结: 名称 时间 最好 最坏 空间 稳定 备注 计数排序 $O(n+d)$ $O(n+d)$ $O(n+d)$ $O(k)$ 是 桶排序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;list&gt;using namespace std;void insert(list&lt;int&gt;&amp; bucket,int val)&#123; auto iter = bucket.begin(); while(iter != bucket.end() &amp;&amp; val &gt;= *iter) ++iter; //insert会在iter之前插入数据，这样可以稳定排序 bucket.insert(iter,val);&#125;void bucketSort(vector&lt;int&gt;&amp; arr)&#123; int len = arr.size(); if(len &lt;= 1) return; int min = arr[0],max = min; for(int i=1;i&lt;len;++i) &#123; if(min&gt;arr[i]) min = arr[i]; if(max&lt;arr[i]) max = arr[i]; &#125; int k = 10;//k为数字之间的间隔 //向上取整，例如[0,9]有10个数，(9 - 0)/k + 1 = 1; int bucketsNum = (max - min)/k + 1; vector&lt;list&lt;int&gt;&gt; buckets(bucketsNum); for(int i=0;i&lt;len;++i) &#123; int value = arr[i]; //(value-min)/k就是在哪个桶里面 insert(buckets[(value-min)/k],value); &#125; int index = 0; for(int i=0;i&lt;bucketsNum;++i) &#123; if(buckets[i].size()) &#123; for(auto&amp; value:buckets[i]) arr[index++] = value; &#125; &#125;&#125;int main()&#123; vector&lt;int&gt; A=&#123;-100,13,14,94,33,82,25,59,94,65,23,45,27,43,25,39,10,35,54,90,-200,58&#125;; for(auto value:A) cout&lt;&lt;value&lt;&lt;&quot; &quot;; cout&lt;&lt;endl; bucketSort(A); return 0;&#125; 分析总结: 名称 时间 最好 最坏 空间 稳定 备注 桶排序 $O(n+d)$ $O(n+d)$ $O(n*n)$ $O(n+k)$ 是 基数排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;void countSort(vector&lt;int&gt;&amp; array, int exp)&#123; vector&lt;int&gt; range(10, 0); int length = array.size(); vector&lt;int&gt; tmpVec(length, 0); for (int i = 0; i &lt; length; ++i) &#123; range[(array[i] / exp) % 10]++; &#125; for (int i = 1; i &lt; range.size(); ++i) &#123; range[i] += range[i - 1];//统计本应该出现的位置 &#125; for (int i = length - 1; i &gt;= 0; --i) &#123; tmpVec[range[(array[i] / exp) % 10] - 1] = array[i]; range[(array[i] / exp) % 10]--; &#125; array = tmpVec;&#125;void radixSort(vector&lt;int&gt; &amp;array)&#123; int length = array.size(); int max = -1; for (int i = 0; i &lt; length; ++i) &#123; if (array[i] &gt; max) max = array[i]; &#125; //提取每一位并进行比较，位数不足的高位补0 for (int exp = 1; max / exp &gt; 0; exp *= 10) countSort(array, exp);&#125;int main()&#123; vector&lt;int&gt; array&#123;53, 3, 542, 748, 14, 214, 154, 63, 616, 589&#125;; radixSort(array); for (int i = 0; i &lt; array.size(); ++i) &#123; cout &lt;&lt; array[i] &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; return 0;&#125; 分析总结: 名称 时间 最好 最坏 空间 稳定 备注 桶排序 $O(n+d)$ $O(n+d)$ $O(n*n)$ $O(n+k)$ 是","categories":[{"name":"算法","slug":"算法","permalink":"https://lingyixia.github.io/categories/算法/"}],"tags":[{"name":"排序","slug":"排序","permalink":"https://lingyixia.github.io/tags/排序/"}]},{"title":"序列标注常用方式","slug":"CommonLabel","date":"2018-08-28T07:11:55.000Z","updated":"2021-09-19T18:01:25.945Z","comments":true,"path":"2018/08/28/CommonLabel/","link":"","permalink":"https://lingyixia.github.io/2018/08/28/CommonLabel/","excerpt":"常用序列标注方式","text":"常用序列标注方式 BIO 基本标注方式为:$(B-begin,I-inside,O-other)$. $BIO$标注:将每个元素标注为“B_X”、“I_X”或者“O”。其中,“B_X”表示此元素所在的片段属于$X$类型并且此元素在此片段的开头,“I_X”表示此元素所在的片段属于$X$类型并且此元素在此片段的中间位置,“O”表示不属于任何类型。eg: 比如,我们将X表示为名词短语(Noun Phrase, NP),则BIO的三个标记为: B-NP：名词短语的开头 I-NP：名词短语的中间 O:不是名词短语 BIOES 基本标注方式为: $(B-begin,I-inside,O-other,E-end,S-single)$","categories":[],"tags":[{"name":"Ner","slug":"Ner","permalink":"https://lingyixia.github.io/tags/Ner/"}]}]}