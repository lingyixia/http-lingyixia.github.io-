{"meta":{"title":"灵翼俠的个人博客","subtitle":null,"description":null,"author":"陈飞宇","url":"http://yoursite.com"},"pages":[{"title":"","date":"2018-12-20T07:36:15.721Z","updated":"2018-12-20T07:36:15.721Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":"title: categories date: 2018-12-20 15:35:06"},{"title":"","date":"2018-12-20T07:35:57.658Z","updated":"2018-12-20T07:35:57.658Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":"title: tags date: 2018-12-20 15:35:03"}],"posts":[{"title":"股票最大利润","slug":"leetcode_121","date":"2018-12-20T08:05:09.705Z","updated":"2018-10-29T02:50:04.121Z","comments":true,"path":"2018/12/20/leetcode_121/","link":"","permalink":"http://yoursite.com/2018/12/20/leetcode_121/","excerpt":"","text":"Say you have an array for which the ith element is the price of a given stock on day i. If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit. Note that you cannot sell a stock before you buy one. 1234567891011121314151617//第一种方法：记录到i-1为止，最小下标，然后比较prices[i]-prices[currintMinIndex]int maxProfit(vector&lt;int&gt;&amp; prices) &#123; int currintMinIndex = 0; int currentMax = 0; int maxSum = 0; for(int i = 1;i&lt;prices.size();i++) &#123; if(prices[i-1]&lt;prices[currintMinIndex]) &#123; currintMinIndex = i-1; &#125; currentMax = prices[i] - prices[currintMinIndex]; maxSum = maxSum&gt;currentMax?maxSum:currentMax; &#125; return maxSum;&#125; 123456789101112131415//第二种方法：计算prices[i]-price[i-1],然后题目就可以转为连续数组最大值问题int maxProfit(vector&lt;int&gt;&amp; prices) &#123; int currentProfit = 0; int currentMaxProfit=0; int maxProfit = 0;//此处不能是INT_MIN，因为有可能[7,6,4,3,1]，即一直下降，此时可以选择不买不买，即利润最低为0 for(int i = 1;i&lt;prices.size();i++) &#123; currentProfit = prices[i]-prices[i-1]; currentMaxProfit += currentProfit; maxProfit = currentMaxProfit&gt;maxProfit?currentMaxProfit:maxProfit; currentMaxProfit = currentMaxProfit&lt;0?0:currentMaxProfit; &#125; return maxProfit&gt;0;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"},{"name":"动态规划","slug":"动态规划","permalink":"http://yoursite.com/tags/动态规划/"}]},{"title":"背包问题","slug":"backpack","date":"2018-12-20T08:05:09.692Z","updated":"2018-10-29T02:50:04.121Z","comments":true,"path":"2018/12/20/backpack/","link":"","permalink":"http://yoursite.com/2018/12/20/backpack/","excerpt":"","text":"0-1背包、完全背包、多重背包问题简记 1.0-1背包问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263//递推公式为：values[i][j] = max(values[i - 1][j], values[i-1][j - woods[i-1].volume] + woods[i-1].value);#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;int items[5] = &#123;0,0,0,0,0&#125;;struct Woods&#123; int volume; int value;&#125;;void backPack(vector&lt;Woods&gt; woods, int bag);void findItems(int** values, vector&lt;Woods&gt; woods, int i, int j);int main()&#123; vector&lt;Woods&gt; woods = &#123;Woods&#123;2,3&#125;,Woods&#123;3,4&#125;,Woods&#123;4,5&#125;,Woods&#123;5,6&#125; &#125;; backPack(woods,8); return 0;&#125;void backPack(vector&lt;Woods&gt; woods, int bag)&#123; int** values = new int*[woods.size()+1]; for (int i = 0; i &lt;= woods.size(); i++) &#123; values[i] = new int[bag+1](); &#125; for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = 1; j &lt;= bag; j++) &#123; if (j &lt; woods[i-1].volume) &#123; values[i][j] = values[i - 1][j]; &#125; else &#123; values[i][j] = max(values[i - 1][j], values[i-1][j - woods[i-1].volume] + woods[i-1].value); &#125; &#125; &#125; findItems(values, woods, 4, 8); for (int i = 0; i &lt;= woods.size(); i++) &#123; delete [] values[i]; &#125; delete values;&#125;void findItems(int** values, vector&lt;Woods&gt; woods,int i ,int j)&#123; if (i&gt;0) &#123; if (values[i][j] == values[i-1][j]) &#123; items[i] = 0; findItems(values, woods,i-1,j); &#125; else &#123; items[i] = 1; findItems(values, woods,i-1,j-woods[i-1].volume); &#125; &#125;&#125; 上诉方法使用二维数组保存中间值，比较消耗空间，而且我们可以看出，对于每一步要求的values[i][j]而言，只依赖于values[i-1][:],即前一行，因此我们只需要一维数组即可，循环保存前一行数据。但是这样貌似就无法找物品组成了，回头在想。123456789101112131415//空间优化写法void backPack(vector&lt;Woods&gt; woods, int bag)&#123; int* values = new int[bag + 1](); for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = bag; j &gt;= 0; j--)//倒序，保证在改完某个数据之前，它依赖的前面的数据还没改 &#123; if (j - woods[i-1].volume &gt;= 0) &#123; values[j] = max(values[j], values[j - woods[i-1].volume] + woods[i-1].value); &#125; &#125; &#125;&#125; 123456789101112//再次改善void backPack(vector&lt;Woods&gt; woods, int bag)&#123; int* values = new int[bag + 1](); for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = bag; j &gt;= woods[i-1].volume; j--)//倒序，保证更改某个数据之前它依赖的前面的数据未改 &#123; values[j] = max(values[j], values[j - woods[i-1].volume] + woods[i-1].value); &#125; &#125;&#125; 2.完全背包问题 12与0-1背包唯一的不同就是递推公式：values[i][j] = max(values[i - 1][j], values[i][j - woods[i-1].volume] + woods[i-1].value);//不在详细说明 123456789101112//空间优化写法void backPack(vector&lt;Woods&gt; woods, int bag)&#123; int* values = new int[bag + 1](); for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = woods[i-1].volume; j &lt;= bag; j++)//正序 &#123; values[j] = max(values[j], values[j - woods[i-1].volume] + woods[i-1].value); &#125; &#125;&#125; 3.多重背包问题（即每种物品的数量有上限） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//在0-1背包的基础上增加数量循环判断,递推公式为：values[i][j] = max(values[i][j], values[i-1][j - k * woods[i - 1].volume] + k * woods[i - 1].value);#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;struct Woods&#123; int volume; int value; int num;&#125;;void backPack(vector&lt;Woods&gt; woods, int bag);int main()&#123; vector&lt;Woods&gt; woods = &#123; Woods&#123; 80 ,20 ,4 &#125;,Woods&#123; 40 ,50, 9 &#125;,Woods&#123; 30 ,50, 7 &#125;,Woods&#123; 40 ,30 ,6 &#125;,Woods&#123; 20 ,20 ,1 &#125; &#125;; backPack(woods, 1000); return 0;&#125;void backPack(vector&lt;Woods&gt; woods, int bag)&#123; int** values = new int*[woods.size() + 1]; for (int i = 0; i &lt;= woods.size(); i++) &#123; values[i] = new int[bag + 1](); &#125; for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = 1; j &lt;= bag; j++) &#123; values[i][j] = values[i - 1][j]; for (int k = 1; k &lt;= woods[i - 1].num; k++) &#123; if (j &lt; k * woods[i - 1].volume) &#123; break; &#125; values[i][j] = max(values[i][j], values[i-1][j - k * woods[i - 1].volume] + k * woods[i - 1].value); &#125; &#125; &#125; for (int i = 0; i &lt;= woods.size(); i++) &#123; delete[] values[i]; &#125; delete values;&#125; 1234567891011121314151617181920//空间优化写法void backPack(vector&lt;Woods&gt; woods, int bag)&#123; int* values = new int[bag + 1](); for (int i = 1; i &lt;= woods.size(); i++) &#123; for (int j = bag; j &gt;= 0; j--) &#123; for (int k = 1; k &lt;= woods[i - 1].num; k++) &#123; if (j &lt; k * woods[i - 1].volume) &#123; break; &#125; values[j] = max(values[j], values[j - k * woods[i - 1].volume] + k * woods[i - 1].value); &#125; &#125; &#125; delete[] values;&#125; 二进制写法先不想了","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"}]},{"title":"连续数组最大和","slug":"leetcode_53","date":"2018-12-20T08:05:09.691Z","updated":"2018-10-29T02:50:04.120Z","comments":true,"path":"2018/12/20/leetcode_53/","link":"","permalink":"http://yoursite.com/2018/12/20/leetcode_53/","excerpt":"","text":"Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. 12345678910111213141516171819202122//动态规划//递推公式为DP[i] = max&#123;DP[i-1] + A[i],A[i]&#125;int maxSubArray(vector&lt;int&gt;&amp; nums)&#123; int currentMax = 0;//currentMax是i处以及之前的连续最大值 int max = INT_MIN; for (int i = 0; i &lt; nums.size(); i++) &#123; currentMax += nums[i]; max = currentMax&gt;max?currentMax:max; currentMax = currentMax&lt;0?0:currentMax; //if (currentMax&gt;max) //&#123; // max = currentMax; //&#125; //if (currentMax&lt;0)//之所以不用else if是考虑譬如[-2,1]的情况 //&#123; // currentMax = 0; //&#125; &#125; return max;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"},{"name":"动态规划","slug":"动态规划","permalink":"http://yoursite.com/tags/动态规划/"}]},{"title":"C++数组长度","slug":"cpp-array","date":"2018-12-20T08:05:09.690Z","updated":"2018-09-25T01:14:22.715Z","comments":true,"path":"2018/12/20/cpp-array/","link":"","permalink":"http://yoursite.com/2018/12/20/cpp-array/","excerpt":"","text":"记录函数内获取数组长度的坑 看代码:12345678910111213141516171819202122#include&lt;iostream&gt;using namespace std;void fun(int* array)&#123; cout &lt;&lt; array &lt;&lt; endl; cout &lt;&lt; sizeof(array);&#125;int main()&#123; int array[] = &#123; 1,2,3,4,5,6 &#125;; cout &lt;&lt; array &lt;&lt; endl; cout &lt;&lt; sizeof(array) &lt;&lt; endl; fun(array); return 0;&#125;结果：00DEFCA02400DEFCA04 可以看出，虽然函数体内外的array所指的地址相同，sizeof后并不一样，前者是实际数组的大小，后者是纯指针的大小（编译器决定），也就是说，当数组传到函数内后，就意味着它是一个纯指针，不再有数组的意义了，要想在函数内获取数组长度，只能以参数的形式传入。总之，想在函数内获取数组长度就用vector吧！！！","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"github博客小记","slug":"jekyll_github_record","date":"2018-12-20T08:05:09.674Z","updated":"2018-09-25T01:14:22.715Z","comments":true,"path":"2018/12/20/jekyll_github_record/","link":"","permalink":"http://yoursite.com/2018/12/20/jekyll_github_record/","excerpt":"","text":"简单记录自己github page搭建的过程，只是为了记录自己踩一些坑，不是新手教程 本博客使用了jekyll+github page,要使用到jekyll的主题，可以先把主题下载下来，本地预览调试，在传到github上托管。也可以直接fork到自己的github中，改名字，在下载下来。我是直接fork的作者大大的博客，然后改名字即可。 1.：github如果不自定义域名，会强制使用https协议，这样，如果某个页面中引用了http协议的连接会出错（似乎只能用一样的协议），这时可以在header中加入： 1&lt;meta http-equiv=&quot;Content-Security-Policy&quot;content=&quot;upgrade-insecure-requests&quot;&gt; 作用是强制http使用https。 但是！！！！！！当本地预览的时候地址是一定要将该句注释掉，因为本地预览是http:/127.0.0.1:4000，不支持https。 2.jekyll本地预览首先需要安装ruby+devkit，接下来：gem install jekyll（安装jekyll）gem install bundler（安装bundler）然后到博客目录下，新建Gemfile，里面输入： 12source &apos;https://rubygems.org&apos;gem &apos;github-pages&apos;, group: :jekyll_plugins 然后：bundle install(需要上诉Gemfile)，最后bundle exec jekyll serve启动服务器即可 3.simple-jekyll-search:最让人头疼的是找不到json,基本步骤按照这里即可以，一定不要忘了那个123---layout: null--- 也就是说其实这根本不是一个标准的json，我就是没加上面那部分说啥也不行,坑苦了我，本来还以为不是json的一部分。","categories":[{"name":"技术","slug":"技术","permalink":"http://yoursite.com/categories/技术/"}],"tags":[{"name":"github","slug":"github","permalink":"http://yoursite.com/tags/github/"}]},{"title":"数据科学常用之Pandas","slug":"data_science_pandas","date":"2018-12-20T08:05:09.673Z","updated":"2018-10-31T12:32:14.830Z","comments":true,"path":"2018/12/20/data_science_pandas/","link":"","permalink":"http://yoursite.com/2018/12/20/data_science_pandas/","excerpt":"","text":"包含数据科学常用Pandas函数 1.pd.set_option[set_option文档地址] (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html?highlight=set_option#pandas.set_option)123456#显示所有列pd.set_option(&apos;display.max_columns&apos;, None)#显示所有行pd.set_option(&apos;display.max_rows&apos;, None)#设置value的显示长度为100，默认为50pd.set_option(&apos;max_colwidth&apos;,100)","categories":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/categories/pandas/"}],"tags":[{"name":"数据科学","slug":"数据科学","permalink":"http://yoursite.com/tags/数据科学/"}]},{"title":"Tensorflow中Rnn的实现","slug":"tensorflow_rnn","date":"2018-12-20T08:05:09.668Z","updated":"2018-10-29T07:57:40.550Z","comments":true,"path":"2018/12/20/tensorflow_rnn/","link":"","permalink":"http://yoursite.com/2018/12/20/tensorflow_rnn/","excerpt":"","text":"包含TensorFlow中BasicRNNCell,BasicLSTMCell等的实现 1.BasicRNNCell基本结构如图: 在TensorFlow中，BasicRNNCellm每一步输出的state和output相同，源代码如下: 12345678def call(self, inputs, state): &quot;&quot;&quot;Most basic RNN: output = new_state = act(W * input + U * state + B).&quot;&quot;&quot; gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) output = self._activation(gate_inputs) return output, output 公式如下: $$ht=tanh(W_k[x_t,h_{t-1}]+b)$$ 或 $$ht=tanh(W_x+Uh_{t-1}+b)$$ 但是我个人认为应该是: $$ht=tanh([x_t,h_{t-1}]*W+b) \\tag{1}$$ 我也不知道为啥会都写作上面那两种形式.eg: 123456789101112131415161718192021222324252627282930import tensorflow as tfimport numpy as npbatch_size = 3input_dim = 2output_dim = 4inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, input_dim))cell = tf.contrib.rnn.BasicRNNCell(num_units=output_dim)previous_state = cell.zero_state(batch_size, dtype=tf.float32)output, state = cell(inputs, previous_state)kernel = cell.variablesX = np.ones(shape=(batch_size, input_dim))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) output, state, kernel = sess.run([output, state,kernel], feed_dict=&#123;inputs: X&#125;) print(X.shape) print(kernel[0].shape) print(kernel[1].shape) print(previous_state.shape) print(state.shape) print(output.shape)结果为:(3, 2)(6, 4)(4,)(3, 4)(3, 4)(3, 4) 分析:(kernel中是所有参数的list，此处是W和bias)根据公式(1),$output = ([X,previous_state] W+bias)$,即$([(3, 2),(3, 4)](6, 4)+(4,)) = (3,4)$，代码中也较容易看出. 2.BasicLSTMCell基本结构如图: 源码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243def call(self, inputs, state): &quot;&quot;&quot;Long short-term memory cell (LSTM). Args: inputs: `2-D` tensor with shape `[batch_size, input_size]`. state: An `LSTMStateTuple` of state tensors, each shaped `[batch_size, self.state_size]`, if `state_is_tuple` has been set to `True`. Otherwise, a `Tensor` shaped `[batch_size, 2 * self.state_size]`. Returns: A pair containing the new hidden state, and the new state (either a `LSTMStateTuple` or a concatenated state, depending on `state_is_tuple`). &quot;&quot;&quot; sigmoid = math_ops.sigmoid one = constant_op.constant(1, dtype=dtypes.int32) # Parameters of gates are concatenated into one multiply for efficiency. if self._state_is_tuple: c, h = state else: c, h = array_ops.split(value=state, num_or_size_splits=2, axis=one) gate_inputs = math_ops.matmul( array_ops.concat([inputs, h], 1), self._kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) # i = input_gate, j = new_input, f = forget_gate, o = output_gate i, j, f, o = array_ops.split( value=gate_inputs, num_or_size_splits=4, axis=one) forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype) # Note that using `add` and `multiply` instead of `+` and `*` gives a # performance improvement. So using those at the cost of readability. add = math_ops.add multiply = math_ops.multiply new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))), multiply(sigmoid(i), self._activation(j))) new_h = multiply(self._activation(new_c), sigmoid(o)) if self._state_is_tuple: new_state = LSTMStateTuple(new_c, new_h) else: new_state = array_ops.concat([new_c, new_h], 1) return new_h, new_state 公式如下: $$f_t = \\sigma(W_f[h_{t-1},x_t]+b_f)\\i_t = \\tanh(W_i[h_{t-1,x_t}]+b_i) \\\\widetilde C_t = \\tanh(W_C[h_{t-1,x_t}]+b_C) \\O_t = \\sigma(W_o[h_{t-1},x_t]+b_o)\\C_t = f_tC_{t-1}+i_t\\widetilde C_t \\h_t = O_t * \\tanh(C_t)$$ 同理，我感觉应该是: $$f_t = \\sigma([h_{t-1},x_t]W_f+b_f)\\i_t = \\tanh([h_{t-1,x_t}]W_i+b_i) \\\\widetilde C_t = \\tanh([h_{t-1,x_t}]W_C+b_C) \\O_t = \\sigma([h_{t-1},x_t]W_o+b_o)\\C_t = f_tC_{t-1}+i_t\\widetilde C_t \\h_t = O_t * \\tanh(C_t)$$ eg:1234567891011121314151617181920212223242526272829303132333435import tensorflow as tfimport numpy as npbatch_size = 3input_dim = 2output_dim = 4inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, input_dim))cell = tf.contrib.rnn.BasicLSTMCell(num_units=output_dim)previous_state = cell.zero_state(batch_size, dtype=tf.float32)output, state = cell(inputs, previous_state)kernel = cell.variablesX = np.ones(shape=(batch_size, input_dim))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) output, state,kernel = sess.run([output, state, kernel], feed_dict=&#123;inputs: X&#125;) print(X.shape) print(previous_state[0].shape) print(previous_state[1].shape) print(kernel[0].shape) print(kernel[1].shape) print(state[0].shape) print(state[1].shape) print(output.shape)结果:(3, 2)(3, 4)(3, 4)(6, 16)(16,)(3, 4)(3, 4)(3, 4) 分析: (kernel是所有参数，即W和bias)根据上诉公式，在源码中求遗忘门:$f_t$,输入门$i_t$和$\\widetilde C_t$s输出门$O_t$的代码为: 1i, j, f, o = array_ops.split(value=gate_inputs, num_or_size_splits=4, axis=one) 这里也解释了为什么eg中kernel[0]是(6,16),因为代码中是将4个W同时初始化在一起，即（6,16）中其实是有4个W，并在上诉代码中分别计算，kernal[1]同理。这样得到的i,j,f,o应该都是(3,4),在源码中可以看出计算i,j,f,o是矩阵相乘，但是计算$C_t$和$h_t$是各个元素相乘,因此得到的$C_t$和$h_t$都是(3,4). 3.GRU源代码为: 12345678910111213141516171819def call(self, inputs, state): &quot;&quot;&quot;Gated recurrent unit (GRU) with nunits cells.&quot;&quot;&quot; gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._gate_kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias) value = math_ops.sigmoid(gate_inputs) r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1) r_state = r * state candidate = math_ops.matmul( array_ops.concat([inputs, r_state], 1), self._candidate_kernel) candidate = nn_ops.bias_add(candidate, self._candidate_bias) c = self._activation(candidate) new_h = u * state + (1 - u) * c return new_h, new_h eg: 1234567891011121314151617181920212223242526272829303132333435import tensorflow as tfimport numpy as npbatch_size = 3input_dim = 2output_dim = 4inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, input_dim))cell = tf.contrib.rnn.GRUCell(num_units=output_dim)previous_state = cell.zero_state(batch_size, dtype=tf.float32)output, state = cell(inputs, previous_state)kernel = cell.variablesX = np.ones(shape=(batch_size, input_dim))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) output, state, kernel = sess.run([output, state, kernel], feed_dict=&#123;inputs: X&#125;) print(X.shape) print(previous_state.shape) print(kernel[0].shape) print(kernel[1].shape) print(kernel[2].shape) print(kernel[3].shape) print(state.shape) print(output.shape)结果:(3, 2)(3, 4)(6, 8)(8,)(6, 4)(4,)(3, 4)(3, 4)","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://yoursite.com/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://yoursite.com/tags/tensorflow/"}]},{"title":"C++优先队列","slug":"priority_queue","date":"2018-12-20T08:05:09.664Z","updated":"2018-10-29T02:50:04.120Z","comments":true,"path":"2018/12/20/priority_queue/","link":"","permalink":"http://yoursite.com/2018/12/20/priority_queue/","excerpt":"","text":"优先队列是队列和栈结合形成的一种数据结构，c++中使用堆来构建。 首先函数在头文件中，归属于命名空间std，使用的时候需要注意。有两种声明方式:12std::priority_queue&lt;T&gt; pq;std::priority_queue&lt;T, std::vector&lt;T&gt;, cmp&gt; pq; 第一种方式较为简单，当T是基本数据类型的时候使用当T是自定义数据结构的时候(一般是struct)需要使用第二中声明方式.三个参数从左到右第一个指的是优先队列中的数据类型，第二个表示存储堆的数据结构，一般是vector即可，第三个是比较结构，是一个struct，默认是less，即小的在前，但是默认声明方式只针对基本数据结构，自定义的需要重写该结构体，less也可以改为greater。 greater和less是std实现的两个仿函数（就是使一个类的使用看上去像一个函数。其实现就是类中实现一个operator()，这个类就有了类似函数的行为，就是一个仿函数类了） eg:123456789101112131415161718192021222324#include&lt;iostream&gt;#include&lt;queue&gt;using namespace std;struct Node&#123; int val; Node(int val) :val(val) &#123;&#125;&#125;;int main()&#123; priority_queue&lt;Node&gt; A; //大根堆 priority_queue&lt;Node, vector&lt;Node&gt;, greater&lt;Node&gt; &gt; B; //小根堆 A.push(Node(1)); A.push(Node(9)); A.push(Node(4)); cout &lt;&lt; A.top().val &lt;&lt; endl; A.pop(); cout &lt;&lt; A.top().val &lt;&lt; endl; A.pop(); cout &lt;&lt; A.top().val &lt;&lt; endl; A.pop(); return 0;&#125; 这样运行出错，因为priority_queue A; 默认使用的是less，而less的源代码是: 1234567891011struct less&#123; // functor for operator&lt; _CXX17_DEPRECATE_ADAPTOR_TYPEDEFS typedef _Ty first_argument_type; _CXX17_DEPRECATE_ADAPTOR_TYPEDEFS typedef _Ty second_argument_type; _CXX17_DEPRECATE_ADAPTOR_TYPEDEFS typedef bool result_type; constexpr bool operator()(const _Ty&amp;_Left, const _Ty&amp; _Right) const &#123; // apply operator&lt; to operands return (_Left &lt; _Right); &#125;&#125;; return (_Left &lt; _Right)对于一个自定义结构体而言程序不知道&lt;是什么操作，因此需要要么在结构体中重载&lt;符号，要么重写less函数，因此有两种写法。第一种: 12345678910struct Node&#123; int val; Node(int val) :val(val) &#123;&#125; bool operator &lt;(Node a) const &#123; return val &lt; a.val; &#125; bool operator &gt;(Node a) const &#123; return val &gt; a.val; &#125;&#125;;``` 第二种: #include #includeusing namespace std;struct Node{ int val; Node(int val) :val(val) {} };struct cmp{ bool operator()(Node a, Node b) { return a.val &lt; b.val; }};int main(){ priority_queue&lt;Node,vector,cmp&gt; A; //大根堆 priority_queue&lt;Node, vector, less &gt; B; //小根堆 A.push(Node(1)); A.push(Node(9)); A.push(Node(4)); cout &lt;&lt; A.top().val &lt;&lt; endl; A.pop(); cout &lt;&lt; A.top().val &lt;&lt; endl; A.pop(); cout &lt;&lt; A.top().val &lt;&lt; endl; A.pop(); return 0;}`","categories":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"C++参数传递方式","slug":"parameter_passing","date":"2018-12-20T08:05:09.648Z","updated":"2018-10-29T02:50:04.119Z","comments":true,"path":"2018/12/20/parameter_passing/","link":"","permalink":"http://yoursite.com/2018/12/20/parameter_passing/","excerpt":"","text":"C++函数的三种传递方式为：值传递、指针传递和引用传递 值传递(略)指针传递(实质也是值传递)12345678910void fun(int *x)&#123; *x += 5; //修改的是指针x指向的内存单元值&#125;void main(void)&#123; int y = 0; fun(&amp;y);cout&lt;&lt;&lt;&lt;\\&quot;y = \\&quot;&lt;&lt;y&lt;&lt;endl; //y = 5;&#125; 实质是传递地址的值，即地址的‘值传递’ 引用传递12345678910void fun(int &amp;x)&#123; x += 5; //修改的是x引用的对象值 &amp;x = y;&#125;void main(void)&#123;int y = 0;fun(y);cout&lt;&lt;&lt;&lt;\\&quot;y = \\&quot;&lt;&lt;y&lt;&lt;endl; //y = 5;&#125; 实质是取别名，&amp;在C++中有两种作用，取别名和取地址（C中只有后者）,=号左边是引用，=号右边是取址。123int a=3; int &amp;b=a;//引用 int *p=&amp;a; //取地址 引用传递是C++的特性，值传递和指针传递是C语言中本来就有的方式。","categories":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"为什么使用交叉熵损失函数","slug":"LossFunction","date":"2018-12-20T08:05:09.639Z","updated":"2018-10-29T02:50:04.113Z","comments":true,"path":"2018/12/20/LossFunction/","link":"","permalink":"http://yoursite.com/2018/12/20/LossFunction/","excerpt":"","text":"为什么使用交叉熵损失函数而不是二次代价函数 本文基本转自这篇文章，感谢作者 前奏 作为神经网络，应当具有自学习的能力，为了更好的模拟人学习的过程，神经网络的学习能力应当能够自我调整，当发现自己犯的错误越大时，改正的力度就越大。比如投篮：当运动员发现自己的投篮方向离正确方向越远，那么他调整的投篮角度就应该越大，篮球就更容易投进篮筐。这所谓的学习能力便体现在损失函数中，常见的损失函数有两种：二次代价函数和交叉熵损失函数，前者主要用在线性回归中，而在神经网络中主要用后者，下面我们来说明为什么。 以一个神经元的二类分类训练为例，进行两次实验,激活函数采用$sigmoid$：输入一个相同的样本数据x=1.0（该样本对应的实际分类y=0）；两次实验各自随机初始化参数，从而在各自的第一次前向传播后得到不同的输出值，形成不同的$Loss$： 在实验1中，随机初始化参数，使得第一次输出值为0.82（该样本对应的实际值为0）;经过300次迭代训练后，输出值由0.82降到0.09，逼近实际值。而在实验2中，第一次输出值为0.98，同样经过300迭代训练，输出值只降到了0.20。从两次实验的代价曲线中可以看出：实验1的代价随着训练次数增加而快速降低，但实验2的代价在一开始下降得非常缓慢；直观上看，初始的误差越大，收敛得越缓慢。 下面计算两种损失函数,eg:$y$是真实值,$\\hat y$是计算值,$z=wx+b$,$\\hat y = \\sigma (z)$ 二次代价损失函数$$C=\\frac{1}{2n}\\sum_x|y-\\hat y |^2$$ 以一个样本为例: $$L=\\frac{(y-\\hat y)^2}{2}$$ 则有 $$\\begin{align}\\frac{\\partial L}{\\partial w} &amp;=\\frac{\\partial L}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial z} \\frac{\\partial z}{\\partial w} \\&amp;=(\\hat y-y) \\sigma\\prime(x)x\\end{align}$$ $$\\begin{align}\\frac{\\partial L}{\\partial w} &amp;=\\frac{\\partial L}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial z} \\&amp;=(\\hat y-y) \\sigma\\prime(x)\\end{align}$$ 其中，z表示神经元的输入，表示激活函数。从以上公式可以看出，w和b的梯度跟激活函数的梯度成正比，激活函数的梯度越大，w和b的大小调整得越快，训练收敛得就越快。而神经网络常用的激活函数为sigmoid函数，该函数的曲线如下所示: 如图所示，实验2的初始输出值（0.98）对应的梯度明显小于实验1的输出值（0.82），因此实验2的参数梯度下降得比实验1慢。这就是初始的代价（误差）越大，导致训练越慢的原因。与我们的期望不符，即：不能像人一样，错误越大，改正的幅度越大，从而学习得越快。 交叉熵损失函数$$C=\\frac{1}{n}\\sum_x[y\\ln \\hat y+(1-y)\\ln (1-\\hat y)]$$ 以一个样本为例: $$L=-\\sum_x[y\\ln \\hat y+(1-y)\\ln (1-\\hat y)]$$ 则有 $$\\begin{align}\\frac{\\partial L}{\\partial w}&amp;=-\\sum_x[\\frac{y}{\\hat y}-\\frac{(1-y)}{1-\\hat y}] \\frac{\\partial \\hat y}{\\partial z}\\frac{\\partial z}{\\partial w}\\&amp;=-\\sum_x[\\frac{y}{\\hat y}-\\frac{(1-y)}{1-\\hat y}]\\sigma \\prime(x)x\\&amp;=-\\sum_x[\\frac{y- \\sigma (x)}{\\sigma(x)(1-\\sigma(x))}] \\sigma \\prime(x) x \\&amp;= -\\sum_x[y-\\sigma(x)]x\\end{align}$$ $$\\begin{align}\\frac{\\partial L}{\\partial b}&amp;=-\\sum_x[y-\\sigma(x)]\\end{align}$$ 因此，$w$的梯度公式中$\\sigma \\prime (z)$原来的被消掉了;另外，该梯度公式中的表示输出值与实际值之间的误差。所以，当误差越大，梯度就越大，参数$w$调整得越快，训练速度也就越快,$b$的梯度同理. 交叉熵函数来源以$w$的偏导为例:在二次代价函数中: $$\\frac{\\partial L}{\\partial w} = (\\hat y - y) \\sigma \\prime(x) x$$ 为了消除$\\sigma \\prime(x)$,我们令要计算的$w$偏导为: $$\\frac{\\partial L}{\\partial w} = (\\hat y - y) x$$ 而$w$偏导实际计算为: $$\\begin{align}\\frac{\\partial L}{\\partial w} &amp;=\\frac{\\partial L}{\\partial \\hat y}\\sigma \\prime(x)x \\&amp;=\\frac{\\partial L}{\\partial \\hat y}\\hat y(1-\\hat y) x \\\\end{align}$$ 则: $$\\frac{\\partial L}{\\partial \\hat y}\\hat y(1-\\hat y) x= (\\hat y - y) x$$ x被消掉得: $$\\frac{\\partial L}{\\partial \\hat y} = \\frac{\\hat y-y}{(1-\\hat y)\\hat y}$$ 求积分得: $$L=-[y \\ln \\hat y +(1-y) \\ln (1- \\hat y)]$$ 即交叉熵函数.","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"CNN理解","slug":"CNN","date":"2018-12-20T08:05:09.630Z","updated":"2018-10-29T02:50:04.112Z","comments":true,"path":"2018/12/20/CNN/","link":"","permalink":"http://yoursite.com/2018/12/20/CNN/","excerpt":"","text":"CNN的卷积层和普通神经网络全连接层对比 为了过渡，首先看一下这篇笔记 举例: 只有一个数据:$6\\times6$,$Filter:3\\times3$,如图所示: ($filter$是随机初始化的，一般开始并不是左图这样 全连接神经网络 参数数量为 $(6\\times 6+1)\\times n$ CNN 参数数量为 $(3\\times 3+1)\\times n$ 对比可以很明显的看出，其实两者相差并不很多，其实feather map数量就是全连接层中的神经元数量，由于每个神经元所含参数只有9个，不能像全连接(每个神经元36个参数)那样按照矩阵相乘想乘得到一个数字，因此是用卷积的方式得到一个feather map，卷积过程中36个数据共享9个参数。 其他1.很明显，通过卷积+池化+全连接层也能反向传播 其实能学到东西，关键在于初始化$Weights$，在普通全连接神经网络中: 12345678910def add_layer(inputs, in_size, out_size, activation_function=None): Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases output = tf.nn.dropout(output, keep_prob) if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs 在cnn的卷积层中: 1234567def conv2d(input, shape, activation_function): Weights = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1)) biases = tf.constant(0.1, shape=[1, shape[3]]) convOutput = tf.nn.conv2d(input, filter=Weights, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;) + biases if activation_function: convOutput = activation_function(convOutput) return convOutput 也就是说，同一层中多个神经元的参数初始化是随机的，千万不能初始化为同一个值，这样会导致每个神经元的输出永远相同，也就是每个神经元学到的东西是相同的！！！每个神经元初始化Weights的不同是每个神经元学到不同特征的前提.","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"各种熵","slug":"entropy","date":"2018-12-20T08:05:09.623Z","updated":"2018-10-29T02:50:04.112Z","comments":true,"path":"2018/12/20/entropy/","link":"","permalink":"http://yoursite.com/2018/12/20/entropy/","excerpt":"","text":"熵(信息熵)、交叉熵和相对熵(KL散度)、条件熵 熵(信息熵) 信息熵有两个含义:1.系统包含的信息量的期望 2.定量描述该系统所需的编码长度的期望 公式推导定性推导:设$h(x)$为$x$包含的信息量,如果我们有俩个不相关的事件$x$和$y$,那么我们观察到的俩个事件同时发生时获得的信息量应该等于观察到的事件各自发生时获得的信息量之和,即:$h(x,y) = h(x) + h(y)$,由于$x$,$y$是俩个不相关的事件，则满足$p(x,y) = p(x)*p(y)$.那么要想让$h(x,y) = h(x) + h(y)$,$h(x)$就只能是$\\log p(x)$,为了让信息量为非负,我们在其前面加负号，得到信息量公式: $$h(x) = -\\log p(x)$$ 定量推导:参考该博客这个很牛逼！！！！！开始推导：首先说明信息量的定义，谨记x是个概率值，不是事件: 1.信息量是概率的递减函数，记为$f(x)$,$x\\in[0,1]$2.$f(1)=0,f(0)=+∞$3.独立事件(概率之积)的信息量等于各自信息量之和:$f (x_1*x_2)=f(x_1)+f(x_2),x_1,x_2\\in[0,1]$ $$\\begin{align}f(x)^\\prime &amp;= \\lim_{\\Delta x \\to 0}\\frac{f(x+\\Delta x)-f(x)}{\\Delta x} \\&amp;= \\lim_{\\Delta x \\to 0}\\frac{f(\\frac{x+\\Delta x}{x}*x)-f(x)}{\\Delta x} \\&amp;= \\lim_{\\Delta x \\to 0}\\frac{f(x+\\Delta x)+f(x)-f(x)}{\\Delta x} \\&amp;=\\lim_{\\Delta x \\to 0}\\frac{f(\\frac{x+\\Delta x}{x})}{\\Delta x} \\&amp;= \\frac{1}{x}\\lim_{\\Delta x \\to 0}\\frac{f(1+\\frac{\\Delta x}{x})}{\\frac{\\Delta x}{x}} \\&amp;=\\frac{1}{x}f(1)^\\prime\\end{align}$$ 积分得: $$f(x) = f(1)^\\prime\\ln|x|+C \\qquad x\\in[0,1]$$ 令$x=1$,得$C=0$,故 $$\\begin{align}f(x) &amp;= f(1)^\\prime\\ln x \\ &amp;=f(1)^\\prime\\frac{\\log_ax}{\\log_ae} \\ &amp;= \\frac{f(1)^\\prime}{\\log_ae} *\\log_ax\\end{align}$$ 而$\\frac{f(1)^\\prime}{\\log_ae}$是个常数，令为1，则 $$\\begin{align}f(x) &amp;= \\log_ax \\ &amp;= -\\log_a\\frac{1}{x}\\end{align}$$ 证毕!!! 编码推导:参考此处首先需要知道的是Karft不等式:$\\sum r^{-l_i}\\le1$,其中$r$是进制数，一般取二进制,$l_i$表示第$i$个信息的码长.问题可以转化为: $$\\min_{l_i}\\sum p_il_i \\s.t.\\quad \\sum r^{-l_i}\\le1$$ 由拉格朗日乘数法: $$L(l_i,\\lambda)=\\sum p_il_i+\\lambda(\\sum r^{-l_i}-1)$$ 根据拉格朗日法则求极值得: $$\\begin{cases}p_i-\\lambda*r^{-l_i}\\ln r =0 \\\\sum r^{-l_i}-1=0 \\\\end{cases}$$ 由上面公式得: $$\\begin{cases}r^{-l_i}= \\frac{p_i}{\\lambda * \\ln r} \\\\sum r^{-l_i}=1 \\\\end{cases}$$ 一式带入二式得:$\\sum r^{-l_i} = \\sum \\frac{p_i}{\\lambda \\ln r} = \\frac{1}{\\lambda \\ln r} = 1$ 因此:$\\lambda = \\frac{1}{\\ln r}$ 最终得:$l_i = \\log r \\frac{\\lambda * \\ln r}{p_i}=\\log r\\frac{1}{p_i}$，正好是熵!!! 则公式信息熵: $$H(x) = -p(x)\\log p(x)$$ 自然是系统信息量的期望，或称为编码长度的期望. 交叉熵和相对熵(KL散度)现在有两个分布，真实分布p和非真实分布q，我们的样本来自真实分布p,按照真实分布p来编码样本所需的编码长度的期望为(信息熵): $$H(p) = \\sum -p\\log p$$ 按照不真实分布q来编码样本所需的编码长度的期望为(交叉熵): $$H(p,q)= \\sum -p\\log q$$ 注意:$H(p,q)≠H(q,p)!!!$而KL散度(相对熵)为: $$H(p||q)=H(p,q)-H(p)$$ 它表示两个分布的差异，差异越大，相对熵越大。 条件熵条件熵针对得是某个特征范围内来计算，而不是整个样本，即熵是整个样本的不确定性，而条件熵是特定特征条件下样本的不确定性。案例见此,公式表达为: $$\\begin{align}H(Y|X) &amp;=\\sum_{x\\in X}p(X)H(Y|X=x) \\ &amp;=\\sum _{x,y}-p(x,y)\\log p(y|x)\\end{align}$$ X表示总体样本的某个特征，条件熵和其他熵最大的不同就是条件熵和特征有关，其他熵只和Label有关，和特征无关。 联合熵表示X和Y是同一个分布中的两个特征，没有特征和Label之分. $$H(X,Y) = \\sum_{x,y}-p(x,y)\\log p(x,y)$$ 联合熵和条件熵的关系: $$\\begin{align}H(X,Y) - H(Y|X) &amp;= -\\sum_{x,y}p(x,y)\\log p(x,y) + \\sum_{x,y}p(x,y)\\log p(y|x) \\&amp;= -\\sum_{x,y}p(x,y)\\log p(x) \\&amp;=-\\sum_xp(x)\\log p(x) \\&amp;=H(X)\\end{align}$$ 注意，要和条件熵的符号区分。 互信息(信息增益)$$\\begin{align}I(X,Y) &amp;= H(X)-H(X|Y) \\ &amp;= H(Y)-H(Y|X) \\ &amp;= H(X)+H(Y) - H(X,Y) \\ &amp;= H(X,Y) - H(X|Y) -H(Y|X)\\end{align}$$","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"线性回归和逻辑回归对比","slug":"regression","date":"2018-12-20T08:05:09.597Z","updated":"2018-10-29T02:50:04.144Z","comments":true,"path":"2018/12/20/regression/","link":"","permalink":"http://yoursite.com/2018/12/20/regression/","excerpt":"","text":"温故而知新啊，今天复习线性回归和逻辑回归，发现了以前没想过的东西,即为什么逻辑回归要用交叉熵函数。 简单对比1.最终函数 线性回归: $$f(x)_{w,b}=wx+b$$ 逻辑回归: $$P(Y=1|X)=\\frac{e^{wx+b}}{1+e^{wx+b}}\\P(Y=0|X)=\\frac{1}{1+e^{wx+b}}$$ 令: $$f(x)_{w,b}=P(1|X)=\\frac{e^{wx+b}}{1+e^{wx+b}}$$ 2.损失函数 线性回归(注意谁减谁): $$L(w,b)=\\frac{1}{2}\\sum_{i=1}^N(y_i-f(x_i))^2$$ 逻辑回归(注意谁前谁后): $$L(w,b)=\\sum_{i=1}^NC(f(x_i),y_i)$$ 3.梯度计算: 线性回归: $$\\nabla_wL(w,b)=\\sum_{i=1}^N(y_i-f(x_i))(-x_i)\\\\nabla_bL(w,b)=\\sum_{i=1}^N(y_i-f(x_i))$$ 逻辑回归: $$\\begin{align}\\sum_{i=1}^NC(f(x_i),y_i) &amp;= \\sum_{i=1}^N-[y_i\\ln(f(x_i))+(1-y_i)\\ln(1-f(x_i))]\\ &amp;= -\\sum_{i=1}^N[y_i\\ln(\\frac{f(x_i)}{1-f(x_i)})-\\ln(1-f(x_i))]\\ &amp;= -\\sum_{i=1}^N[y_i(wx_i+b)-\\ln(1-f(x_i))]\\ &amp;= -\\sum_{i=1}^N[y_i(wx_i+b)-\\ln(1+e^{wx+b})]\\end{align}$$ $$\\nabla_wL(w,b)=-\\sum_{i=1}^N(y_i-f(x_i))x_i\\\\nabla_bL(w,b)=-\\sum_{i=1}^N(y_i-f(x_i))$$ 至此发现线性回归和逻辑回归的参数偏导公式完全相同，然后梯度上升或下降即可(上升还是下降取决于线性回归谁减谁，逻辑回归交叉熵谁先谁后)。 交叉熵含义对于 $$T={(x_1,y_1),(x_2,y_2),…(x_N,y_N)}$$ 要想让得到回归函数$f(x)$最符合要求,只需使后验概率概率最大即可: $$\\prod_{i=1}^N[f(x_i)]^{y_i}[1-f(x_i)]^{1-y_i}$$ 其中,$y_i$是标签为1的数据，这其实是个似然函数,然后取$\\log$: $$\\begin{align}L(w,b) &amp;= \\sum_{i=i}^N[y_i\\log(f(x_i))+(1-y_i)\\log(1-f(x_i))]\\ &amp;= \\sum_{i=i}^N[y_i(wx_i+b)-\\ln(1+e^{wx+b})]\\end{align}$$ 发现$L(w,b)=\\sum_{i=1}^NC(f(x_i),y_i)$,因此，交叉熵的含义其实就是后验概率最大化。","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"有监督","slug":"有监督","permalink":"http://yoursite.com/tags/有监督/"}]},{"title":"朴素贝叶斯","slug":"NaiveBayes","date":"2018-12-20T07:36:51.000Z","updated":"2018-12-20T07:37:33.489Z","comments":true,"path":"2018/12/20/NaiveBayes/","link":"","permalink":"http://yoursite.com/2018/12/20/NaiveBayes/","excerpt":"","text":"朴素贝叶斯简单推导 问题定义设输入空间$\\chi\\subseteq R^n$，为n维空间的集合，输出空间为类标记集合$\\gamma={c_1,c_2,…,c_K}$，输入为特征向量$x\\subset\\chi$，输出为类标记$y\\subset\\gamma$，$X$是输入控件$\\chi$上的随机向量,$Y$是定义在输出空间$\\gamma$,$P(X,Y)$是$X$和$Y$的联合分布，训练数据集: $$T={(x_1,y_1),(x_2,y_2),…(x_N,y_N)}$$ 由$P(X,Y)$独立同分布产生(谨记：$(x_1,y_1)$中的$x_1\\subset\\chi$，即$x_1$是n维)。现在需要用T训练贝叶斯模型，并判断$x’$的标签。 问题探究显然，$x’$的标签可以$y_1$~$y_N$中任何一个，若一定要判断标签也等同于哪个标签的概率最高，即计算: $$y=arg\\max_{c_k}P(Y=c_k|X=x) \\qquad k=1,2,3,…,K \\tag{1}$$ 已知: $$P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\Sigma_kP(X=x|Y=c_k)P(Y=c_k)} \\quad k=1,2,3,…K \\tag{2}$$ 其中，分母对于每个$c_k$都是相同的，略去不计算，只需计算:$P(X=x|Y=c_k)P(Y=c_k)$，即: $$y=arg\\max_{c_k}P(X=x|Y=c_k)P(Y=c_k) \\qquad k=1,2,3,…K \\tag{3}$$ 对于$P(Y=c_k)$只需要在给出的$T$中数出来即可，因此现在只需要计算$P(X=x|Y=c_k)$，这也是整个朴素贝叶斯过程最困难的一步，为什么说困难呢？$P(X=x|y=c_k)$完整表示为$P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},X^{(3)}=x^{(3)},….,X^{(N)}=x^{(N)}|Y=c_k)$若和$P(Y=c_k)$一样只有一个维度，很容易数出来，但这里需要同时考虑N个维度，数的过程是相当复杂的，因此，朴素贝叶斯在此处做了条件独立性假设： $$P(X=x|y=c_k)=\\prod_{i=1}^n(X^{(i)}=x^{(i)}|Y=c_k) \\qquad k=1,2,3,…K \\tag{4}$$ 这样就只需要同时考虑一个维度，简单了许多，最后将$(4)$带入$(3)$中得: $$y=arg\\max_{c_k}\\prod_{i=1}^n(X^{(i)}=x^{(i)}|Y=c_k)P(Y=c_k) \\qquad k=1,2,3,…K \\tag{5}$$ 至此朴素贝叶斯过程结束。 名词谨记1.先验概率:$P(Y=c_k)$，即不管$X$，直接在$T$中数出来的概率 2.后验概率:$arg\\max_{c_k}P(Y=c_k|X=x)$，即考虑$X$的约束下，数出来的概率（注意：两个概率都是针对$Y$的概率） 原理其实前面已经可以说是整个朴素贝叶斯的过程，但是还要继续说明一个问题：$(1)$式是整个朴素贝叶斯要计算的基本公式，我们可以看出，其实该公式就是后验概率，也就是说，朴素贝叶斯是一个后验概率最大化的过程。 ####后验概率最大化的含义先说结论：后验概率最大化的含义就是选择$0-1$损失函数时的期望风险最小化，下面证明这个结论：首先$0-1$损失函数： $$L(Y,f(X))= \\begin{cases}1 &amp; Y ≠ f(X)\\0 &amp; Y=f(X) \\\\end{cases}$$ 其中$Y$和$f(X)$分别是$X$的实际标签和计算所的标签，则期望风险函数为： $$\\begin{align}R_{exp}(f(X)) &amp;= E[L(Y,f(X))] \\ &amp;=\\sum_{x=1}^N\\sum_{k=1}^K[L(Y=c_k,f(X)]P(Y=c_k,X=x_i)\\ &amp;=E_X(\\sum_{k=1}^K[L(Y=c_k,f(X)]P(Y=c_k|X))\\end{align}$$ 意思就是先对确定$X=x$求条件期望,其实就是两层而来来算。这样，针对确定的$X=x$而言： $$\\begin{align}f(x) &amp;= arg\\min_{y\\subset\\chi}\\sum_{k=1}^KL(Y=c_k,f(X)=y)P(Y=c_k,|X=x) \\ &amp;= arg\\min_{y\\subset\\chi}\\sum_{k=1}^KP(Y≠c_k,|X=x) \\ &amp;= arg\\min_{y\\subset\\chi}\\sum_{k=1}^K(1-P(Y=c_k,|X=x)) \\ &amp;= arg\\max_{y\\subset\\chi}\\sum_{k=1}^KP(Y=c_k,|X=x)\\end{align}$$ 由此，我们从期望风险最小化入手，最终得到了后验概率最小化，即朴素贝叶斯的原理。","categories":[{"name":"技术","slug":"技术","permalink":"http://yoursite.com/categories/技术/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"无监督","slug":"无监督","permalink":"http://yoursite.com/tags/无监督/"}]}]}