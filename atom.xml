<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>灵翼俠的个人博客</title>
  
  <subtitle>不做搬运工</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://lingyixia.github.io/"/>
  <updated>2021-10-23T11:15:41.811Z</updated>
  <id>https://lingyixia.github.io/</id>
  
  <author>
    <name>陈飞宇</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>神经网络生成模型学习总结</title>
    <link href="https://lingyixia.github.io/2021/10/20/NetWorkGenerator/"/>
    <id>https://lingyixia.github.io/2021/10/20/NetWorkGenerator/</id>
    <published>2021-10-20T15:59:19.000Z</published>
    <updated>2021-10-23T11:15:41.811Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h1><p>$\qquad$众所周知，神经网络是个<strong>判别模型</strong>，此处所指”生成模型”指的是功能性质，即利用神经网络生成我们需要的数据，比如对话生成/图像生成/语音生成。<br>$\qquad$试想，我们现在想生成一个图片，需要怎么做呢？</p><ol><li>需要知道我们要生成一个什么样图片,比如我们想生成一个卡通头像</li><li>需要找到类似所需卡通图片的很多卡通图像</li><li>用2中的所有图像训练一个模型</li><li>给3中的模型一个输入，得到所需卡通头像</li></ol><h1 id="AutoRegression"><a href="#AutoRegression" class="headerlink" title="AutoRegression"></a>AutoRegression</h1><p><img src="/img/autoregression.png"></p><center>AutoRegression</center><p>自回归是一种Step By Step的方式<br>优点：对对话/语音这类顺序明显的类别比较友好<br>缺点：</p><ol><li>对图像而言，很难确定一个好的生成顺序</li><li>在inference阶段，当前步骤严重依赖前一步骤</li><li>慢<h1 id="AutoEncoder"><a href="#AutoEncoder" class="headerlink" title="AutoEncoder"></a>AutoEncoder</h1><img src="/img/autoencoder.png"><center>AutoEncoder</center></li></ol><p>优点:</p><ol><li>无需Step By Step，可以并行生成</li><li>生成数据更规则，一板一眼。</li></ol><p>缺点：loss是对元素级别的监督度量，全局信息关注不足，因此模型泛化不够，两个非常接近的输入，可能输出天差地别。</p><h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><p><img src="/img/gan.png"></p><p><center>GAN</center></p><ol><li>从任意分布（如高斯分布）sample一个向量，输入Generator得到和所需图片相同size的输出</li><li>把1中的输出当作副样本，从训练数据中sample一个正样本，以二分类的形式训练Discriminator(freeze Generator)。</li><li>重新sample一个向量，输入Generator得到和所需图片相同size的输出，将其当作正样本输入Discriminator，计算loss 训练Generator(freeze Discriminator)<br>上诉三个步骤是一个轮次。<br>这里面有两个问题：</li><li>为什么Generator不自己学？<br>如果这样做的话训练应该是这样一个流程：<br>从任意分布（如高斯分布）sample一个向量，输入Generator得到和所需图片相同size的输出，从训练数据中sample一个图像，和该输出计算loss并反向传播。<br>答：Generatory自己学其实就是训练AutoDecoder的Decoder部分，这样计算loss的时候只是对元素级别的监督度量，全局信息关注不足。</li><li>为什么</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;生成模型&quot;&gt;&lt;a href=&quot;#生成模型&quot; class=&quot;headerlink&quot; title=&quot;生成模型&quot;&gt;&lt;/a&gt;生成模型&lt;/h1&gt;&lt;p&gt;$\qquad$众所周知，神经网络是个&lt;strong&gt;判别模型&lt;/strong&gt;，此处所指”生成模型”指的是功能性质，即利用
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Lipschitz连续条件</title>
    <link href="https://lingyixia.github.io/2021/09/05/Lipschitz/"/>
    <id>https://lingyixia.github.io/2021/09/05/Lipschitz/</id>
    <published>2021-09-05T08:00:20.000Z</published>
    <updated>2021-09-19T18:03:58.740Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lipschitz约束"><a href="#Lipschitz约束" class="headerlink" title="Lipschitz约束"></a>Lipschitz约束</h2><p>$\qquad$如果对于函数$f_w(x)$的定义域内任意输入$(x_1,f_w(x_1)),(x_2,f_w(x_2))$都满足存在一个常数$L_w$使得：</p><script type="math/tex; mode=display">||f_w(x_1)-f_w(x_2)|| \leq L_w||x_1-x_2|| \tag{1}</script><p>$\qquad$则称$f_w(x)$满足利普希茨连续条件,其中，最小的$L_w$叫做Lipschitz常数。可以看到，Lipschitz连续条件约束的是函数的<strong>范数</strong>，当$f_w(x)$是实值函数时，该范数即是绝对值，该公示可以简单理解为，一个函数的一阶导数有界。</p><h1 id="模型鲁棒性"><a href="#模型鲁棒性" class="headerlink" title="模型鲁棒性"></a>模型鲁棒性</h1><p>$\qquad$做算法的会经常看到鲁棒性这个词，用来描述算法的抗干扰能力，这个性质就是说一个模型，对于两个很接近的输入，其输出也必须很接近，就说其鲁棒性强，用数学语言描述：</p><script type="math/tex; mode=display"> \lim\limits_{\Delta x\to+0} f_w(x+\Delta x) -f_w(x)\rightarrow 0 \tag{2}</script><h1 id="Lipschitz与激活函数"><a href="#Lipschitz与激活函数" class="headerlink" title="Lipschitz与激活函数"></a>Lipschitz与激活函数</h1><p>$\qquad$大家知道,我们常用的激活函数有这些$sigmoid$、$relu$,$tanh$,如果问为什么需要激活函数，回到应该是非线性化，那如果问为什么常用的激活函数是这三个呢？$x^3$行不行？<br>$\qquad$ 答案是不行，从Lipschitz的角度分析，这三个激活函数不仅仅是能够做到非线性化，而且其一阶导函数是有界的，如果使用$x^3$，当输入一个很大的数据$x$ 时，其一阶导函数必然非常大，则$f_w(x+\Delta x)-f_w(x)$也必然很大，则在模型看来，两个很接近的数据其输出却差这么多，这样学下去模型很容易废掉。<br>$\qquad$ 而这三个激活函数不仅保证一阶函数有界，而且在最大导数点在原点，还能更方便的和Normalization结合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lipschitz约束&quot;&gt;&lt;a href=&quot;#Lipschitz约束&quot; class=&quot;headerlink&quot; title=&quot;Lipschitz约束&quot;&gt;&lt;/a&gt;Lipschitz约束&lt;/h2&gt;&lt;p&gt;$\qquad$如果对于函数$f_w(x)$的定义域内任意输入$(x
      
    
    </summary>
    
      <category term="数学" scheme="https://lingyixia.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>n维空间任意两向量夹角分布</title>
    <link href="https://lingyixia.github.io/2021/08/26/ndimvectorangle/"/>
    <id>https://lingyixia.github.io/2021/08/26/ndimvectorangle/</id>
    <published>2021-08-26T06:29:23.000Z</published>
    <updated>2021-09-19T18:01:25.961Z</updated>
    
    <content type="html"><![CDATA[<h1 id="命题"><a href="#命题" class="headerlink" title="命题"></a>命题</h1><p>本文要证明的是高<strong>n维空间中，任意两个向量都几乎垂直</strong>，注意：不是两个向量垂直的概率大，而是”几乎”垂直，即夹角接近$\frac{π}{2}$。基本思路就是考虑两个随机向量的夹角$\theta$分布，然后求导得到概率密度，就可以看出在$\theta$在哪个范围内最大。</p><h1 id="命题重定义"><a href="#命题重定义" class="headerlink" title="命题重定义"></a>命题重定义</h1><p>随机两个向量不好求，我们可以先固定一个，让另一个随机即可，假设固定向量为：</p><script type="math/tex; mode=display">x=(1,0,...,0) \tag{1}</script><p>随机向量为：</p><script type="math/tex; mode=display">y=(y_1,y_2,...,y_n) \tag{2}</script><p>现在我们把原命题重新定义为<strong>n维单位超球面上，任意一个点与原点组成的单位向量和$(1,0,0,…,0)$向量都几乎垂直</strong><br>直接计算可以得到:</p><script type="math/tex; mode=display">cos \langle x,y \rangle=\frac{x_1}{\sqrt(x_1^2+x_2^2+...x_n^2)} \tag{3}</script><p>现在要求的就是公式(3)的概率分布和密度，到这里还是一筹莫展。</p><h1 id="球坐标系"><a href="#球坐标系" class="headerlink" title="球坐标系"></a>球坐标系</h1><p>将$y$直角坐标转为球坐标系后为：</p><script type="math/tex; mode=display">\left\{    \begin{aligned}    y_1 & =  cos(\varphi_1) \\    y_2 & =  sin(\varphi_1)cos(\varphi_2) \\    y_3 &= sin(\varphi_1)sin(\varphi_2) cos(\varphi_3) \\    .    .    .\\    y_{n-1} &= sin(\varphi_1)sin(\varphi_2)...sina(\varphi_{n-2}) cos(\varphi_{n-1}) \\    y_{n} &= sin(\varphi_1)sin(\varphi_2)...sina(\varphi_{n-2}) sin(\varphi_{n-1})    \end{aligned}\right. \tag{4}</script><p>其中，$\varphi_{n-1} \in[0,2π]$,  $\varphi_{0…n-2} \in[0,π]$<br>此时,公式(3)中$cos \langle x,y \rangle$恰好等于$\varphi_1$,即两者之间的角度就是$\varphi_1$</p><script type="math/tex; mode=display">P_n(\varphi_1<\theta)=\frac{n维超球面上\varphi_1<\theta 部分面积(可以想像为三维球体的环带)}{n维超球体表面积} \tag{5}</script><p>n维超球面上的积分微元是$\sin^{n−2}(\varphi_1)\sin^{n−3}(\varphi_2)⋯sin(\varphi_{n−2})d\varphi1d\varphi2⋯d\varphi_{n−2}d\varphi_{n−1}$<br>因此n维球面积积分为:</p><script type="math/tex; mode=display">\begin{align*}S_n&=\int_0^{2π}d\varphi_{n−1}\int_0^πsin(\varphi_{n−2})d\varphi_{n−2}...\int_0^π\sin^{n−3}(\varphi_2)d\varphi_2\int_0^π\sin^{n−2}(\varphi_1)d\varphi_1 \tag{6}\end{align*}</script><p>故:</p><script type="math/tex; mode=display">\begin{align*}P_n(\varphi_1<\theta) &= \frac{n维超球面上\varphi_1<\theta 部分面积}{n维超球体表面积} \\&=\frac{\int_0^{2π}\int_0^π...\int_0^π \int_0^\theta \sin^{n−2}(\varphi_1)\sin^{n−3}(\varphi_2)⋯sin(\varphi_{n−2})d\varphi1d\varphi2⋯d\varphi_{n−2}d\varphi_{n−1}}{\int_0^{2π}\int_0^π...\int_0^π \int_0^π\sin^{n−2}(\varphi_1)\sin^{n−3}(\varphi_2)⋯sin(\varphi_{n−2})d\varphi_1d\varphi_2⋯d\varphi_{n−2}d\varphi_{n−1}} \\&=\frac{\int_0^{2π}d\varphi_{n−1}\int_0^πsin(\varphi_{n−2})d\varphi_{n−2}...\int_0^π\sin^{n−3}(\varphi_2)d\varphi_2\int_0^\theta\sin^{n−2}(\varphi_1)d\varphi_1}{\int_0^{2π}d\varphi_{n−1}\int_0^πsin(\varphi_{n−2})d\varphi_{n−2}...\int_0^π\sin^{n−3}(\varphi_2)d\varphi_2\int_0^π\sin^{n−2}(\varphi_1)d\varphi_1} \\&=\frac{(n−1)维单位超球的表面积 \times \int_0^\theta\sin^{n−2}(\varphi_1)d\varphi_1}{n维单位超球的表面积} \\&=\frac{\Gamma(\frac{n}{2})}{\Gamma(\frac{n-1}{2})\times \sqrt(π)} \times \int_0^\theta\sin^{n−2}(\varphi_1)d\varphi_1 \tag{7}\end{align*}</script><blockquote><blockquote><p>小插曲:<br>n维球体积:$V_n=\frac{\pi^{\frac{n}{2}}}{\Gamma(n/2)}r^{n-1}$,n维球面积$S_n=\frac{2\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}r^{n-1}=V_n’$</p></blockquote></blockquote><p>因此,  概率密度为:</p><script type="math/tex; mode=display">p_n(\theta)=P_n(\varphi_1<\theta)'=\frac{\Gamma(\frac{n}{2})}{\Gamma(\frac{n-1}{2})\times \sqrt(π)} \times \sin^{n−2} \tag{8}\theta</script><h1 id="密度函数图像"><a href="#密度函数图像" class="headerlink" title="密度函数图像"></a>密度函数图像</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding=utf-8</span><br><span class="line"></span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import seaborn as sns</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fun(n, x):</span><br><span class="line">    return (math.gamma(n / 2) * math.pow(math.sin(x), n - 2)) / (math.sqrt(math.pi) * math.gamma((n - 1) / 2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    datas_dict = dict()</span><br><span class="line">    xs = np.arange(0, 3, 0.01)</span><br><span class="line">    ys = list()</span><br><span class="line">    for n in [2, 3, 5, 10, 50, 100]:</span><br><span class="line">        for x in xs:</span><br><span class="line">            ys.append(fun(n, x))</span><br><span class="line">        datas_dict[str(n)] = ys.copy()</span><br><span class="line">        ys.clear()</span><br><span class="line">    data = pd.DataFrame(datas_dict, index=list(xs))</span><br><span class="line">    sns.lineplot(data=data)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/nvector.jpeg" alt title>                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;命题&quot;&gt;&lt;a href=&quot;#命题&quot; class=&quot;headerlink&quot; title=&quot;命题&quot;&gt;&lt;/a&gt;命题&lt;/h1&gt;&lt;p&gt;本文要证明的是高&lt;strong&gt;n维空间中，任意两个向量都几乎垂直&lt;/strong&gt;，注意：不是两个向量垂直的概率大，而是”几乎”垂直，即夹
      
    
    </summary>
    
      <category term="数学" scheme="https://lingyixia.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>SpatialPyramidPooling</title>
    <link href="https://lingyixia.github.io/2021/06/25/SpatialPyramidPooling/"/>
    <id>https://lingyixia.github.io/2021/06/25/SpatialPyramidPooling/</id>
    <published>2021-06-25T02:27:27.000Z</published>
    <updated>2021-09-19T18:01:25.948Z</updated>
    
    <content type="html"><![CDATA[<p>中文名：空间金字塔卷积，通常卷积之后会跟一个全链接层，这就造成如果输入图片size不同，卷积之后size也不同，输入全链接层的feature就不同，这样压根不能运行，通常的解决方案是通过clip/wrap预先将图片处理成相同size，现在可以用该方法来代替。<br><a id="more"></a></p><h1 id="赘述"><a href="#赘述" class="headerlink" title="赘述"></a>赘述</h1><p>通常的CNN结构是这样的：</p><script type="math/tex; mode=display">inputs \rightarrow clip/wrap \rightarrow CNN \rightarrow flat \rightarrow dense</script><p>加入ssp后结构是这样的:</p><script type="math/tex; mode=display">inputs \rightarrow CNN \rightarrow ssp \rightarrow dense</script><h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><ul><li><p>原始：<br>$8 \times 204 \times 196 \times 3 \stackrel{cnn}{\rightarrow}  8 \times 13 \times 14 \times 256 \stackrel{flat}{\rightarrow} 8 \times 46592$<br>$8 \times 302 \times 197 \times 3 \stackrel{cnn}{\rightarrow}  8 \times 19 \times 14 \times 256  \stackrel{flat}{\rightarrow} 8 \times 68096$</p><blockquote><blockquote><p>显然两者最后的输出不可能输入在同一个dense中训练。</p></blockquote></blockquote></li><li><p>clip/warp<br>$8 \times 204 \times 196 \times 3  \stackrel{clip}{\rightarrow} 8 \times 224 \times 224 \times 3  \stackrel{cnn}{\rightarrow} 8 \times 13 \times 13 \times 256  \stackrel{flat}{\rightarrow} 8 \times 43264$<br>$8 \times 302 \times 197 \times 3  \stackrel{clip}{\rightarrow} 8 \times 224 \times 224 \times 3  \stackrel{cnn}{\rightarrow} 8 \times 13 \times 13 \times 256  \stackrel{flat}{\rightarrow} 8 \times 43264$</p></li><li>ssp<br>$8 \times 204 \times 196 \times 3 \stackrel{cnn}{\rightarrow}  8 \times 13 \times 14 \times 256 \stackrel{ssp}{\rightarrow} 8 \times 5376$<br>$8 \times 302 \times 197 \times 3 \stackrel{cnn}{\rightarrow}  8 \times 19 \times 14 \times 256  \stackrel{ssp}{\rightarrow} 8 \times 5376$</li></ul><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def SpatialPyramidPooling(previous_conv, out_pool_size_list):</span><br><span class="line">    b, w, h, c = previous_conv.shape</span><br><span class="line">    for index, pool_size in enumerate(out_pool_size_list):</span><br><span class="line">        w_wid = tf.cast(tf.math.ceil(w / pool_size), tf.int64)</span><br><span class="line">        h_wid = tf.cast(tf.math.ceil(h / pool_size), tf.int64)</span><br><span class="line">        max_pooling = tf.keras.layers.MaxPooling2D(pool_size=(w_wid, h_wid), strides=(w_wid, h_wid), padding=&apos;same&apos;)</span><br><span class="line">        result = tf.reshape(max_pooling(previous_conv), (b, -1))</span><br><span class="line">        spp = result if index == 0 else tf.concat([spp, result], axis=-1)</span><br><span class="line">    return spp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    inputs = tf.random.normal(shape=(8, 19, 14, 256))</span><br><span class="line">    result = SpatialPyramidPooling(inputs, out_pool_size_list=[4, 2, 1])</span><br><span class="line">    print(result.shape)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;中文名：空间金字塔卷积，通常卷积之后会跟一个全链接层，这就造成如果输入图片size不同，卷积之后size也不同，输入全链接层的feature就不同，这样压根不能运行，通常的解决方案是通过clip/wrap预先将图片处理成相同size，现在可以用该方法来代替。&lt;br&gt;
    
    </summary>
    
      <category term="神经网络" scheme="https://lingyixia.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="卷积" scheme="https://lingyixia.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>相对编码</title>
    <link href="https://lingyixia.github.io/2021/04/04/realtive-position-transformer/"/>
    <id>https://lingyixia.github.io/2021/04/04/realtive-position-transformer/</id>
    <published>2021-04-04T15:16:54.000Z</published>
    <updated>2021-09-19T18:01:25.963Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Relative-Position-Embedding"><a href="#Relative-Position-Embedding" class="headerlink" title="Relative Position Embedding"></a>Relative Position Embedding</h1><p><a href="https://arxiv.org/pdf/1803.02155.pdf" target="_blank" rel="noopener">论文地址</a><br><a href="https://wyydsb.xin/other/relativepositionembed.html" target="_blank" rel="noopener">参考博客1</a><br><a href="https://blog.csdn.net/weixin_41089007/article/details/91477253" target="_blank" rel="noopener">参考博客1</a></p><p>该论文的考虑出发点为原始的编码方式仅仅考虑了位置的<strong>距离</strong>关系，没有考虑位置的<strong>先后</strong>关系，本抛弃了<code>vanilla transformer</code>中静态位置编码，使用一个可训练的相对位置矩阵来表示位置信息。<br>公示很简单：<br>原始Attention：</p><script type="math/tex; mode=display">e_{ij}=\frac{x_iW_q x_iW_k^T}{\sqrt{d_{model}}}  \\a_{ij} = softmax(w_ij) \\z_i = \sum_{j=1}^n a_{ij}x_jW_v</script><p>Relative Position Attention:</p><script type="math/tex; mode=display">e_{ij}=\frac{x_iW_q (x_iW_k+a_{ij}^k)^T}{\sqrt{d_{model}}}  \\a_{ij} = softmax(w_{ij}) \\z_i = \sum_{j=1}^n a_{ij}(x_jW_v+a_{ij}^v)</script><blockquote><p>其中，$a_{ij}^k$和$a_{ij}^v$分别表示两个可学习的位置信息,至于为什么加在这两个地方，自然是因为这两个地方计算了相对位置。<br>同时，作者发现如果两个单词距离超过某个阈值$k$提升不大，因此在此限制了位置最大距离，即超过$k$的距离也按照$k$距离的位置信息计算。<br>位置信息本质是在训练两个矩阵$W^K=(w_{-k}^K,…,w_k^K)$和$W^V=(w_{-k}^V,…,w_k^V)$</p><script type="math/tex; mode=display">a_{ij}^K=W_{clip(j-i,k)}^K \\a_{ij}^V=W_{clip(j-i,k)}^V \\clip(x,k)=max(-k,min(k,x))</script></blockquote><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">torch.manual_seed(2021)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RelativePosition(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, num_units, max_relative_position):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_units = num_units</span><br><span class="line">        self.max_relative_position = max_relative_position</span><br><span class="line">        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))</span><br><span class="line">        nn.init.xavier_uniform_(self.embeddings_table)</span><br><span class="line"></span><br><span class="line">    def forward(self, length_q, length_k):</span><br><span class="line">        range_vec_q = torch.arange(length_q)</span><br><span class="line">        range_vec_k = torch.arange(length_k)</span><br><span class="line">        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]</span><br><span class="line">        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)</span><br><span class="line">        final_mat = distance_mat_clipped + self.max_relative_position</span><br><span class="line">        final_mat = torch.LongTensor(final_mat)</span><br><span class="line">        embeddings = self.embeddings_table[final_mat]</span><br><span class="line"></span><br><span class="line">        return embeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MultiHeadAttentionLayer(nn.Module):</span><br><span class="line">    def __init__(self, hid_dim, n_heads, dropout, device):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        assert hid_dim % n_heads == 0</span><br><span class="line"></span><br><span class="line">        self.hid_dim = hid_dim</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.head_dim = hid_dim // n_heads</span><br><span class="line">        self.max_relative_position = 2</span><br><span class="line"></span><br><span class="line">        self.relative_position_k = RelativePosition(self.head_dim, self.max_relative_position)</span><br><span class="line">        self.relative_position_v = RelativePosition(self.head_dim, self.max_relative_position)</span><br><span class="line"></span><br><span class="line">        self.fc_q = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc_k = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc_v = nn.Linear(hid_dim, hid_dim)</span><br><span class="line"></span><br><span class="line">        self.fc_o = nn.Linear(hid_dim, hid_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)</span><br><span class="line"></span><br><span class="line">    def forward(self, query, key, value, mask=None):</span><br><span class="line">        # query = [batch size, query len, hid dim]</span><br><span class="line">        # key = [batch size, key len, hid dim]</span><br><span class="line">        # value = [batch size, value len, hid dim]</span><br><span class="line">        batch_size = query.shape[0]</span><br><span class="line">        len_k = key.shape[1]</span><br><span class="line">        len_q = query.shape[1]</span><br><span class="line">        len_v = value.shape[1]</span><br><span class="line"></span><br><span class="line">        query = self.fc_q(query)</span><br><span class="line">        key = self.fc_k(key)</span><br><span class="line">        value = self.fc_v(value)</span><br><span class="line"></span><br><span class="line">        r_q1 = query.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        r_k1 = key.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2))  # q对k元素的attention</span><br><span class="line"></span><br><span class="line">        r_k2 = self.relative_position_k(len_q, len_k)</span><br><span class="line">        attn2 = torch.einsum(&apos;bhqe,qke-&gt;bhqk&apos;, r_q1, r_k2)  # q对k位置的attention</span><br><span class="line">        attn = (attn1 + attn2) / self.scale</span><br><span class="line">        if mask is not None:</span><br><span class="line">            attn = attn.masked_fill(mask == 0, -1e10)</span><br><span class="line">        attn = self.dropout(torch.softmax(attn, dim=-1))</span><br><span class="line">        r_v1 = value.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        weight1 = torch.matmul(attn, r_v1)  # qk对v元素的attention</span><br><span class="line">        r_v2 = self.relative_position_v(len_q, len_v)</span><br><span class="line">        weight2 = torch.einsum(&apos;bhav,ave-&gt;bhae&apos;, attn, r_v2)  # qk对v位置的attention</span><br><span class="line">        x = weight1 + weight2</span><br><span class="line">        x = x.permute(0, 2, 1, 3).contiguous()</span><br><span class="line">        x = x.view(batch_size, -1, self.hid_dim)</span><br><span class="line">        x = self.fc_o(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    multiHeadAttentionLayer = MultiHeadAttentionLayer(128, 8, 0.5, &apos;cpu&apos;)</span><br><span class="line">    x = torch.randn(4, 43, 128)</span><br><span class="line">    result = multiHeadAttentionLayer(x, x, x)</span><br><span class="line">    print(result)</span><br><span class="line">    # x = torch.randn(64, 8, 43, 16)</span><br><span class="line">    # y = torch.randn(43, 43, 16)</span><br><span class="line">    # print(torch.einsum(&apos;bhqe,qke-&gt;bhqk&apos;, [x, y]))</span><br></pre></td></tr></table></figure><h2 id="TENER"><a href="#TENER" class="headerlink" title="TENER"></a>TENER</h2><blockquote><p>本来应该先写Transformer_xl,但还没完全懂，用这个做过度。</p></blockquote><p><a href="https://arxiv.org/pdf/1911.04474.pdf" target="_blank" rel="noopener">论文地址</a><br><a href="https://blog.csdn.net/Rock_y/article/details/109123472" target="_blank" rel="noopener">参考博客1</a><br><a href="https://www.cnblogs.com/shiyublog/p/11236212.html" target="_blank" rel="noopener">参考博客2</a><br>该论文用在<code>NER</code>任务中，同样的，主要目的也是解决普通<code>Transformer</code>只有<strong>距离</strong>没有<strong>先后</strong>的问题，主要思路是回归了<code>vanilla transformer</code>的编码方式，但是将静态<strong>相对位置</strong>信息加入其中，其实是一个不可训练的相对位置矩阵(具体可看代码，其实表示思路和第一篇相似)。<br>相对位置矩阵为：</p><script type="math/tex; mode=display">R_{t-j}=[...,sin(\frac{t-j}{10000^{2i/d_{model}}}),cos(\frac{t-j}{10000^{2i/d_{model}}})]</script><blockquote><p>可以看到，对于sin而言，正负号是有影响的，但是cos无影响。</p></blockquote><p>原始<code>Transformer</code>中<code>Attention score</code>计算公示为:</p><script type="math/tex; mode=display">A_{i,j}=\underbrace{E_i^TW_qW_kE_j}_a+\underbrace{E_i^TW_qW_kU_j}_b+ \underbrace{U_i^TW_qW_kE_j}_c+\underbrace{U_i^TW_qW_kU_j}_d</script><blockquote><blockquote><p>解释：<br>a:第i个单词<strong>内容</strong>对第j个单词<strong>内容</strong>的score<br>b:第i个单词<strong>内容</strong>对第j个单词<strong>位置</strong>的score<br>c:第i个单词<strong>位置</strong>对第j个单词<strong>内容</strong>的score<br>d:第i个单词<strong>位置</strong>对第j个单词<strong>位置</strong>的score</p></blockquote></blockquote><p>更改后<code>Attention score</code>计算公示为：</p><script type="math/tex; mode=display">A_{i,j}=Q_tK_j^T+Q_tR_{i-j}^T+uK_j^T+vR_{i-j}</script><p>这个看着不方便，其实就是Transformer_xl的公示，应该为：</p><script type="math/tex; mode=display">A_{i,j}=\underbrace{E_i^TW_qW_{k,E}E_j}_a+\underbrace{E_i^TW_qW_{k,R}R_{i-j}}_b+ \underbrace{u^TW_kE_j}_c+\underbrace{v^TW_kU_j}_d</script><blockquote><blockquote><p>不同于之前，这里的四项是分开计算的，其中a不变，b其实就是第一篇论文的$e_{ij}$计算部分，很显然，第一篇论文少了c和d</p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Relative-Position-Embedding&quot;&gt;&lt;a href=&quot;#Relative-Position-Embedding&quot; class=&quot;headerlink&quot; title=&quot;Relative Position Embedding&quot;&gt;&lt;/a&gt;Relat
      
    
    </summary>
    
      <category term="深度学习" scheme="https://lingyixia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Attention" scheme="https://lingyixia.github.io/tags/Attention/"/>
    
      <category term="Transformer" scheme="https://lingyixia.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>泊松分布和指数分布</title>
    <link href="https://lingyixia.github.io/2020/09/20/possionAndexp/"/>
    <id>https://lingyixia.github.io/2020/09/20/possionAndexp/</id>
    <published>2020-09-20T08:38:57.000Z</published>
    <updated>2021-09-19T18:01:25.963Z</updated>
    
    <content type="html"><![CDATA[<p>一直不理解一些分布是干什么用的，怎么来的，看了<a href="https://www.matongxue.com/search/?q=%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83" target="_blank" rel="noopener">马同学高数</a>才有了一些了解，目前的感觉是，似乎常见的分布都来自于二项分布。此处加以记录。</p><a id="more"></a><h1 id="从二项分布到泊松分布"><a href="#从二项分布到泊松分布" class="headerlink" title="从二项分布到泊松分布"></a><a href="https://blog.csdn.net/ccnt_2012/article/details/81114920" target="_blank" rel="noopener">从二项分布到泊松分布</a></h1><ul><li>已知：馒头老板统计了一周内每天卖的馒头数量</li><li>问题：一个馒头店每天需要准备多少馒头？<br>最简单的办法是求平均值，每天就准备这么多馒头就行了，但是问题是如果每天卖出馒头数量方差较大，很容易有好几天准备不足或准备过剩。</li><li>分析：将一天销售时间均分为$n$个阶段，似的每一阶段只卖出一个馒头，那么，每一阶段卖出与不卖出馒头就是一个<strong>伯努利分布</strong>，$n$个阶段就是<strong>二项分布</strong>。设每天卖出馒头为随机变量X，则一天销售时间卖出$k$个馒头的概率为：<script type="math/tex; mode=display">P(X=K)=C_n^kp^k(1-p)^{n-k} \tag{1}</script>很显然，时间是连续的，不能这样分，但是当$n \to+\infty$的时候，可以认为是连续的。<br>上诉二项分布$p$怎么求呢？由$np=\mu$得$p=\frac{\mu}{p}$,$\mu$可由一周内均值近似，则有：<script type="math/tex; mode=display">\begin{aligned}P(X=k)&=\lim_{n \to +\infty} C_n^k(\frac{\mu}{n})^k(1-\frac{\mu}{n})^{n-k}\\&=\lim_{x \to +\infty} \frac{n(n-1)...(n-k+1)}{k!}\frac{\mu^k}{n^k}(1-\frac{\mu}{n})^{n-k} \\&=\lim_{x \to +\infty}\frac{\mu^k}{k!} \frac{n}{n}\frac{n-1}{n}...\frac{n-k+1}{n}(1-\frac{\mu}{n})^{-k}(1-\frac{\mu}{n})^n\end{aligned}</script>其中$\frac{n}{n} \frac{n-1}{n}…\frac{n-k+1}{n}$和$(1-\frac{\mu}{n})^{-k}$在$n \to +\infty$下都是1，对于最后一个因式:<script type="math/tex; mode=display">\lim_{n \to +\infty}(1-\frac{\mu}{n})^n=\lim_{n\to+\infty}((1+(-\frac{1}{\frac{n}{\mu}}))^{-\frac{n}{\mu}})^{-\mu} = e^{-\mu}</script>故而原式可化为$P(X=k)=\frac{\mu^k}{k!}e^{-\mu}$,即泊松分布的公式。<br>因此，可以认为，泊松分布是描述某段时间内，卖出多少馒头的分布，其中$\mu$代表这段时间内卖出馒头的期望。<h1 id="从泊松分布到指数分布"><a href="#从泊松分布到指数分布" class="headerlink" title="从泊松分布到指数分布"></a>从泊松分布到指数分布</h1>上诉泊松分布只告诉了我们确定时间段内的分布，改造该公式如下：<script type="math/tex; mode=display">P(X=k,t)=\frac{(\mu t)^k}{k!}e^{-\mu t}</script>称之为<strong>波松过程</strong>，可以看出，当$t=1$时就是普通的泊松分布，当$t=n$的时候表示时间间隔为$n$内卖出馒头的分布。<br>对于馒头店老板而言，不仅仅需要知道每天要准备多少馒头，还需要知道卖出馒头的时间间隔，好以此适时调整服务员人数。<br>设随机变量X表示两次卖出馒头之间的间隔，则有：<script type="math/tex; mode=display">P(Y>t)=P(X=0,0)=\frac{(\mu t)^0}{0!}e^{-\mu t}=e^{-\mu t}</script><blockquote><blockquote><p>$P(X=k,t)$表示时间段t内卖出k个馒头的概率，P(Y&gt;t)表示两次卖出馒头时间间隔大于t的概率，则就是时间t内卖出0个馒头的概率。<br>则:</p><script type="math/tex; mode=display">P(Y\le t)=1-e^{-\mu t}</script><p>即指数分布</p></blockquote></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一直不理解一些分布是干什么用的，怎么来的，看了&lt;a href=&quot;https://www.matongxue.com/search/?q=%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;马同学高数&lt;/a&gt;才有了一些了解，目前的感觉是，似乎常见的分布都来自于二项分布。此处加以记录。&lt;/p&gt;
    
    </summary>
    
      <category term="概率论" scheme="https://lingyixia.github.io/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="泊松分布" scheme="https://lingyixia.github.io/tags/%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83/"/>
    
      <category term="指数分布" scheme="https://lingyixia.github.io/tags/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83/"/>
    
  </entry>
  
  <entry>
    <title>神经网络到底保存了什么</title>
    <link href="https://lingyixia.github.io/2020/03/18/NetWorkStorage/"/>
    <id>https://lingyixia.github.io/2020/03/18/NetWorkStorage/</id>
    <published>2020-03-18T06:36:26.000Z</published>
    <updated>2021-09-21T06:29:00.303Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>都知道，无论使用什么框架，神经网络都是非常消耗显存的，那么，这些消耗的显存到底保存了什么？</p></blockquote><h1 id="从一个例子开始"><a href="#从一个例子开始" class="headerlink" title="从一个例子开始"></a>从一个例子开始</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/fullnet.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>标注解释:<br>$z_i^k$:第$k$层第$i$个$feature$的输出(不经过激活函数))<br>$a_i^k$：其值为$g(z_i^k)$,$g(x)$为激活函数，这里把输入层数据$x_i$看作第0层。<br>$w_{ij}^k$: 从第$k-1$层到第$k$层的传播权重<br>这里去掉了偏置权重$b$</p><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播:"></a>正向传播:</h2><script type="math/tex; mode=display">\left\{    \begin{aligned}    z_1^1=w_{11}^1a_1^0+w_{21}^1a_2^0 \\    z_2^1=w_{12}^1a_1^0+w_{22}^1a_2^0 \\    z_3^1=w_{13}^1a_1^0+w_{23}^1a_2^0    \end{aligned}\right. \tag{1}</script><script type="math/tex; mode=display">a_i^1=g(z_i^1) \tag{2}</script><script type="math/tex; mode=display">\left\{    \begin{aligned}    z_1^2=w_{11}^2a_1^1+w_{21}^2a_2^1+ w_{31}^2a_3^1\\    z_2^2=w_{12}^2a_1^1+w_{22}^2a_2^1+ w_{32}^2a_3^1\\    \end{aligned}\right. \tag{3}</script><script type="math/tex; mode=display">a_i^2=g(z_i^2) \tag{4}</script><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>假设损失函数为MSE损失:</p><script type="math/tex; mode=display">Loss=\frac{1}{2}\sum_{i=0}^n(y_i-a_i^2)^2 \tag{5}</script><p>下面开始计算对各个参数$w_{ij}^k$的偏导。<br>从最近的开始:</p><script type="math/tex; mode=display">\left\{    \begin{aligned}    \frac{\partial L}{\partial w_{11}^2}= \frac{\partial L}{\partial z_1^2} \frac{\partial z_1^2}{w_{11}^2}=\frac{\partial L}{\partial z_1^2} a_1^1 \\    \frac{\partial L}{\partial w_{21}^2}= \frac{\partial L}{\partial z_1^2} \frac{\partial z_1^2}{w_{21}^2}=\frac{\partial L}{\partial z_1^2} a_2^1 \\    \frac{\partial L}{\partial w_{31}^2}= \frac{\partial L}{\partial z_1^2} \frac{\partial z_1^2}{w_{31}^2}=\frac{\partial L}{\partial z_1^2} a_3^1 \\\\    \frac{\partial L}{\partial w_{12}^2}= \frac{\partial L}{\partial z_2^2} \frac{\partial z_2^2}{w_{12}^2}=\frac{\partial L}{\partial z_2^2} a_1^1 \\    \frac{\partial L}{\partial w_{22}^2}= \frac{\partial L}{\partial z_2^2} \frac{\partial z_2^2}{w_{22}^2}=\frac{\partial L}{\partial z_2^2} a_2^1 \\    \frac{\partial L}{\partial w_{32}^2}= \frac{\partial L}{\partial z_2^2} \frac{\partial z_2^2}{w_{32}^2}=\frac{\partial L}{\partial z_3^2} a_3^1    \end{aligned}\right. \tag{6}</script><script type="math/tex; mode=display">\left\{    \begin{aligned}    \frac{\partial L}{\partial w_{11}^1}= \frac{\partial L}{\partial z_1^1} \frac{\partial z_1^1}{\partial w_{11}^1}=\frac{\partial L}{\partial z_1^1} a_1^0 \\    \frac{\partial L}{\partial w_{21}^1}= \frac{\partial L}{\partial z_1^1} \frac{\partial z_1^1}{\partial w_{21}^1}=\frac{\partial L}{\partial z_1^1} a_2^0 \\\\    \frac{\partial L}{\partial w_{12}^1}= \frac{\partial L}{\partial z_1^1} \frac{\partial z_2^1}{\partial w_{12}^1}=\frac{\partial L}{\partial z_2^1} a_1^0 \\    \frac{\partial L}{\partial w_{22}^1}= \frac{\partial L}{\partial z_2^1} \frac{\partial z_2^1}{\partial w_{22}^1}=\frac{\partial L}{\partial z_1^1} a_2^0 \\\\    \frac{\partial L}{\partial w_{13}^1}= \frac{\partial L}{\partial z_3^1} \frac{\partial z_3^1}{\partial w_{13}^1}=\frac{\partial L}{\partial z_3^1} a_1^0 \\    \frac{\partial L}{\partial w_{23}^1}= \frac{\partial L}{\partial z_3^1} \frac{\partial z_3^1}{\partial w_{23}^1}=\frac{\partial L}{\partial z_3^1} a_2^0    \end{aligned}\right. \tag{7}</script><p>由公示(6)(7)可以总结出来:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w_{ij}^k}=\frac{\partial L}{\partial z_j^k}a_i^{k-1} \tag{8}</script><p>其中$a_i^{k-1}$在正向传播中已经算出，下面计算$\frac{\partial L}{\partial z_j^k}$:</p><script type="math/tex; mode=display">\left\{    \begin{aligned}    \frac{\partial L}{\partial z_1^2}=\frac{\partial L}{\partial a_1^2}g'(z_1^2) \\    \frac{\partial L}{\partial z_2^2}=\frac{\partial L}{\partial a_2^2}g'(z_2^2)     \end{aligned}\right. \tag{9}</script><script type="math/tex; mode=display">\left\{    \begin{aligned}    \frac{\partial L}{\partial z_1^1}=\frac{\partial L}{\partial a_1^1}g'(z_1^1)=(\frac{\partial L}{\partial z_1^2}w_{11}^2+\frac{\partial L}{\partial z_2^2}w_{12}^2)g'(z_1^1) \\    \frac{\partial L}{\partial z_2^1}=\frac{\partial L}{\partial a_2^1}g'(z_2^1)=(\frac{\partial L}{\partial z_1^2}w_{21}^2+\frac{\partial L}{\partial z_2^2}w_{22}^2)g'(z_2^1) \\    \frac{\partial L}{\partial z_3^1}=\frac{\partial L}{\partial a_3^1}g'(z_3^1)=(\frac{\partial L}{\partial z_1^2}w_{31}^2+\frac{\partial L}{\partial z_2^2}w_{32}^2)g'(z_3^1)    \end{aligned}\right. \tag{10}</script><p>由(9)(10)可知，要求得$\frac{\partial L}{\partial z_j^k}$，需要$w_{ij}^k$和$z_i^k$。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>$\qquad$现在可以得出结论，在整个神经网络训练过程中，出于反向传播的需要，我们需要在正向传播的时候在显存中记录$a_i^k$、$w_{ij}^k$和$z_i^k$。但是由于我们常用的激活函数都是可逆的，即知道$a_i^k$能反算出$z_i^k$，因此也可以只保存$a_i^k$、$w_{ij}^k$。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;都知道，无论使用什么框架，神经网络都是非常消耗显存的，那么，这些消耗的显存到底保存了什么？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;从一个例子开始&quot;&gt;&lt;a href=&quot;#从一个例子开始&quot; class=&quot;headerlink&quot; title
      
    
    </summary>
    
      <category term="深度学习" scheme="https://lingyixia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>差分法</title>
    <link href="https://lingyixia.github.io/2019/09/23/difference/"/>
    <id>https://lingyixia.github.io/2019/09/23/difference/</id>
    <published>2019-09-23T00:20:44.000Z</published>
    <updated>2021-10-23T12:36:58.352Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从一道例题开始"><a href="#从一道例题开始" class="headerlink" title="从一道例题开始"></a>从一道例题开始</h1><blockquote><p>有一片大海，大海中隐藏着若干陆地，当海平面下降的时候漏出的连续陆地会组成一个小岛，问题：当海平面处于多高的时候才能使漏出的小岛数量最多？数量是多少？</p></blockquote><h1 id="差分法解释"><a href="#差分法解释" class="headerlink" title="差分法解释"></a>差分法解释</h1><p>一个数组$A[1,2,3,4,5,6,7,8,9]$他的差分数组就是$A[i]-A[i-1]$，为方便计算我们给数组$A$头部插入一个0，变为$A[0,1,2,3,4,5,6,7,8,9]$，则其差分数组为$B[0,1,1,1,1,1,1,1,1,1]$(头部0也是单独插入的)，差分数组又一个很重要的性质,$\sum_0^nB[i]=A[n]$，即$B$的前缀和等于$A$的相应下表的值，这有什么用呢？例如我们想给$A[2:6]$区间+1，除了直接在$A$中循环，还可以把$B$更改为$[0,1,2,1,1,1,1,0,1,1]$,即$B[2]+1,B[7]-1$,然后在按照前缀和的形式加回去，也能算出来，这就是差分法，上题就是一个经典案例。</p><h1 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a>暴力法</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line">#include &lt;algorithm&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    vector&lt;int&gt; nums = &#123;6, 5, 7, 1, 4, 2, 4&#125;;</span><br><span class="line">    nums.insert(nums.begin(), 0);</span><br><span class="line">    nums.push_back(0);</span><br><span class="line">    int max_value = *max_element(nums.begin(), nums.end());</span><br><span class="line">    vector&lt;int&gt; levels(max_value + 1);//海平面</span><br><span class="line">    for(int i = 1; i &lt; nums.size() - 1; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        if(nums[i] &gt; nums[i - 1])</span><br><span class="line">        &#123;</span><br><span class="line">            for(int j = nums[i - 1] + 1; j &lt;= nums[i]; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                levels[j]++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; *max_element(levels.begin(), levels.end());</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>很明显，复杂度$O(n^2)$</p></blockquote></blockquote><h1 id="差分法"><a href="#差分法" class="headerlink" title="差分法"></a>差分法</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line">#include &lt;algorithm&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">void add_diff(vector&lt;int&gt;&amp; diff, int low, int high)</span><br><span class="line">&#123;</span><br><span class="line">    diff[low]++;</span><br><span class="line">    diff[high + 1]--;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    vector&lt;int&gt; nums = &#123;6, 5, 7, 1, 4, 2, 4&#125;;</span><br><span class="line">    nums.insert(nums.begin(), 0);</span><br><span class="line">    nums.push_back(0);</span><br><span class="line">    int max_value = *max_element(nums.begin(), nums.end());</span><br><span class="line">    vector&lt;int&gt; diff(max_value + 1);//差分数组</span><br><span class="line">    for(int i = 1; i &lt; nums.size() - 1; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        if(nums[i] &gt; nums[i - 1])</span><br><span class="line">        &#123;</span><br><span class="line">            add_diff(diff, nums[i - 1] + 1, nums[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    int maxn = 0, pre = 0;</span><br><span class="line">    for(int p = 1; p &lt;= max_value; p++)</span><br><span class="line">    &#123;</span><br><span class="line">        pre += diff[p];</span><br><span class="line">        maxn = max(maxn, pre);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; maxn;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>复杂度$O(n)$</p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;从一道例题开始&quot;&gt;&lt;a href=&quot;#从一道例题开始&quot; class=&quot;headerlink&quot; title=&quot;从一道例题开始&quot;&gt;&lt;/a&gt;从一道例题开始&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;有一片大海，大海中隐藏着若干陆地，当海平面下降的时候漏出的连续陆地会组成一
      
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="差分法" scheme="https://lingyixia.github.io/tags/%E5%B7%AE%E5%88%86%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>BPE 和 WordPiece</title>
    <link href="https://lingyixia.github.io/2019/08/10/BPE/"/>
    <id>https://lingyixia.github.io/2019/08/10/BPE/</id>
    <published>2019-08-10T09:37:51.000Z</published>
    <updated>2021-09-19T18:01:25.943Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>WordPiece 是从 BPE(byte pair encoder) 发展而来的一种处理词的技术，目的是解决 OOV 问题,以翻译模型为例,原理是抽取公共二元串(bigram),首先看下BPE(Transformer的官方代码也是使用的这种方式):</p></blockquote></blockquote><h1 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h1><h2 id="关键代码"><a href="#关键代码" class="headerlink" title="关键代码"></a>关键代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">import re, collections</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_pairs(word):</span><br><span class="line">    &quot;&quot;&quot;Return set of symbol pairs in a word.</span><br><span class="line">    Word is represented as a tuple of symbols (symbols being variable-length strings).</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    pairs = set()</span><br><span class="line">    prev_char = word[0]</span><br><span class="line">    for char in word[1:]:</span><br><span class="line">        pairs.add((prev_char, char))</span><br><span class="line">        prev_char = char</span><br><span class="line">    return pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def encode(orig):</span><br><span class="line">    &quot;&quot;&quot;Encode word based on list of BPE merge operations, which are applied consecutively&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    word = tuple(orig) + (&apos;&lt;/w&gt;&apos;,)</span><br><span class="line">    print(&quot;__word split into characters:__ &lt;tt&gt;&#123;&#125;&lt;/tt&gt;&quot;.format(word))</span><br><span class="line"></span><br><span class="line">    pairs = get_pairs(word)</span><br><span class="line"></span><br><span class="line">    if not pairs:</span><br><span class="line">        return orig</span><br><span class="line"></span><br><span class="line">    iteration = 0</span><br><span class="line">    while True:</span><br><span class="line">        iteration += 1</span><br><span class="line">        print(&quot;__Iteration &#123;&#125;:__&quot;.format(iteration))</span><br><span class="line">        print(&quot;bigrams in the word: &#123;&#125;&quot;.format(pairs))</span><br><span class="line">        print(pairs)</span><br><span class="line">        bigram = min(pairs, key=lambda pair: bpe_codes.get(pair, float(&apos;inf&apos;)))</span><br><span class="line">        print(&quot;candidate for merging: &#123;&#125;&quot;.format(bigram))</span><br><span class="line">        if bigram not in bpe_codes:</span><br><span class="line">            print(&quot;__Candidate not in BPE merges, algorithm stops.__&quot;)</span><br><span class="line">            break</span><br><span class="line">        first, second = bigram</span><br><span class="line">        new_word = []</span><br><span class="line">        i = 0</span><br><span class="line">        while i &lt; len(word):</span><br><span class="line">            try:</span><br><span class="line">                j = word.index(first, i)</span><br><span class="line">                new_word.extend(word[i:j])</span><br><span class="line">                i = j</span><br><span class="line">            except:</span><br><span class="line">                new_word.extend(word[i:])</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">            if word[i] == first and i &lt; len(word) - 1 and word[i + 1] == second:</span><br><span class="line">                new_word.append(first + second)</span><br><span class="line">                i += 2</span><br><span class="line">            else:</span><br><span class="line">                new_word.append(word[i])</span><br><span class="line">                i += 1</span><br><span class="line">        new_word = tuple(new_word)</span><br><span class="line">        word = new_word</span><br><span class="line">        print(&quot;word after merging: &#123;&#125;&quot;.format(word))</span><br><span class="line">        if len(word) == 1:</span><br><span class="line">            break</span><br><span class="line">        else:</span><br><span class="line">            pairs = get_pairs(word)</span><br><span class="line"></span><br><span class="line">    # don&apos;t print end-of-word symbols</span><br><span class="line">    if word[-1] == &apos;&lt;/w&gt;&apos;:</span><br><span class="line">        word = word[:-1]</span><br><span class="line">    elif word[-1].endswith(&apos;&lt;/w&gt;&apos;):</span><br><span class="line">        word = word[:-1] + (word[-1].replace(&apos;&lt;/w&gt;&apos;, &apos;&apos;),)</span><br><span class="line"></span><br><span class="line">    return word</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_stats(vocab):</span><br><span class="line">    pairs = collections.defaultdict(int)</span><br><span class="line">    for word, freq in vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        for i in range(len(symbols) - 1):</span><br><span class="line">            pairs[symbols[i], symbols[i + 1]] += freq</span><br><span class="line">    return pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def merge_vocab(pair, v_in):</span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(&apos; &apos;.join(pair))</span><br><span class="line">    p = re.compile(r&apos;(?&lt;!\S)&apos; + bigram + r&apos;(?!\S)&apos;)</span><br><span class="line">    for word in v_in:</span><br><span class="line">        w_out = p.sub(&apos;&apos;.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    return v_out</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    train_data = &#123;&apos;l o w&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 5,</span><br><span class="line">                  &apos;l o w e r&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 2,</span><br><span class="line">                  &apos;n e w e s t&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 6,</span><br><span class="line">                  &apos;w i d e s t&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 3&#125;</span><br><span class="line">    bpe_codes = &#123;&#125;</span><br><span class="line">    bpe_codes_reverse = &#123;&#125;</span><br><span class="line">    num_merges = 1000</span><br><span class="line">    for i in range(num_merges):</span><br><span class="line">        pairs = get_stats(train_data)</span><br><span class="line">        if not pairs:</span><br><span class="line">            break</span><br><span class="line">        print(&quot;Iteration &#123;&#125;&quot;.format(i + 1))</span><br><span class="line">        best = max(pairs, key=pairs.get)</span><br><span class="line">        train_data = merge_vocab(best, train_data)</span><br><span class="line">        bpe_codes[best] = i</span><br><span class="line">        bpe_codes_reverse[best[0] + best[1]] = best</span><br><span class="line">        print(&quot;new merge: &#123;&#125;&quot;.format(best))</span><br><span class="line">        print(&quot;train data: &#123;&#125;&quot;.format(train_data))</span><br></pre></td></tr></table></figure><p>输出结果:</p><blockquote><blockquote><p>Iteration 1<br>new merge: (‘e’, ‘s’)<br>train data: {‘l o w&lt;/w&gt;’: 5, ‘l o w e r&lt;/w&gt;’: 2, ‘n e w es t&lt;/w&gt;’: 6, ‘w i d es t&lt;/w&gt;’: 3}<br>Iteration 2<br>new merge: (‘es’, ‘t&lt;/w&gt;’)<br>train data: {‘l o w&lt;/w&gt;’: 5, ‘l o w e r&lt;/w&gt;’: 2, ‘n e w est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 3<br>new merge: (‘l’, ‘o’)<br>train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘n e w est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 4<br>new merge: (‘n’, ‘e’)<br>train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘ne w est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 5<br>new merge: (‘ne’, ‘w’)<br>train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘new est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 6<br>new merge: (‘new’, ‘est&lt;/w&gt;’)<br>train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 7<br>new merge: (‘lo’, ‘w&lt;/w&gt;’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 8<br>new merge: (‘w’, ‘i’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘wi d est&lt;/w&gt;’: 3}<br>Iteration 9<br>new merge: (‘wi’, ‘d’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘wid est&lt;/w&gt;’: 3}<br>Iteration 10<br>new merge: (‘wid’, ‘est&lt;/w&gt;’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}<br>Iteration 11<br>new merge: (‘lo’, ‘w’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘low e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}<br>Iteration 12<br>new merge: (‘low’, ‘e’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lowe r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}<br>Iteration 13<br>new merge: (‘lowe’, ‘r&lt;/w&gt;’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lower&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}</p><p>可以看到，首先输入是词典{单词:词频}的形式,在每一个轮次都会寻找一个最大的子串，上诉第一次频率最大的子串就是(‘e’, ‘s’),然后把字典中所有的(‘e’, ‘s’)合并就得到了{‘l o w &lt;\/w&gt;’: 5, ‘l o w e r &lt;\/w&gt;’: 2, ‘n e w es t &lt;\/w&gt;’: 6, ‘w i d es &lt;\/w&gt;’: 3, ‘f o l l o w &lt;\/w&gt;’: 1},后面以此类推,直到最大的词频小于某个阈值为止，上面设置的是2，最终得到的词表是:train data: {‘low&lt;\/w&gt;’: 5, ‘lower&lt;\/w&gt;’: 2, ‘newest&lt;\/w&gt;’: 6, ‘wides&lt;\/w&gt;’: 3, ‘f o l low&lt;\/w&gt;’: 1}，</p></blockquote></blockquote><p>这就是处理原语料的过程，在训练的时候，首先用上诉的<code>encode</code>代码把训练数据根据<code>code.file</code>映射到’voc.txt’中的词，然后进行训练(label方面的处理方式是独立的，也不一定需要BPE处理)</p><h2 id="subword-nmt使用"><a href="#subword-nmt使用" class="headerlink" title="subword-nmt使用"></a>subword-nmt使用</h2><p>数据准备类似：<a href="https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/tests/data/corpus.en" target="_blank" rel="noopener">https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/tests/data/corpus.en</a></p><ol><li><p>subword-nmt learn-bpe -s {num_operations} &lt; {train_file} &gt; {codes_file}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作用：生成分词器</span><br><span class="line">eg: subword-nmt learn-bpe -s 30000 &lt; corpus.en &gt; codes_file</span><br><span class="line">codes_file生成的就是接下来用到的分词器，其实就是一个词对组成的文件，其中每一行都是当时预料中词对中频率最高的一个。（上诉代码对应这部分）</span><br></pre></td></tr></table></figure></li><li><p>subword-nmt apply-bpe -c {codes_file} &lt; {test_file} &gt; {out_file}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">作用：用分词器处理预料</span><br><span class="line">eg:subword-nmt apply-bpe -c codes_file &lt; corpus.en &gt; out_file</span><br><span class="line">out_file中就是靠分词器生成的语料</span><br><span class="line">这里的操作单元是对原始预料的各个单词，比如&apos;cement&apos;，分为&apos;c e m e n t&lt;/w&gt;&apos;</span><br><span class="line">1. 词对为:[&lt;c,e&gt;,&lt;e,m&gt;,&lt;m,e&gt;,&lt;e,n&gt;,&lt;n,t&lt;/w&gt;&gt;],其中[&lt;e,m&gt;,&lt;m,e&gt;,&lt;e,n&gt;]在codes_file,并且&lt;e,n&gt;在codes_file排名靠前(语料中词频高),合并结果为:&apos;c e m en t&lt;/w&gt;&apos;</span><br><span class="line">2. 词对为:[&lt;c,e&gt;,&lt;e,m&gt;,&lt;m,en&gt;,&lt;en,t&lt;/w&gt;&gt;],其中[&lt;e,m&gt;,&lt;en,t&lt;/w&gt;&gt;]在codes_file,并且&lt;en,t&lt;/w&gt;&gt;在codes_file中排名靠前，合并结果为:&apos;c e m ent&lt;/w&gt;&apos;</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">最终合并结果为:&apos;c ement&lt;/w&gt;&apos;此时只有一个词对&lt;c,ement&lt;/w&gt;&gt;,并且不再codes_file中，因此合并停止，该词分为两个子词:c,ement,在预料中为:c@@ ement</span><br></pre></td></tr></table></figure></li><li><p>subword-nmt get-vocab —train_file {train_file} —vocab_file {vocab_file}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作用：生成词典（训练模型要用）</span><br><span class="line">eg: subword-nmt get-vocab --input out_file --output vocab_file</span><br><span class="line">vocab_file就是预料对应的词典(把out_file 中的单词set一便即可)，即接下来用vocab_file作为词典，out_file作为语料训练模型即可</span><br></pre></td></tr></table></figure></li><li><p>模型训练完成后，在具体场景使用的时候，必定会有@（因为词典中有@，用来区分该单词是前缀还是独立单词），因此要对后缀是@的单词跟下一个单词合并。</p></li></ol><h1 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h1><blockquote><p>WordPiece是Bert使用的处理方式,这个过两天在写吧，有点事。。。</p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考:"></a>参考:</h1><ol><li><a href="https://github.com/wszlong/sb-nmt" target="_blank" rel="noopener">https://github.com/wszlong/sb-nmt</a></li><li><a href="https://blog.csdn.net/u013453936/article/details/80878412" target="_blank" rel="noopener">https://blog.csdn.net/u013453936/article/details/80878412</a></li><li><a href="http://ufal.mff.cuni.cz/~helcl/courses/npfl116/ipython/byte_pair_encoding.html" target="_blank" rel="noopener">http://ufal.mff.cuni.cz/~helcl/courses/npfl116/ipython/byte_pair_encoding.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;WordPiece 是从 BPE(byte pair encoder) 发展而来的一种处理词的技术，目的是解决 OOV 问题,以翻译模型为例,原理是抽取公共二元串(bigram),首先看下BPE(Transformer的官
      
    
    </summary>
    
      <category term="NLP" scheme="https://lingyixia.github.io/categories/NLP/"/>
    
    
      <category term="算法" scheme="https://lingyixia.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>顿悟最大熵</title>
    <link href="https://lingyixia.github.io/2019/07/28/maxlikehood/"/>
    <id>https://lingyixia.github.io/2019/07/28/maxlikehood/</id>
    <published>2019-07-28T07:27:59.000Z</published>
    <updated>2021-09-30T06:08:06.246Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>虽然以前也了解最大熵模型，甚至以为自己完全理解了最大熵,知道今天才发现我以前都回了点啥。。。。今天发现自己顿悟了一下，特此记录，希望能给看到的人一点帮助，再次膜拜孔圣人的远见”温故而知新，可以为师矣.”(未完待续)</p></blockquote></blockquote><h1 id="从最大熵思想开始"><a href="#从最大熵思想开始" class="headerlink" title="从最大熵思想开始"></a>从最大熵思想开始</h1><p>其实在我看来，所谓的最大熵思想就是对已知的条件<strong>充分考虑</strong>,对未知的条件<strong>不做任何假设</strong>,这就是最大熵的真谛.</p><h1 id="这就是最大熵模型"><a href="#这就是最大熵模型" class="headerlink" title="这就是最大熵模型"></a>这就是最大熵模型</h1><p>5个海贼团$A、B、C、D、E$去争夺罗杰的宝藏,宝藏只有一份,问谁能得到?<br>我们给他建立的模型是<strong>均匀模型</strong>$P(X=A)=\frac{1}{6},P(X=B)=\frac{1}{6},P(X=C)=\frac{1}{6},P(X=D)=\frac{1}{6},P(X=E)=\frac{1}{6},P(X=6)=\frac{1}{6}$<br>我们为毛会建立这样的模型呢?因为均匀模型的<strong>熵最大</strong>,我们对这个5个海贼团一无所知,因此只能创建<strong>均匀模型</strong>来保证<strong>熵最大</strong>，这就是<strong>最大熵</strong>的一个最简单应用</p><p>5个海贼团$A、B、C、D、E$去争夺罗杰的宝藏,我们已知:$A$得到宝藏的概率和$B$一样,都是$\frac{1}{10}$,$C$和$D$一样,都是$\frac{3}{10}$,而$E$就比较牛逼了,他的概率是$\frac{5}{10},因为他叫路飞$,宝藏只有一份,问谁能得到?<br>现在我们有了条件，就不能简单的创建<strong>均匀模型</strong>了,因为此时我们要找的模型不是$P(X,\theta)$,而是$P(Y|X,\theta)$,我们需要的是$P(Y|X,\theta)$的熵最大，这便是<strong>最大熵模型</strong></p><h1 id="看看公式"><a href="#看看公式" class="headerlink" title="看看公式"></a>看看公式</h1><p>已知:$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N) }$<br>目标:运用最大熵原理找最大熵模型<br>由上诉<strong>最大熵模型</strong>的由来我们很容易得出公式,不是要找一个模型,在$X$的条件下这个模型的$P(Y|X,\theta)$的熵最大么,很明显这是个条件熵呀,那么这个模型在这份数据上的条件熵的公式:</p><script type="math/tex; mode=display">H(Y|X) = \sum_{x \in X} \widetilde{P}(x)H(Y|X=x)) \tag{1}</script><p>其中$\widetilde{P}(x)=\frac{N_x}{N}$<br>现在我们的目标确定了,我们要从<script type="math/tex">P(Y|X,\theta_1)，P(Y|X,\theta_2)，P(Y|X,\theta_3)，P(Y|X,\theta_4),,,</script>很多的模型中找到一个模型,这个模型<strong>充分考虑</strong>了已知数据$T$,而且对未知<strong>不做任何假设</strong>，<strong>不做任何假设</strong>解决了,我们让公式$(1)$要尽量大即可,那么<strong>充分考虑</strong>已知数据要怎样表示呢?说到这有人可能会有些思绪了,其实就是从$T$中找一个用来表示<strong>充分考虑</strong>已知条件的约束,让公式$(1)$在这个约数下尽量大.找到这个约束,一个最优化问题就出现了,现在我们的目的就是找这个约束。</p><h1 id="充分考虑"><a href="#充分考虑" class="headerlink" title="充分考虑"></a>充分考虑</h1><p>现在我们想想，什么是<strong>充分考虑</strong>了已知数据呢?比如我们找到了一个模型$P(X|Y，\theta)$,要考量它是对已知数据的考虑程度,把这句话数学化就是考量这个模型对原始数据<strong>特征</strong>的考虑程度,怎样表示原始数据的特征呢?<strong>特征函数</strong>出现了！！！<br>我们一般用这样一个<strong>特征函数</strong>来描述<strong>特征</strong>:</p><script type="math/tex; mode=display">f(x,y)=\begin{cases}1 这个x和y满足某一事实\\0 这个x和y不满足某一事实\\\end{cases}</script><p>那么这分数据特征的总值是多少呢?找期望呗</p><script type="math/tex; mode=display">E_{\widetilde p}(f)=\sum_{x,y}\widetilde P(x,y)f(x,y) \tag{2}</script><p>其中$\widetilde P(x,y)=\frac{N<em>{xy}}{N}$，谨记:$E</em>{\widetilde p}(f)$是原始数据的特征总值.<br>对于我们找的的模型$P(X|Y，\theta)$在这份数据的特征总值是多少呢?</p><script type="math/tex; mode=display">E_{p}(f)=\sum_{x,y}\widetilde P(x)P(y|x,\theta)f(x,y) \tag{3}</script><p>要想让$P(Y|X,\theta)$<strong>充分考虑</strong>原始数据咋做呢?两个特征总值相等呗</p><script type="math/tex; mode=display">E_{\widetilde p}(f)=E_{p}(f) \tag{4}</script><p>这就是约束条件了.</p><h1 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h1><p>现在我们的目标确定了,我们要让公式$(1)$在约束条件为公式$(4)$的条件下越大越好,现在我们把公式$(1)$展开:</p><script type="math/tex; mode=display">\begin{align}H(Y|X) &= \sum_{x \in X} \widetilde{P}(x)H(Y|X=x)) \\&=-\sum_{x \in X}\widetilde P(x)\sum_{y \in Y}P(y|X=x) \log P(Y|X=x)\\&= -\sum_{x \in X}\sum_{y \in Y}\widetilde P(x)P(y|X=x) \log P(Y|X=x)\\&=-\sum_{x ,y}\widetilde P(x)P(y|x) \log P(y|x)\end{align}</script><p>所以最终公式为:</p><script type="math/tex; mode=display">\max \quad \quad \quad -\sum_{x ,y}\widetilde P(x)P(y|x) \log P(y|x) \\s.t. \quad \quad \quad \quad E_{\widetilde p}(f_i)=E_{p}(f_i)  \quad i=1,2,3... \\ \quad \quad \quad \quad \sum_{y}P(y|x)=1(隐含条件)</script><p>其中i代表第i个特征函数<br>完毕散花!!!!卧槽写完后发现写的好清楚…不信你看了还不明白！</p><h1 id="最大熵模型求解"><a href="#最大熵模型求解" class="headerlink" title="最大熵模型求解"></a>最大熵模型求解</h1><p>1.首先引入拉格朗日乘子$w_0,w_1…$</p><script type="math/tex; mode=display">\begin{align}L(P(y|x),w)&=-\sum_{x ,y}\widetilde P(x)P(y|x) \log P(y|x)+w_0(1-\sum_{y}P(y|x))+\sum_{i=0}^{n}w_i(E_{\widetilde p}(f_i)-E_{p}(f_i))\\&=\sum_{x ,y}\widetilde P(x)P(y|x) \log P(y|x)+w_0(1-\sum_{y}P(y|x))+\sum_{i=1}^n w_i(\sum_{x,y} w_i\widetilde P(x,y)f_i(x,y)-\sum_{x,y}\widetilde P(y|x)f_i(x,y))\end{align}</script><p>原始问题是:</p><script type="math/tex; mode=display">\min_{P(y|x)} \max_wL(P(y|x),w)</script><p>对偶问题是:</p><script type="math/tex; mode=display">\max_{w} \min_{P(y|x)}L(P(y|x),w)</script><p>令:</p><script type="math/tex; mode=display">\Psi(w)=\min_{P(y|x)}L(P(y|x),w)</script><p>首先求内部:</p><script type="math/tex; mode=display">\begin{align}\frac{\partial P(y|x,w)}{\partial P(y|x)}&=\sum_{x,y}\widetilde P(x)(\log P(y|x)+1)-\sum_y w_0 - \sum_{x,y}(\widetilde P(x)\sum_{i=1}^n w_if_i(x,y)) \\&=\sum_{x,y}\widetilde P(x)(\log P(y|x)+1-w_0-\sum_{i=1}^n w_if_i(x,y))\end{align}</script><p>令偏导数为0，得:</p><script type="math/tex; mode=display">P(y|x)=e^{\sum_{i=1}^nw_if_i(x,y)+w_0-1}=\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{e^{1-w_0}}</script><p>因为有:</p><script type="math/tex; mode=display">\sum_{y}P(y|x)=1</script><p>因此:</p><script type="math/tex; mode=display">e^{1-w_0}=\sum_y e^{\sum_{i=1}^n w_if_i(x,y)}=Z_w(x)</script><p>因此，最终求得:</p><script type="math/tex; mode=display">P(y|x,\theta)=e^{\sum_{i=1}^nw_if_i(x,y)+w_0-1}=\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{\sum_y e^{\sum_{i=1}^n w_if_i(x,y)}}</script><p><strong>注意看这里，这不就是Softmax么!!!!!!!!!!!!!!!!!!</strong><br>最后一步在求<br>$\max_w$即可.</p><h1 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h1><p>最大似然估计的一般公式为:</p><script type="math/tex; mode=display">L(P_w)=log \prod_{x,y}P(y|x)^{\widetilde P(x,y)}=\sum_{x,y}\widetilde P(x,y)logP(x|y)</script><p>我们求得的最大熵模型为:</p><script type="math/tex; mode=display">\Psi(w)=\sum_{x ,y}\widetilde P(x)P(y|x) \log P(y|x)+w_0(1-\sum_{y}P(y|x))+\sum_{i=1}^n w_i(\sum_{x,y} w_i\widetilde P(x,y)f_i(x,y)-\sum_{x,y}\widetilde P(y|x)f_i(x,y))</script><p>我们要证明这两个式子求得得结果是相同的,即:</p><script type="math/tex; mode=display">P(y|x)=\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{e^{1-w_0}}</script><p>带入得:</p><script type="math/tex; mode=display">\begin{align}L(P_w)&=\sum_{x,y}\widetilde P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x,y}\widetilde P(x,y)\log Z_w(x) \\&=\sum_{x,y}\widetilde P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x \widetilde P(x)\log Z_w(x) \end{align}</script><p>得:</p><script type="math/tex; mode=display">\begin{align}\Psi(w)&=\sum_{x,y}^n\widetilde P(x,y)\sum_{i=1}^nw_if_i(x,y)+\sum_{x,y}\widetilde P(x)P(y|x)(\log P(y|x)-\sum_{i=1}w_if_i(x,y))\\&=\sum_{x,y}^n\widetilde P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x,y}\widetilde P(x)P_w(y|x)\log Z_w(x)\\&=\sum_{x,y}^n\widetilde P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x}\widetilde P(x)\log Z_w(x)\end{align}</script><p>因此，当带入$P(y|x,\theta)$时，得到得公式是相同得，因此求得的$\max_w$也一定相同.</p><h1 id="从最大熵模型到逻辑回归"><a href="#从最大熵模型到逻辑回归" class="headerlink" title="从最大熵模型到逻辑回归"></a>从最大熵模型到逻辑回归</h1><p>二元逻辑回归的似然函数为:</p><script type="math/tex; mode=display">L(\theta)=\prod_{i}^n[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}</script><p>在这里，其实$\pi(x_i)$就是$P(Y=1|X=x_i)$而且$y_i$完全可以写成数据中$Y=1$的数量你说对也不对。。其实在细想还可以写成数据中$X=x_i,Y=1$的数量.所以可以写成:</p><script type="math/tex; mode=display">L(\theta)=P(Y=1|X=x_i)^{N_{1x_i}}P(Y=0|X=x_i)^{N_{0x_i}}</script><p>在开个N(N是数据总量)次方得:</p><script type="math/tex; mode=display">L(\theta)=P(Y=1|X=x_i)^\frac{N_{1x_i}}{N}P(Y=0|X=x_i)^\frac{N_{0x_i}}{N}</script><p>想想$\frac{N_{0x_i}}{N}$是啥,,,这不就是$\widetilde{P}(x,y)$么,,,而且开个N次方对优化也没有影响,拿这个公式和$公式(3)$去掉log,即$P(y|x)^{\widetilde{p}(x,y)}$对比一下，这不一样么.哈哈哈，有没有恍然大明白的感觉。</p><h1 id="再到最大似然估计"><a href="#再到最大似然估计" class="headerlink" title="再到最大似然估计"></a>再到最大似然估计</h1><p>概率论与数理统计中的最大似然概率为公式为:</p><script type="math/tex; mode=display">\begin{align}L(\theta)&=log\prod_iP(x_i;\theta) \\&=log\prod_iP(x_i;\theta)^{Nx_i} \\&=\sum_iN{x_i}logP(x_i;\theta) \\&=N\sum_i\frac{Nx_i}{N}logP(x_i;\theta)\\&=N\sum_i\widetilde{p}(x_i)logP(x_i;\theta)\\&=Nlog\prod_iP(x_i;\theta)^{\widetilde{P}(x_i)}\end{align}</script><p>这里只是一元$x$,怎么对应到二元$(x,y)$呢？其实两者同理，需要正确的理解<strong>无论是几元，这里的$p(x_i;\theta)$的真正意义是变量特征函数的联合分布</strong>，对应到二元就是:</p><script type="math/tex; mode=display">L(\theta)=Nlog\prod_{i,j}P(x_i,y_j;\theta)^{\widetilde{P}(x_i,y_j)}</script><p>也就是说概率论与数理统计中的最大似然估计其实完全针对的是<strong>联合分布</strong>，特征和标签的地位完全一样,得到的似然函数公式其实是一个<strong>信息熵</strong>。但是我们的目的得到输入任何$x$后$y$的分布，是个<strong>边缘分布</strong>,则该边缘分布的熵就是个<strong>条件熵</strong>：</p><script type="math/tex; mode=display">\begin{align}H(Y|x)&={P}(x) \sum_{y\in Y}P(x|y;\theta)logP(y|x;\theta) \\&=\sum_{y\in Y}P(x,y;\theta)logP(y|x;\theta) \\&=\prod_y logP(y|x;\theta)^{P(x,y;\theta)} \\&=\prod_y logP(y|x;\theta)^{\widetilde P(x,y)} (近似用数出来的数据代替P(x,y;\theta))\\\end{align}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;虽然以前也了解最大熵模型，甚至以为自己完全理解了最大熵,知道今天才发现我以前都回了点啥。。。。今天发现自己顿悟了一下，特此记录，希望能给看到的人一点帮助，再次膜拜孔圣人的远见”温故而知新，可以为师矣.”(未完待续)&lt;/p&gt;
      
    
    </summary>
    
      <category term="数学" scheme="https://lingyixia.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="机器学习" scheme="https://lingyixia.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>贪心</title>
    <link href="https://lingyixia.github.io/2019/07/27/greedy/"/>
    <id>https://lingyixia.github.io/2019/07/27/greedy/</id>
    <published>2019-07-27T02:37:31.000Z</published>
    <updated>2021-09-19T18:01:25.958Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Jump-Game"><a href="#Jump-Game" class="headerlink" title="Jump Game"></a><a href="https://leetcode.com/problems/jump-game/" target="_blank" rel="noopener">Jump Game</a></h1><p>贪心一:</p><blockquote><blockquote><p>每到一个i,如果i&lt;=reach意味着[0,i-1]的坐标能达到reach,如果i&gt;reach,则意味着根本就到不了这里,无需继续。</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">bool canJump(vector&lt;int&gt;&amp; nums) </span><br><span class="line">&#123;</span><br><span class="line">int reach = 0;</span><br><span class="line">for (int i =0; i &lt; nums.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">if (i &gt; reach || i &gt;= nums.size() - 1) break;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">reach = max(reach, i + nums[i]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">return reach &gt;= nums.size() - 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>贪心二:</p><blockquote><blockquote><p>和上诉形式不一样,思想差不多</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bool canJump(vector&lt;int&gt;&amp; nums) </span><br><span class="line">&#123;</span><br><span class="line">int len = nums.size();</span><br><span class="line">int curMax = nums[0];</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt;= curMax; i++)</span><br><span class="line">&#123;</span><br><span class="line">if (nums[i] + i &gt;= len - 1) return true;</span><br><span class="line">curMax = max(curMax, nums[i] + i);</span><br><span class="line">&#125;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>动态规划:</p><blockquote><blockquote><p>dp[i]表示到达i时候最多还剩下多少步</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bool canJump(vector&lt;int&gt;&amp; nums) </span><br><span class="line">&#123;</span><br><span class="line">vector&lt;int&gt; dp(nums.size(), 0);</span><br><span class="line">for (int i = 1; i &lt; nums.size(); ++i) </span><br><span class="line">&#123;</span><br><span class="line">dp[i] = max(dp[i - 1], nums[i - 1]) - 1;</span><br><span class="line">if (dp[i] &lt; 0) return false;</span><br><span class="line">&#125;</span><br><span class="line">return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="推销员"><a href="#推销员" class="headerlink" title="推销员"></a>推销员</h1><blockquote><blockquote><p>阿明是一名推销员，他奉命到螺丝街推销他们公司的产品。螺丝街是一条死胡同，出口与入口是同一个，街道的一侧是围墙，另一侧是住户。螺丝街一共有 N 家住户，第 i 家住户到入口的距离为 Si 米。由于同一栋房子里可以有多家住户，所以可能有多家住户与入口的距离相等。阿明会从入口进入，依次向螺丝街的 X 家住户推销产品，然后再原路走出去。<br>阿明每走 1 米就会积累 1 点疲劳值，向第 i 家住户推销产品会积累 Ai 点疲劳值。阿明是工作狂，他想知道，对于不同的 X，在不走多余的路的前提下，他最多可以积累多少点疲劳值。</p></blockquote></blockquote><p>暂存</p><p>#</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Jump-Game&quot;&gt;&lt;a href=&quot;#Jump-Game&quot; class=&quot;headerlink&quot; title=&quot;Jump Game&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://leetcode.com/problems/jump-game/&quot; target=&quot;
      
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Bert</title>
    <link href="https://lingyixia.github.io/2019/07/22/Bert/"/>
    <id>https://lingyixia.github.io/2019/07/22/Bert/</id>
    <published>2019-07-22T13:55:40.000Z</published>
    <updated>2021-09-19T18:01:25.943Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文不讲Bert原理,拟进行Bert源码分析和应用</p></blockquote><h1 id="Bert源码分析"><a href="#Bert源码分析" class="headerlink" title="Bert源码分析"></a>Bert源码分析</h1><h2 id="组织结构"><a href="#组织结构" class="headerlink" title="组织结构"></a>组织结构</h2><blockquote><blockquote><p><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">这是Bert Github地址</a>,打开后会看到这样的结构:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/Bert/bert.jpeg" alt title>                </div>                <div class="image-caption"></div>            </figure><br>下面我将逐个分析上诉图片中加框的文件,其他的文件不是源码,不用分析.</p></blockquote></blockquote><h2 id="modeling-py"><a href="#modeling-py" class="headerlink" title="modeling.py"></a>modeling.py</h2><blockquote><blockquote><p>该文件是整个Bert模型源码,包含两个类:</p><ul><li>BertConfig:Bert配置类</li><li>BertModel:Bert模型类</li><li>embedding_lookup:用来返回函数token embedding词向量</li><li>embedding_postprocessor:得到token embedding+segment embedding+position embedding</li><li>create_attention_mask_from_input_mask得到mask,用来attention该attention的部分</li><li>transformer_model和attention_layer:Transform的ender部分,也就是self-attention,不解释了，看太多遍了.</li></ul></blockquote></blockquote><p><strong>注意上面的顺序,不是乱写的,是按照BertModel调用顺序组织的.</strong></p><h3 id="BertConfig"><a href="#BertConfig" class="headerlink" title="BertConfig"></a>BertConfig</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">class BertConfig(object):</span><br><span class="line">    def __init__(self,</span><br><span class="line">                 vocab_size,#词表大小</span><br><span class="line">                 hidden_size=768,#即是词向量维度又是Transform的隐藏层维度</span><br><span class="line">                 num_hidden_layers=12,#Transformer encoder中的隐藏层数,普通Transform中是6个</span><br><span class="line">                 num_attention_heads=12,multi-head attention 的head的数量,普通Transform中是8个</span><br><span class="line">                 intermediate_size=3072,encoder的“中间”隐层神经元数,普通Transform中是一个feed-forward</span><br><span class="line">                 hidden_act=&quot;gelu&quot;,#隐藏层激活函数</span><br><span class="line">                 hidden_dropout_prob=0.1,#隐层dropout率</span><br><span class="line">                 attention_probs_dropout_prob=0.1,#注意力部分的dropout</span><br><span class="line">                 max_position_embeddings=512,#最大位置编码长度,也就是序列的最大长度</span><br><span class="line">                 type_vocab_size=16,#token_type_ids的大小,所谓的token_type_ids在Bert中是0或1，也就是上句标记为0，下句标记为1，鬼知道默认为16是啥意思。。。</span><br><span class="line">                 initializer_range=0.02):随机初始化的参数</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_hidden_layers = num_hidden_layers</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        self.hidden_act = hidden_act</span><br><span class="line">        self.intermediate_size = intermediate_size</span><br><span class="line">        self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">        self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">        self.max_position_embeddings = max_position_embeddings</span><br><span class="line">        self.type_vocab_size = type_vocab_size</span><br><span class="line">        self.initializer_range = initializer_range</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_dict(cls, json_object):</span><br><span class="line">        config = BertConfig(vocab_size=None)</span><br><span class="line">        for (key, value) in six.iteritems(json_object):</span><br><span class="line">            config.__dict__[key] = value</span><br><span class="line">        return config</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_json_file(cls, json_file):</span><br><span class="line">        with tf.gfile.GFile(json_file, &quot;r&quot;) as reader:</span><br><span class="line">            text = reader.read()</span><br><span class="line">        return cls.from_dict(json.loads(text))</span><br><span class="line"></span><br><span class="line">    def to_dict(self):</span><br><span class="line">        output = copy.deepcopy(self.__dict__)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">    def to_json_string(self):</span><br><span class="line">        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + &quot;\n&quot;</span><br></pre></td></tr></table></figure><h3 id="BertModel"><a href="#BertModel" class="headerlink" title="BertModel"></a>BertModel</h3><blockquote><blockquote><p>现在进入正题,开始分析Bert模型源码</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">class BertModel(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None,use_one_hot_embeddings=False, scope=None):</span><br><span class="line">        config = copy.deepcopy(config)</span><br><span class="line">        if not is_training:</span><br><span class="line">            config.hidden_dropout_prob = 0.0</span><br><span class="line">            config.attention_probs_dropout_prob = 0.0</span><br><span class="line">        input_shape = get_shape_list(input_ids, expected_rank=2)</span><br><span class="line">        batch_size = input_shape[0]</span><br><span class="line">        seq_length = input_shape[1]</span><br><span class="line">        if input_mask is None:</span><br><span class="line">            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">        if token_type_ids is None:</span><br><span class="line">            token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">        with tf.variable_scope(scope, default_name=&quot;bert&quot;):</span><br><span class="line">            with tf.variable_scope(&quot;embeddings&quot;):</span><br><span class="line">                (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">                    input_ids=input_ids,</span><br><span class="line">                    vocab_size=config.vocab_size,</span><br><span class="line">                    embedding_size=config.hidden_size,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">                    use_one_hot_embeddings=use_one_hot_embeddings)#调用embedding_lookup得到初始词向量</span><br><span class="line">                self.embedding_output = embedding_postprocessor(</span><br><span class="line">                    input_tensor=self.embedding_output,</span><br><span class="line">                    use_token_type=True,</span><br><span class="line">                    token_type_ids=token_type_ids,</span><br><span class="line">                    token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">                    token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">                    use_position_embeddings=True,</span><br><span class="line">                    position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">                    dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">            with tf.variable_scope(&quot;encoder&quot;):</span><br><span class="line">                attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask)</span><br><span class="line">                self.all_encoder_layers = transformer_model(</span><br><span class="line">                    input_tensor=self.embedding_output,</span><br><span class="line">                    attention_mask=attention_mask,</span><br><span class="line">                    hidden_size=config.hidden_size,</span><br><span class="line">                    num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">                    num_attention_heads=config.num_attention_heads,</span><br><span class="line">                    intermediate_size=config.intermediate_size,</span><br><span class="line">                    intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">                    hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    do_return_all_layers=True)</span><br><span class="line"></span><br><span class="line">            self.sequence_output = self.all_encoder_layers[-1]</span><br><span class="line">            with tf.variable_scope(&quot;pooler&quot;):</span><br><span class="line">                first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)</span><br><span class="line">                self.pooled_output = tf.layers.dense(</span><br><span class="line">                    first_token_tensor,</span><br><span class="line">                    config.hidden_size,</span><br><span class="line">                    activation=tf.tanh,</span><br><span class="line">                    kernel_initializer=create_initializer(config.initializer_range))</span><br><span class="line">    def get_pooled_output(self):</span><br><span class="line">        return self.pooled_output</span><br><span class="line">    def get_sequence_output(self):</span><br><span class="line">        return self.sequence_output</span><br><span class="line"></span><br><span class="line">    def get_all_encoder_layers(self):</span><br><span class="line">        return self.all_encoder_layers</span><br><span class="line"></span><br><span class="line">    def get_embedding_output(self):</span><br><span class="line">        return self.embedding_output</span><br><span class="line"></span><br><span class="line">    def get_embedding_table(self):</span><br><span class="line">        return self.embedding_table</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明:</p><ul><li><code>config</code>:一个<code>BertConfig</code>实例</li><li><code>is_training</code>:<code>bool</code>类型,是否是训练流程,用类控制是否dropout</li><li><code>input_ids</code>:输入<code>Tensor</code>, <code>shape</code>是<code>[batch_size, seq_length]</code>.</li><li><code>input_mask</code>:<code>shape</code>是<code>[batch_size, seq_length]</code>无需细讲</li><li><code>token_type_ids</code>:<code>shape</code>是<code>[batch_size, seq_length]</code>,<code>bert</code>中就是0或1</li><li><code>use_one_hot_embeddings</code>:在<code>embedding_lookup</code>返回词向量的时候使用,详细见<code>embedding_lookup</code>函数</li></ul></blockquote></blockquote><h3 id="embedding-lookup"><a href="#embedding-lookup" class="headerlink" title="embedding_lookup"></a>embedding_lookup</h3><blockquote><blockquote><p>为了得到进入模型的词向量(token embedding)</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def embedding_lookup(input_ids,vocab_size,embedding_size=128,initializer_range=0.02,word_embedding_name=&quot;word_embeddings&quot;,use_one_hot_embeddings=False):</span><br><span class="line">  if input_ids.shape.ndims == 2:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[-1])</span><br><span class="line">  embedding_table = tf.get_variable(name=word_embedding_name,shape=[vocab_size, embedding_size],initializer=create_initializer(initializer_range))</span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [-1])</span><br><span class="line">  if use_one_hot_embeddings:</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  else:</span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line">  output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size])</span><br><span class="line">  return (output, embedding_table)</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明：</p><ul><li>input_ids：[batch_size, seq_length]</li><li>vocab_size:词典大小</li><li>initializer_range：初始化参数</li><li>word_embedding_name:不解释</li><li>use_one_hot_embeddings:是否使用one_hot方式初始化(为啥我感觉这里是True还是False结果得到的结果是一样的？？？？？)如下代码.</li></ul></blockquote></blockquote><p>return:token embedding:[batch_size, seq_length, embedding_size].和embedding_table(不解释)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line">flat_input_ids = [2, 4, 5]</span><br><span class="line">embedding_table = tf.constant(value=[[1, 2, 3, 4],</span><br><span class="line">                                     [5, 6, 7, 8],</span><br><span class="line">                                     [9, 1, 2, 3],</span><br><span class="line">                                     [5, 6, 7, 8],</span><br><span class="line">                                     [6, 4, 78, 9],</span><br><span class="line">                                     [6, 8, 9, 3]],dtype=tf.float32)</span><br><span class="line">one_hot_input_ids = tf.one_hot(flat_input_ids, depth=6)</span><br><span class="line">output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">print(output)</span><br><span class="line">print(100*&apos;*&apos;)</span><br><span class="line">output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><h3 id="embedding-postprocessor"><a href="#embedding-postprocessor" class="headerlink" title="embedding_postprocessor"></a>embedding_postprocessor</h3><blockquote><blockquote><p>bert模型的输入向量有三个,embedding_lookup得到的是token embedding 我们还需要segment embedding和position embedding,这三者的维度是完全相同的(废话不相同怎么加啊。。。)本部分代码会将这三个embeddig加起来并dropout</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def embedding_postprocessor(input_tensor,</span><br><span class="line">                            use_token_type=False,</span><br><span class="line">                            token_type_ids=None,</span><br><span class="line">                            token_type_vocab_size=16,</span><br><span class="line">                            token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">                            use_position_embeddings=True,</span><br><span class="line">                            position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">                            initializer_range=0.02,</span><br><span class="line">                            max_position_embeddings=512,</span><br><span class="line">                            dropout_prob=0.1):</span><br><span class="line">    input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">    batch_size = input_shape[0]</span><br><span class="line">    seq_length = input_shape[1]</span><br><span class="line">    width = input_shape[2]</span><br><span class="line">    output = input_tensor</span><br><span class="line">    if use_token_type:</span><br><span class="line">        if token_type_ids is None:</span><br><span class="line">            raise ValueError(&quot;`token_type_ids` must be specified if&quot;&quot;`use_token_type` is True.&quot;)</span><br><span class="line">        token_type_table = tf.get_variable(name=token_type_embedding_name,</span><br><span class="line">                                           shape=[token_type_vocab_size, width],</span><br><span class="line">                                           initializer=create_initializer(initializer_range))</span><br><span class="line">        flat_token_type_ids = tf.reshape(token_type_ids, [-1])</span><br><span class="line">        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">        token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width])</span><br><span class="line">        output += token_type_embeddings</span><br><span class="line">    if use_position_embeddings:</span><br><span class="line">        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">        with tf.control_dependencies([assert_op]):</span><br><span class="line">            full_position_embeddings = tf.get_variable(name=position_embedding_name,</span><br><span class="line">                                                       shape=[max_position_embeddings, width],</span><br><span class="line">                                                       initializer=create_initializer(initializer_range))</span><br><span class="line">            position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])</span><br><span class="line">            num_dims = len(output.shape.as_list())</span><br><span class="line">            position_broadcast_shape = []</span><br><span class="line">            for _ in range(num_dims - 2):</span><br><span class="line">                position_broadcast_shape.append(1)</span><br><span class="line">            position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">            position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)</span><br><span class="line">            output += position_embeddings</span><br><span class="line">    output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">    return output</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明:</p><ul><li>input_tensor:token embedding[batch_size, seq_length, embedding_size]</li><li>use_token_type是否使用segment embedding</li><li>token_type_ids:[batch_size, seq_length],这两个参数其实就是控制生成segment embedding的,上诉代码中的<code>output += token_type_embeddings</code>就是得到token embedding+segment embedding</li><li>use_position_embeddings:是否使用位置信息</li><li>max_position_embeddings:序列最大长度<br>注:</li><li>本部分代码中的<code>width</code>其实就是词向量维度(换个<code>embedding_size</code>能死啊。。。)</li><li>可以看出位置信息跟Transform的固定方式不一样，它是训练出来的.</li><li><code>output += position_embeddings</code>就得到了三者的想加结果<br>return :token embedding+segment embedding+position_embeddings</li></ul></blockquote></blockquote><h3 id="create-attention-mask-from-input-mask"><a href="#create-attention-mask-from-input-mask" class="headerlink" title="create_attention_mask_from_input_mask"></a>create_attention_mask_from_input_mask</h3><blockquote><blockquote><p>目的是将本来shape为[batch_size, seq_length]转为[batch_size, seq_length,seq_length],为什么要这样的维度呢?因为…..算了麻烦不写了，去我的另一篇<a href="https://lingyixia.github.io/2019/04/05/transformer/">Transform</a>中看吧</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def create_attention_mask_from_input_mask(from_tensor, to_mask):</span><br><span class="line">    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])</span><br><span class="line">    batch_size = from_shape[0]</span><br><span class="line">    from_seq_length = from_shape[1]</span><br><span class="line">    to_shape = get_shape_list(to_mask, expected_rank=2)</span><br><span class="line">    to_seq_length = to_shape[1]</span><br><span class="line">    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)</span><br><span class="line">    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)</span><br><span class="line">    mask = broadcast_ones * to_mask</span><br><span class="line">    return mask</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明:</p><ul><li>from_tensor:[batch_size, seq_length].</li><li>to_mask:[batch_size, seq_length]<br>注:<code>Transform</code>中的<code>mask</code>和平常用的不太一样,这里的<code>mask</code>是为了在计算<code>attention</code>的时候”看不到不应该看到的内容”,计算方式为该看到的<code>mask</code>为0，不该看到的<code>mask</code>为一个负的很大的数字,然后两者相加(平常使用<code>mask</code>是看到的为1，看不到的为0，然后两者做点乘)，这样在计算<code>softmax</code>的时候那些负数的<code>attention</code>会非常非常小,也就基本看不到了.</li></ul></blockquote></blockquote><h3 id="transformer-model"><a href="#transformer-model" class="headerlink" title="transformer_model"></a>transformer_model</h3><blockquote><blockquote><p>这一部分是<code>Transform</code>部分,但是只有<code>encoder</code>部分,从<code>BertModel</code>中的<code>with tf.variable_scope(&quot;encoder&quot;):</code>这一部分也可以看出来</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">def transformer_model(input_tensor,</span><br><span class="line">                      attention_mask=None,</span><br><span class="line">                      hidden_size=768,</span><br><span class="line">                      num_hidden_layers=12,</span><br><span class="line">                      num_attention_heads=12,</span><br><span class="line">                      intermediate_size=3072,</span><br><span class="line">                      intermediate_act_fn=gelu,</span><br><span class="line">                      hidden_dropout_prob=0.1,</span><br><span class="line">                      attention_probs_dropout_prob=0.1,</span><br><span class="line">                      initializer_range=0.02,</span><br><span class="line">                      do_return_all_layers=False):</span><br><span class="line">    if hidden_size % num_attention_heads != 0:</span><br><span class="line">        raise ValueError(</span><br><span class="line">            &quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span><br><span class="line">            &quot;heads (%d)&quot; % (hidden_size, num_attention_heads))</span><br><span class="line">    attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">    input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">    batch_size = input_shape[0]</span><br><span class="line">    seq_length = input_shape[1]</span><br><span class="line">    input_width = input_shape[2]</span><br><span class="line">    if input_width != hidden_size:</span><br><span class="line">        raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; %</span><br><span class="line">                         (input_width, hidden_size))</span><br><span class="line">    prev_output = reshape_to_matrix(input_tensor)#这个不单独写了,就是将[batch_size, seq_length, embedding_size]的input 给reahpe为[batch_size*seq_length,embedding_size]</span><br><span class="line">    all_layer_outputs = []</span><br><span class="line">    for layer_idx in range(num_hidden_layers):</span><br><span class="line">        with tf.variable_scope(&quot;layer_%d&quot; % layer_idx):</span><br><span class="line">            layer_input = prev_output</span><br><span class="line">            with tf.variable_scope(&quot;attention&quot;):</span><br><span class="line">                attention_heads = []</span><br><span class="line">                with tf.variable_scope(&quot;self&quot;):</span><br><span class="line">                    attention_head = attention_layer(from_tensor=layer_input,</span><br><span class="line">                                                     to_tensor=layer_input,</span><br><span class="line">                                                     attention_mask=attention_mask,</span><br><span class="line">                                                     num_attention_heads=num_attention_heads,</span><br><span class="line">                                                     size_per_head=attention_head_size,</span><br><span class="line">                                                     attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">                                                     initializer_range=initializer_range,</span><br><span class="line">                                                     do_return_2d_tensor=True,</span><br><span class="line">                                                     batch_size=batch_size,</span><br><span class="line">                                                     from_seq_length=seq_length,</span><br><span class="line">                                                     to_seq_length=seq_length)</span><br><span class="line">                    attention_heads.append(attention_head)</span><br><span class="line">                attention_output = None</span><br><span class="line">                if len(attention_heads) == 1:</span><br><span class="line">                    attention_output = attention_heads[0]</span><br><span class="line">                else:</span><br><span class="line">                    attention_output = tf.concat(attention_heads, axis=-1)</span><br><span class="line">                with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">                    attention_output = tf.layers.dense(attention_output,</span><br><span class="line">                                                       hidden_size,</span><br><span class="line">                                                       kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">                    attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">                    attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line">            with tf.variable_scope(&quot;intermediate&quot;):#feed-forword部分</span><br><span class="line">                intermediate_output = tf.layers.dense(attention_output,</span><br><span class="line">                                                      intermediate_size,</span><br><span class="line">                                                      activation=intermediate_act_fn,</span><br><span class="line">                                                      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">            with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">                layer_output = tf.layers.dense(intermediate_output,</span><br><span class="line">                                               hidden_size,</span><br><span class="line">                                               kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">                layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">                layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">                prev_output = layer_output</span><br><span class="line">                all_layer_outputs.append(layer_output)</span><br><span class="line">    if do_return_all_layers:</span><br><span class="line">        final_outputs = []</span><br><span class="line">        for layer_output in all_layer_outputs:</span><br><span class="line">            final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">            final_outputs.append(final_output)</span><br><span class="line">        return final_outputs</span><br><span class="line">    else:</span><br><span class="line">        final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">        return final_output</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明:</p><ul><li>input_tensor:token embedding+segment embedding+position embedding [batch_size, seq_length, embedding_size]</li><li>attention_mask:[batch_size, seq_length,seq_length]</li><li>hidden_size:不解释</li><li>num_hidden_layers:多少个<code>ecncoder block</code></li><li>num_attention_heads:多少个<code>head</code></li><li>intermediate_size:<code>feed forward</code>隐藏层维度</li><li>intermediate_act_fn:<code>feed forward</code>激活函数<br>其他的不解释了<br>return [batch_size, seq_length, hidden_size],</li></ul></blockquote></blockquote><h3 id="attention-layer"><a href="#attention-layer" class="headerlink" title="attention_layer"></a>attention_layer</h3><blockquote><blockquote><p>其实就是<code>self-attention</code>,但是在计算的时候全都转换为了二维矩阵，按注释的意思是避免反复reshape,因为reshape在CPU/GPU上易于实现，但是在TPU上不易实现,这样可以加速训练.</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">def attention_layer(from_tensor,</span><br><span class="line">                    to_tensor,</span><br><span class="line">                    attention_mask=None,</span><br><span class="line">                    num_attention_heads=1,</span><br><span class="line">                    size_per_head=512,</span><br><span class="line">                    query_act=None,</span><br><span class="line">                    key_act=None,</span><br><span class="line">                    value_act=None,</span><br><span class="line">                    attention_probs_dropout_prob=0.0,</span><br><span class="line">                    initializer_range=0.02,</span><br><span class="line">                    do_return_2d_tensor=False,</span><br><span class="line">                    batch_size=None,</span><br><span class="line">                    from_seq_length=None,</span><br><span class="line">                    to_seq_length=None):</span><br><span class="line">    def transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, width):</span><br><span class="line">        output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line">        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])</span><br><span class="line">        return output_tensor</span><br><span class="line"></span><br><span class="line">    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])</span><br><span class="line">    to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])</span><br><span class="line">    if len(from_shape) != len(to_shape):</span><br><span class="line">        raise ValueError(&quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;)</span><br><span class="line">    if len(from_shape) == 3:</span><br><span class="line">        batch_size = from_shape[0]</span><br><span class="line">        from_seq_length = from_shape[1]</span><br><span class="line">        to_seq_length = to_shape[1]</span><br><span class="line">    elif len(from_shape) == 2:</span><br><span class="line">        if (batch_size is None or from_seq_length is None or to_seq_length is None):</span><br><span class="line">            raise ValueError(</span><br><span class="line">                &quot;When passing in rank 2 tensors to attention_layer, the values &quot;</span><br><span class="line">                &quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot;</span><br><span class="line">                &quot;must all be specified.&quot;)</span><br><span class="line">    from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">    to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line">    query_layer = tf.layers.dense(from_tensor_2d,</span><br><span class="line">                                  num_attention_heads * size_per_head,</span><br><span class="line">                                  activation=query_act,</span><br><span class="line">                                  name=&quot;query&quot;,</span><br><span class="line">                                  kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    key_layer = tf.layers.dense(to_tensor_2d,</span><br><span class="line">                                num_attention_heads * size_per_head,</span><br><span class="line">                                activation=key_act,</span><br><span class="line">                                name=&quot;key&quot;,</span><br><span class="line">                                kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    value_layer = tf.layers.dense(to_tensor_2d,</span><br><span class="line">                                  num_attention_heads * size_per_head,</span><br><span class="line">                                  activation=value_act,</span><br><span class="line">                                  name=&quot;value&quot;,</span><br><span class="line">                                  kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads, from_seq_length, size_per_head)</span><br><span class="line">    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, to_seq_length, size_per_head)</span><br><span class="line">    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)</span><br><span class="line">    attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head)))</span><br><span class="line">    if attention_mask is not None:</span><br><span class="line">        attention_mask = tf.expand_dims(attention_mask, axis=[1])</span><br><span class="line">        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0</span><br><span class="line">        attention_scores += adder#这里就是使用mask来attention该attention的部分</span><br><span class="line">    attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line">    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line">    value_layer = tf.reshape(value_layer, [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line">    value_layer = tf.transpose(value_layer, [0, 2, 1, 3])</span><br><span class="line">    context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line">    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])</span><br><span class="line">    if do_return_2d_tensor:</span><br><span class="line">        context_layer = tf.reshape(context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">    else:</span><br><span class="line">        context_layer = tf.reshape(context_layer, [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">    return context_layer</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明:</p><ul><li>from_tensor在Transform中被转为二维[batch_size*seq_length, embedding_size]</li><li>to_shape:传过来的参数跟from_tensor一毛一样,在这里没什么卵用其实,因为q和k的length是一样的</li><li>attention_mask:[batch_size, seq_length,seq_length]</li><li>num_attention_heads:head数量</li><li>size_per_head:每一个head维度,代码中是用总维度除以head数量得到的:attention_head_size = int(hidden_size / num_attention_heads)<br>return: return :[batch_size, from_seq_length,num_attention_heads * size_per_head].</li></ul></blockquote></blockquote><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def gelu(x):</span><br><span class="line">  cdf = 0.5 * (1.0 + tf.tanh(</span><br><span class="line">      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))</span><br><span class="line">  return x * cdf</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>这个激活函数很有特色，其实这个公式就是$x \times \Phi(x)$,后一项是正态函数,也就是说,gelu中后面那一大堆其实近似等于$\int_{-\infty}^{x}\frac{1}{\sqrt(2\pi)}e^{-\frac{x^2}{2}}dx$,至于咋来的这个近似值，还不清楚。<br>测试函数:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from scipy import stats</span><br><span class="line">import math</span><br><span class="line">a = stats.norm.cdf(2, 0, 1)</span><br><span class="line"></span><br><span class="line">def gelu(x):</span><br><span class="line">    return 0.5 * (1.0 + math.tanh((math.sqrt(2 / math.pi) * (x + 0.044715 * math.pow(x, 3)))))</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line">print(gelu(2))</span><br><span class="line">#结果:</span><br><span class="line">#0.9772498680518208</span><br><span class="line">#0.9772988470438875</span><br></pre></td></tr></table></figure></p></blockquote></blockquote><h3 id="总结一"><a href="#总结一" class="headerlink" title="总结一:"></a>总结一:</h3><p>看完模型感觉真特么简单这模型,似乎除了self-attention就啥都没有了,但是先别着急,一般情况下模型是重点，但是对于Bert而言，模型却仅仅是开始，真正的创新点还在下面.</p><h2 id="create-pretraining-data-py"><a href="#create-pretraining-data-py" class="headerlink" title="create_pretraining_data.py"></a>create_pretraining_data.py</h2><blockquote><blockquote><p>这部分代码用来生成训练样本,我们从<code>main</code>函数开始看起,首先进入<code>tokenization.py</code></p></blockquote></blockquote><h3 id="def-main"><a href="#def-main" class="headerlink" title="def main"></a>def main</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def main(_):</span><br><span class="line">    tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line">    tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)</span><br><span class="line">    input_files = []</span><br><span class="line">    for input_pattern in FLAGS.input_file.split(&quot;,&quot;):</span><br><span class="line">        input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line">    tf.logging.info(&quot;*** Reading from input files ***&quot;)</span><br><span class="line">    for input_file in input_files:</span><br><span class="line">        tf.logging.info(&quot;  %s&quot;, input_file)</span><br><span class="line">    rng = random.Random(FLAGS.random_seed)</span><br><span class="line">    instances = create_training_instances(input_files,</span><br><span class="line">                                          tokenizer,</span><br><span class="line">                                          FLAGS.max_seq_length,</span><br><span class="line">                                          FLAGS.dupe_factor,</span><br><span class="line">                                          FLAGS.short_seq_prob,</span><br><span class="line">                                          FLAGS.masked_lm_prob,</span><br><span class="line">                                          FLAGS.max_predictions_per_seq,</span><br><span class="line">                                          rng)</span><br><span class="line"></span><br><span class="line">    output_files = FLAGS.output_file.split(&quot;,&quot;)</span><br><span class="line">    tf.logging.info(&quot;*** Writing to output files ***&quot;)</span><br><span class="line">    for output_file in output_files:</span><br><span class="line">        tf.logging.info(&quot;  %s&quot;, output_file)</span><br><span class="line">    write_instance_to_example_files(instances,</span><br><span class="line">                                    tokenizer, </span><br><span class="line">                                    FLAGS.max_seq_length,</span><br><span class="line">                                    FLAGS.max_predictions_per_seq, </span><br><span class="line">                                    output_files)</span><br></pre></td></tr></table></figure><h3 id="class-TrainingInstance"><a href="#class-TrainingInstance" class="headerlink" title="class TrainingInstance"></a>class TrainingInstance</h3><blockquote><blockquote><p>单个训练样本类,看<code>__init__</code>就能看出来，没什么其他东西</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class TrainingInstance(object):</span><br><span class="line">    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,</span><br><span class="line">                 is_random_next):</span><br><span class="line">        self.tokens = tokens</span><br><span class="line">        self.segment_ids = segment_ids</span><br><span class="line">        self.is_random_next = is_random_next</span><br><span class="line">        self.masked_lm_positions = masked_lm_positions</span><br><span class="line">        self.masked_lm_labels = masked_lm_labels</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        s = &quot;&quot;</span><br><span class="line">        s += &quot;tokens: %s\n&quot; % (&quot; &quot;.join(</span><br><span class="line">            [tokenization.printable_text(x) for x in self.tokens]))</span><br><span class="line">        s += &quot;segment_ids: %s\n&quot; % (&quot; &quot;.join([str(x) for x in self.segment_ids]))</span><br><span class="line">        s += &quot;is_random_next: %s\n&quot; % self.is_random_next</span><br><span class="line">        s += &quot;masked_lm_positions: %s\n&quot; % (&quot; &quot;.join(</span><br><span class="line">            [str(x) for x in self.masked_lm_positions]))</span><br><span class="line">        s += &quot;masked_lm_labels: %s\n&quot; % (&quot; &quot;.join(</span><br><span class="line">            [tokenization.printable_text(x) for x in self.masked_lm_labels]))</span><br><span class="line">        s += &quot;\n&quot;</span><br><span class="line">        return s</span><br></pre></td></tr></table></figure><h3 id="def-create-training-instances"><a href="#def-create-training-instances" class="headerlink" title="def create_training_instances"></a>def create_training_instances</h3><blockquote><blockquote><p>这个函数是重中之重，用来生成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def create_training_instances(input_files, tokenizer, max_seq_length,</span><br><span class="line">                              dupe_factor, short_seq_prob, masked_lm_prob,</span><br><span class="line">                              max_predictions_per_seq, rng):</span><br><span class="line">    all_documents = [[]]#外层是文档，内层是文档中的每个句子</span><br><span class="line">    for input_file in input_files:</span><br><span class="line">        with tf.gfile.GFile(input_file, &quot;r&quot;) as reader:</span><br><span class="line">            while True:</span><br><span class="line">                line = tokenization.convert_to_unicode(reader.readline())</span><br><span class="line">                if not line:</span><br><span class="line">                    break</span><br><span class="line">                line = line.strip()</span><br><span class="line">                if not line:# 空行表示文档分割</span><br><span class="line">                    all_documents.append([])</span><br><span class="line">                tokens = tokenizer.tokenize(line)</span><br><span class="line">                if tokens:</span><br><span class="line">                    all_documents[-1].append(tokens)</span><br><span class="line">    all_documents = [x for x in all_documents if x]</span><br><span class="line">    rng.shuffle(all_documents)</span><br><span class="line">    vocab_words = list(tokenizer.vocab.keys())</span><br><span class="line">    instances = []</span><br><span class="line">    for _ in range(dupe_factor):</span><br><span class="line">        for document_index in range(len(all_documents)):</span><br><span class="line">            instances.extend(</span><br><span class="line">                create_instances_from_document(all_documents,</span><br><span class="line">                                               document_index,</span><br><span class="line">                                               max_seq_length,</span><br><span class="line">                                               short_seq_prob,</span><br><span class="line">                                               masked_lm_prob,</span><br><span class="line">                                               max_predictions_per_seq,</span><br><span class="line">                                               vocab_words,</span><br><span class="line">                                               rng))</span><br><span class="line">    rng.shuffle(instances)</span><br><span class="line">    return instances</span><br></pre></td></tr></table></figure></p><p>参数说明:<br>dupe_factor:每一个句子用几次:因为如果一个句子只用一次的话那么mask的位置就是固定的，这样我们把每个句子在训练中都多用几次,而且没次的mask位置都不相同,就可以防止某些词永远看不到<br>short_seq_prob:长度小于“max_seq_length”的样本比例。因为在fine-tune过程里面输入的target_seq_length是可变的（小于等于max_seq_length），那么为了防止过拟合也需要在pre-train的过程当中构造一些短的样本<br>max_predictions_per_seq:一个句子里最多有多少个[MASK]标记<br>masked_lm_prob:多少比例的Token被MASK掉<br>rng:随机率</p></blockquote></blockquote><h3 id="def-create-instances-from-document"><a href="#def-create-instances-from-document" class="headerlink" title="def create_instances_from_document"></a>def create_instances_from_document</h3><blockquote><blockquote><p>一个文档中抽取训练样本,重中之重</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">def create_instances_from_document(all_documents, document_index, max_seq_length, short_seq_prob,</span><br><span class="line">                                   masked_lm_prob, max_predictions_per_seq, vocab_words, rng):</span><br><span class="line">    document = all_documents[document_index]</span><br><span class="line">    # 为[CLS], [SEP], [SEP]预留三个空位</span><br><span class="line">    max_num_tokens = max_seq_length - 3</span><br><span class="line">    target_seq_length = max_num_tokens  # 以short_seq_prob的概率随机生成（2~max_num_tokens）的长度</span><br><span class="line">    if rng.random() &lt; short_seq_prob:</span><br><span class="line">        target_seq_length = rng.randint(2, max_num_tokens)</span><br><span class="line">    instances = []</span><br><span class="line">    current_chunk = []</span><br><span class="line">    current_length = 0</span><br><span class="line">    i = 0</span><br><span class="line">    while i &lt; len(document):</span><br><span class="line">        segment = document[i]</span><br><span class="line">        current_chunk.append(segment)</span><br><span class="line">        current_length += len(segment)</span><br><span class="line">        # 将句子依次加入current_chunk中，直到加完或者达到限制的最大长度</span><br><span class="line">        if i == len(document) - 1 or current_length &gt;= target_seq_length:</span><br><span class="line">            if current_chunk:</span><br><span class="line">                # `a_end`是第一个句子A结束的下标</span><br><span class="line">                a_end = 1</span><br><span class="line">                if len(current_chunk) &gt;= 2:</span><br><span class="line">                    a_end = rng.randint(1, len(current_chunk) - 1)</span><br><span class="line">                tokens_a = []</span><br><span class="line">                for j in range(a_end):</span><br><span class="line">                    tokens_a.extend(current_chunk[j])</span><br><span class="line">                tokens_b = []</span><br><span class="line">                is_random_next = False</span><br><span class="line">                # `a_end`是第一个句子A结束的下标</span><br><span class="line">                if len(current_chunk) == 1 or rng.random() &lt; 0.5:</span><br><span class="line">                    is_random_next = True</span><br><span class="line">                    target_b_length = target_seq_length - len(tokens_a)</span><br><span class="line">                    # 随机的挑选另外一篇文档的随机开始的句子</span><br><span class="line">                    # 但是理论上有可能随机到的文档就是当前文档，因此需要一个while循环</span><br><span class="line">                    # 这里只while循环10次，理论上还是有重复的可能性，但是我们忽略</span><br><span class="line">                    for _ in range(10):</span><br><span class="line">                        random_document_index = rng.randint(0, len(all_documents) - 1)</span><br><span class="line">                        if random_document_index != document_index:</span><br><span class="line">                            break</span><br><span class="line">                    random_document = all_documents[random_document_index]</span><br><span class="line">                    random_start = rng.randint(0, len(random_document) - 1)</span><br><span class="line">                    for j in range(random_start, len(random_document)):</span><br><span class="line">                        tokens_b.extend(random_document[j])</span><br><span class="line">                        if len(tokens_b) &gt;= target_b_length:</span><br><span class="line">                            break</span><br><span class="line">                    # 对于上述构建的随机下一句，我们并没有真正地使用它们</span><br><span class="line">                    # 所以为了避免数据浪费，我们将其“放回”</span><br><span class="line">                    num_unused_segments = len(current_chunk) - a_end</span><br><span class="line">                    i -= num_unused_segments</span><br><span class="line">                else:</span><br><span class="line">                    is_random_next = False</span><br><span class="line">                    for j in range(a_end, len(current_chunk)):</span><br><span class="line">                        tokens_b.extend(current_chunk[j])</span><br><span class="line">                # 如果太多了，随机去掉一些</span><br><span class="line">                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)</span><br><span class="line">                assert len(tokens_a) &gt;= 1</span><br><span class="line">                assert len(tokens_b) &gt;= 1</span><br><span class="line">                tokens = []</span><br><span class="line">                segment_ids = []</span><br><span class="line">                # 处理句子A</span><br><span class="line">                tokens.append(&quot;[CLS]&quot;)</span><br><span class="line">                segment_ids.append(0)</span><br><span class="line">                for token in tokens_a:</span><br><span class="line">                    tokens.append(token)</span><br><span class="line">                    segment_ids.append(0)</span><br><span class="line">                # 句子A结束，加上【SEP】</span><br><span class="line">                tokens.append(&quot;[SEP]&quot;)</span><br><span class="line">                segment_ids.append(0)</span><br><span class="line">                # 处理句子B</span><br><span class="line">                for token in tokens_b:</span><br><span class="line">                    tokens.append(token)</span><br><span class="line">                    segment_ids.append(1)</span><br><span class="line">                # 句子B结束，加上【SEP】</span><br><span class="line">                tokens.append(&quot;[SEP]&quot;)</span><br><span class="line">                segment_ids.append(1)</span><br><span class="line">                # 调用 create_masked_lm_predictions来随机对某些Token进行mask</span><br><span class="line">                (tokens, masked_lm_positions,</span><br><span class="line">                 masked_lm_labels) = create_masked_lm_predictions(tokens,</span><br><span class="line">                                                                  masked_lm_prob,</span><br><span class="line">                                                                  max_predictions_per_seq,</span><br><span class="line">                                                                  vocab_words, rng)</span><br><span class="line">                instance = TrainingInstance(tokens=tokens,</span><br><span class="line">                                            segment_ids=segment_ids,</span><br><span class="line">                                            is_random_next=is_random_next,</span><br><span class="line">                                            masked_lm_positions=masked_lm_positions,</span><br><span class="line">                                            masked_lm_labels=masked_lm_labels)</span><br><span class="line">                instances.append(instance)</span><br><span class="line">            current_chunk = []</span><br><span class="line">            current_length = 0</span><br><span class="line">        i += 1</span><br><span class="line">    return instances</span><br></pre></td></tr></table></figure><h3 id="def-create-masked-lm-predictions"><a href="#def-create-masked-lm-predictions" class="headerlink" title="def create_masked_lm_predictions"></a>def create_masked_lm_predictions</h3><blockquote><blockquote><p>真正的mask在这里实现</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng):</span><br><span class="line">    cand_indexes = [] # [CLS]和[SEP]不能用于MASK</span><br><span class="line">    for (i, token) in enumerate(tokens):</span><br><span class="line">        if token == &quot;[CLS]&quot; or token == &quot;[SEP]&quot;:</span><br><span class="line">            continue</span><br><span class="line">        if (FLAGS.do_whole_word_mask and len(cand_indexes) &gt;= 1 and</span><br><span class="line">                token.startswith(&quot;##&quot;)):</span><br><span class="line">            cand_indexes[-1].append(i)</span><br><span class="line">        else:</span><br><span class="line">            cand_indexes.append([i])</span><br><span class="line">    rng.shuffle(cand_indexes)</span><br><span class="line">    output_tokens = list(tokens)</span><br><span class="line">    num_to_predict = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob))))</span><br><span class="line">    masked_lms = []</span><br><span class="line">    covered_indexes = set()</span><br><span class="line">    for index_set in cand_indexes:</span><br><span class="line">        if len(masked_lms) &gt;= num_to_predict:</span><br><span class="line">            break</span><br><span class="line">        if len(masked_lms) + len(index_set) &gt; num_to_predict:</span><br><span class="line">            continue</span><br><span class="line">        is_any_index_covered = False</span><br><span class="line">        for index in index_set:</span><br><span class="line">            if index in covered_indexes:</span><br><span class="line">                is_any_index_covered = True</span><br><span class="line">                break</span><br><span class="line">        if is_any_index_covered:</span><br><span class="line">            continue</span><br><span class="line">        for index in index_set:</span><br><span class="line">            covered_indexes.add(index)</span><br><span class="line">            masked_token = None</span><br><span class="line">            # 80% of the time, replace with [MASK]</span><br><span class="line">            if rng.random() &lt; 0.8:</span><br><span class="line">                masked_token = &quot;[MASK]&quot;</span><br><span class="line">            else:</span><br><span class="line">            # 10% of the time, keep original</span><br><span class="line">                if rng.random() &lt; 0.5:</span><br><span class="line">                    masked_token = tokens[index]</span><br><span class="line">            # 10% of the time, replace with random word</span><br><span class="line">                else:</span><br><span class="line">                    masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]</span><br><span class="line">            output_tokens[index] = masked_token</span><br><span class="line">            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))</span><br><span class="line">    assert len(masked_lms) &lt;= num_to_predict</span><br><span class="line">     # 按照下标重排，保证是原来句子中出现的顺序</span><br><span class="line">    masked_lms = sorted(masked_lms, key=lambda x: x.index)</span><br><span class="line">    masked_lm_positions = []</span><br><span class="line">    masked_lm_labels = []</span><br><span class="line">    for p in masked_lms:</span><br><span class="line">        masked_lm_positions.append(p.index)</span><br><span class="line">        masked_lm_labels.append(p.label)</span><br><span class="line">    return (output_tokens, masked_lm_positions, masked_lm_labels)</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>代码流程是这样的:首先嫁给你一个句子随机打乱,并确定一个句子的15%是多少个token，设num_to_predict,然后对于[0,是多少个token，设num_to_predict]token，以80%的概率替换为[mask],10%的概率替换，10%的概率保持,这样就做到了对于15%的toke80% [mask],10%替换,10%保持。而预测的不是那15%的80（标注问题），而是全部15%。为什么要mask呢？你想啊，我们的目的是得到这样一个模型:输入一个句子，输出一个能够尽可能表示该句子的向量(用最容易理解的语言就是我们不知道输入的是什么玩意，但是我们需要知道输出的向量是什么),如果不mask直接训练那不就相当于用1来推导1？而如果我们mask一部分就意味着并不知道输入(至少不知道全部),至于为什么要把15不全部mask，我觉得这个解释很不错，但是过于专业化:</p><ul><li>如果把 100% 的输入替换为 [MASK]：模型会偏向为 [MASK] 输入建模，而不会学习到 non-masked 输入的表征。</li><li>如果把 90% 的输入替换为 [MASK]、10% 的输入替换为随机 token：模型会偏向认为 non-masked 输入是错的。</li><li>如果把 90% 的输入替换为 [MASK]、维持 10% 的输入不变：模型会偏向直接复制 non-masked 输入的上下文无关表征。<br>所以，为了使模型可以学习到相对有效的上下文相关表征，需要以 1:1 的比例使用两种策略处理 non-masked 输入。论文提及，随机替换的输入只占整体的 1.5%，似乎不会对最终效果有影响（模型有足够的容错余量）。<br>通俗点说就是全部mask的话就意味着用mask来预测真正的单词,学习的仅仅是mask(而且mask的每个词都不一样，学到的mask表示也不一样，很显然不合理)，加入10%的替换就意味着用错的词预测对的词，而10%保持不变意味着用1来推导1，因此后两个10%的作用其实是为了学到没有mask的部分。<br>或者还有一种解释方式: 因为每次都是要学习这15%的token，其他的学不到(认识到这一点很重要)倘若某一个词在训练模型的时候被mask了，而微调的时候出现了咋办？因此不管怎样，都必须让模型好歹”认识一下”这个词.<h2 id="tokenization-py"><a href="#tokenization-py" class="headerlink" title="tokenization.py"></a>tokenization.py</h2>按照<code>create_pretraining_data.py</code>中<code>main</code>的调用顺序，先看<code>FullTokenizer</code>类</li></ul></blockquote></blockquote><h3 id="FullTokenizer"><a href="#FullTokenizer" class="headerlink" title="FullTokenizer"></a>FullTokenizer</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class FullTokenizer(object):</span><br><span class="line">    def __init__(self, vocab_file, do_lower_case=True):</span><br><span class="line">        self.vocab = load_vocab(vocab_file)</span><br><span class="line">        self.inv_vocab = &#123;v: k for k, v in self.vocab.items()&#125;</span><br><span class="line">        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)</span><br><span class="line">        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)</span><br><span class="line"></span><br><span class="line">    def tokenize(self, text):</span><br><span class="line">        split_tokens = []</span><br><span class="line">        for token in self.basic_tokenizer.tokenize(text):</span><br><span class="line">            for sub_token in self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">                split_tokens.append(sub_token)</span><br><span class="line">        return split_tokens</span><br><span class="line"></span><br><span class="line">    def convert_tokens_to_ids(self, tokens):</span><br><span class="line">        return convert_by_vocab(self.vocab, tokens)</span><br><span class="line"></span><br><span class="line">    def convert_ids_to_tokens(self, ids):</span><br><span class="line">        return convert_by_vocab(self.inv_vocab, ids)</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>在<code>__init__</code>中可以看到，又得先分析<code>BasicTokenizer</code>类和<code>WordpieceTokenizer</code>类(哎呀真烦，最后在回来做超链接吧),除此之外就是调用了几个小函数,<code>load_vocab</code>它的输入参数是bert模型的词典,返回的是一个<code>OrdereDict</code>:{词:词号}.其他的不说了，没啥意思。</p></blockquote></blockquote><h3 id="class-BasicTokenizer"><a href="#class-BasicTokenizer" class="headerlink" title="class BasicTokenizer"></a>class BasicTokenizer</h3><blockquote><blockquote><p>目的是根据空格，标点进行普通的分词，最后返回的是关于词的列表，对于中文而言是关于字的列表。</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">class BasicTokenizer(object):</span><br><span class="line">  def __init__(self, do_lower_case=True):</span><br><span class="line">    self.do_lower_case = do_lower_case</span><br><span class="line"></span><br><span class="line">  def tokenize(self, text):</span><br><span class="line">  ##其实就是把字符串转为了list，分英文单词和中文单词处理</span><br><span class="line">  ##eg:Mr. Cassius crossed the highway, and stopped suddenly.转为[&apos;mr&apos;, &apos;.&apos;, &apos;cassius&apos;, &apos;crossed&apos;, &apos;the&apos;, &apos;highway&apos;, &apos;,&apos;, &apos;and&apos;, &apos;stopped&apos;, &apos;suddenly&apos;, &apos;.&apos;]</span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    text = self._clean_text(text)</span><br><span class="line">    text = self._tokenize_chinese_chars(text)</span><br><span class="line">    orig_tokens = whitespace_tokenize(text)#无需细说，就是把string按照空格切分为list</span><br><span class="line">    split_tokens = []</span><br><span class="line">    for token in orig_tokens:</span><br><span class="line">      if self.do_lower_case:</span><br><span class="line">        token = token.lower()</span><br><span class="line">        token = self._run_strip_accents(token)#这个函数干了什么我也没看明白,但是对正题流程不重要,略过吧</span><br><span class="line">      split_tokens.extend(self._run_split_on_punc(token))</span><br><span class="line">    output_tokens = whitespace_tokenize(&quot; &quot;.join(split_tokens))</span><br><span class="line">    return output_tokens</span><br><span class="line"></span><br><span class="line">  def _run_strip_accents(self, text):</span><br><span class="line">    text = unicodedata.normalize(&quot;NFD&quot;, text)</span><br><span class="line">    output = []</span><br><span class="line">    for char in text:</span><br><span class="line">      cat = unicodedata.category(char)</span><br><span class="line">      if cat == &quot;Mn&quot;:</span><br><span class="line">        continue</span><br><span class="line">      output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br><span class="line"></span><br><span class="line">  def _run_split_on_punc(self, text):</span><br><span class="line">    chars = list(text)</span><br><span class="line">    i = 0</span><br><span class="line">    start_new_word = True</span><br><span class="line">    output = []</span><br><span class="line">    while i &lt; len(chars):</span><br><span class="line">      char = chars[i]</span><br><span class="line">      if _is_punctuation(char):</span><br><span class="line">        output.append([char])</span><br><span class="line">        start_new_word = True</span><br><span class="line">      else:</span><br><span class="line">        if start_new_word:</span><br><span class="line">          output.append([])</span><br><span class="line">        start_new_word = False</span><br><span class="line">        output[-1].append(char)</span><br><span class="line">      i += 1</span><br><span class="line">    return [&quot;&quot;.join(x) for x in output]</span><br><span class="line"></span><br><span class="line">  def _tokenize_chinese_chars(self, text):</span><br><span class="line">    # 按字切分中文，其实就是英文单词不变,中文在字两侧添加空格</span><br><span class="line">    output = []</span><br><span class="line">    for char in text:</span><br><span class="line">      cp = ord(char)</span><br><span class="line">      if self._is_chinese_char(cp):</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">        output.append(char)</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">      else:</span><br><span class="line">        output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br><span class="line"></span><br><span class="line">  def _is_chinese_char(self, cp):</span><br><span class="line">    # 判断是否是汉字,这个函数很有意义，值得借鉴</span><br><span class="line">    # refer：https://www.cnblogs.com/straybirds/p/6392306.html</span><br><span class="line">    if ((cp &gt;= 0x4E00 and cp &lt;= 0x9FFF) or  #</span><br><span class="line">        (cp &gt;= 0x3400 and cp &lt;= 0x4DBF) or  #</span><br><span class="line">        (cp &gt;= 0x20000 and cp &lt;= 0x2A6DF) or  #</span><br><span class="line">        (cp &gt;= 0x2A700 and cp &lt;= 0x2B73F) or  #</span><br><span class="line">        (cp &gt;= 0x2B740 and cp &lt;= 0x2B81F) or  #</span><br><span class="line">        (cp &gt;= 0x2B820 and cp &lt;= 0x2CEAF) or</span><br><span class="line">        (cp &gt;= 0xF900 and cp &lt;= 0xFAFF) or  #</span><br><span class="line">        (cp &gt;= 0x2F800 and cp &lt;= 0x2FA1F)):  #</span><br><span class="line">      return True</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">  def _clean_text(self, text): # 去除无意义字符以及空格</span><br><span class="line">    output = []</span><br><span class="line">    for char in text:</span><br><span class="line">      cp = ord(char)</span><br><span class="line">      if cp == 0 or cp == 0xfffd or _is_control(char):</span><br><span class="line">        continue</span><br><span class="line">      if _is_whitespace(char):</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">      else:</span><br><span class="line">        output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br></pre></td></tr></table></figure><h3 id="class-WordpieceTokenizer"><a href="#class-WordpieceTokenizer" class="headerlink" title="class WordpieceTokenizer"></a>class WordpieceTokenizer</h3><blockquote><blockquote><p>这个才是重点,跑test的时候出现的那些##都是从这里拿来的，其实就是把未登录词在词表中匹配相应的前缀.</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">class WordpieceTokenizer(object):</span><br><span class="line">  def __init__(self, vocab, unk_token=&quot;[UNK]&quot;, max_input_chars_per_word=200):</span><br><span class="line">    self.vocab = vocab</span><br><span class="line">    self.unk_token = unk_token</span><br><span class="line">    self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">  def tokenize(self, text):</span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    output_tokens = []</span><br><span class="line">    for token in whitespace_tokenize(text):</span><br><span class="line">      chars = list(token)</span><br><span class="line">      if len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">        continue</span><br><span class="line">      is_bad = False</span><br><span class="line">      start = 0</span><br><span class="line">      sub_tokens = []</span><br><span class="line">      while start &lt; len(chars):</span><br><span class="line">        end = len(chars)</span><br><span class="line">        cur_substr = None</span><br><span class="line">        while start &lt; end:</span><br><span class="line">          substr = &quot;&quot;.join(chars[start:end])</span><br><span class="line">          if start &gt; 0:</span><br><span class="line">            substr = &quot;##&quot; + substr</span><br><span class="line">          if substr in self.vocab:</span><br><span class="line">            cur_substr = substr</span><br><span class="line">            break</span><br><span class="line">          end -= 1</span><br><span class="line">        if cur_substr is None:</span><br><span class="line">          is_bad = True</span><br><span class="line">          break</span><br><span class="line">        sub_tokens.append(cur_substr)</span><br><span class="line">        start = end</span><br><span class="line">      if is_bad:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">      else:</span><br><span class="line">        output_tokens.extend(sub_tokens)</span><br><span class="line">    return output_tokens</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>tokenize说明: 使用贪心的最大正向匹配算法<br>  eg:input = “unaffable” output = [“un”, “##aff”, “##able”],首先看”unaffable”在不在词表中，在的话就当做一个词，也就是WordPiece，不在的话在看”unaffabl”在不在，也就是<code>while</code>中的<code>end-=1</code>,最终发现”un”在词表中,算是一个WordPiece,然后start=2,也就是代码中的<code>start=end</code>,看”##affable”在不在词表中,在看”##affabl”(##表示接着前面)，最终返回[“un”, “##aff”, “##able”].注意，这样切分是可逆的，也就是可以根据词表重载”攒回”原词，以此便解决了oov问题.</p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文不讲Bert原理,拟进行Bert源码分析和应用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Bert源码分析&quot;&gt;&lt;a href=&quot;#Bert源码分析&quot; class=&quot;headerlink&quot; title=&quot;Bert源码分析&quot;&gt;&lt;/a&gt;Be
      
    
    </summary>
    
      <category term="NLP" scheme="https://lingyixia.github.io/categories/NLP/"/>
    
    
      <category term="论文" scheme="https://lingyixia.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>dataAugmentation</title>
    <link href="https://lingyixia.github.io/2019/07/16/dataAugmentation/"/>
    <id>https://lingyixia.github.io/2019/07/16/dataAugmentation/</id>
    <published>2019-07-16T12:34:44.000Z</published>
    <updated>2021-09-19T18:01:25.958Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>记录一下常用的NLP数据增加方式,数据增强常用于样本不够或者样本严重不均衡的情况下</p></blockquote></blockquote><h1 id="随机drop和shuffle"><a href="#随机drop和shuffle" class="headerlink" title="随机drop和shuffle"></a>随机drop和shuffle</h1><p>也就是把一个样本随机打乱词语顺序或者扔掉一些词语,当做新的样本,但是不能做过多的drop和shuffle，防止更改了原义</p><h1 id="同义词替换"><a href="#同义词替换" class="headerlink" title="同义词替换"></a>同义词替换</h1><h1 id="回译"><a href="#回译" class="headerlink" title="回译"></a>回译</h1><p>这个很有技巧性，就是吧样本翻译成其他语言，然后在翻译回来，当做新的样本</p><h1 id="生成对抗网络"><a href="#生成对抗网络" class="headerlink" title="生成对抗网络"></a>生成对抗网络</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;记录一下常用的NLP数据增加方式,数据增强常用于样本不够或者样本严重不均衡的情况下&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;随机drop和shuffle&quot;&gt;&lt;a href=&quot;#随机d
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>stackAndqueue</title>
    <link href="https://lingyixia.github.io/2019/07/13/stackAndqueue/"/>
    <id>https://lingyixia.github.io/2019/07/13/stackAndqueue/</id>
    <published>2019-07-13T09:04:58.000Z</published>
    <updated>2021-09-19T18:01:25.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="弹栈压栈"><a href="#弹栈压栈" class="headerlink" title="弹栈压栈"></a>弹栈压栈</h1><blockquote><blockquote><p>输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）<br>算法描述: 只需要按照顺序走一遍即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">bool IsPopOrder(vector&lt;int&gt; pushV,vector&lt;int&gt; popV) &#123;</span><br><span class="line">       stack&lt;int&gt; s;</span><br><span class="line">       int index=0;</span><br><span class="line">       for(int i =0;i&lt;pushV.size();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           s.push(pushV[i]);</span><br><span class="line">           while(!s.empty() &amp;&amp; s.top()==popV[index])</span><br><span class="line">           &#123;</span><br><span class="line">               s.pop();</span><br><span class="line">               index++;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       return s.empty();</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p></blockquote></blockquote><p>Tip: 对于一个入栈顺序的弹栈序列必然有这么一个特征:<br>出栈序列中每个数后面的比它小的数必然按照降序排列<br>比如入栈顺序是:1,2,3,4</p><ol><li>4,1,2,3不可能是出栈顺序,因为4后面比4小的数1,2,3不是降序排列</li><li>3,1,4,2也不合法,3后面比3小的数1,2不是降序排列</li><li>1,2,3,4合法,当前每个数后面没有比它小的</li></ol><h1 id="删除相邻重复字符串"><a href="#删除相邻重复字符串" class="headerlink" title="删除相邻重复字符串"></a><a href="https://leetcode.com/problems/remove-all-adjacent-duplicates-in-string/" target="_blank" rel="noopener">删除相邻重复字符串</a></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">string removeDuplicates(string S)</span><br><span class="line">&#123;</span><br><span class="line">    stack&lt;char&gt; s;</span><br><span class="line">    for (auto ch:S)</span><br><span class="line">    &#123;</span><br><span class="line">        if (s.empty() || ch != s.top())</span><br><span class="line">        &#123;</span><br><span class="line">            s.push(ch);</span><br><span class="line">        &#125; else</span><br><span class="line">        &#123;</span><br><span class="line">            s.pop();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    string result = &quot;&quot;;</span><br><span class="line">    while (!s.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        result = s.top()+result;</span><br><span class="line">        s.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="两个队列实现栈"><a href="#两个队列实现栈" class="headerlink" title="两个队列实现栈"></a>两个队列实现栈</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">class MyStack</span><br><span class="line">&#123;</span><br><span class="line">private:</span><br><span class="line">    queue&lt;int&gt; *q1;</span><br><span class="line">    queue&lt;int&gt; *q2;</span><br><span class="line">public:</span><br><span class="line">    MyStack()</span><br><span class="line">    &#123;</span><br><span class="line">        q1 = new queue&lt;int&gt;();</span><br><span class="line">        q2 = new queue&lt;int&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    void push(int x)</span><br><span class="line">    &#123;</span><br><span class="line">        queue&lt;int&gt; *currentQ = q1-&gt;empty() ? q2 : q1;</span><br><span class="line">        currentQ-&gt;push(x);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int pop()</span><br><span class="line">    &#123;</span><br><span class="line">        queue&lt;int&gt; *currentQ = q1-&gt;empty() ? q2 : q1;</span><br><span class="line">        queue&lt;int&gt; *emptyQ = q1-&gt;empty() ? q1 : q2;</span><br><span class="line">        int current;</span><br><span class="line">        while (!currentQ-&gt;empty())</span><br><span class="line">        &#123;</span><br><span class="line">            current = currentQ-&gt;front();</span><br><span class="line">            currentQ-&gt;pop();</span><br><span class="line">            if (currentQ-&gt;empty()) break;</span><br><span class="line">            emptyQ-&gt;push(current);</span><br><span class="line">        &#125;</span><br><span class="line">        return current;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int top()</span><br><span class="line">    &#123;</span><br><span class="line">        queue&lt;int&gt; *currentQ = q1-&gt;empty() ? q2 : q1;</span><br><span class="line">        queue&lt;int&gt; *emptyQ = q1-&gt;empty() ? q1 : q2;</span><br><span class="line">        int current;</span><br><span class="line">        while (!currentQ-&gt;empty())</span><br><span class="line">        &#123;</span><br><span class="line">            current = currentQ-&gt;front();</span><br><span class="line">            currentQ-&gt;pop();</span><br><span class="line">            emptyQ-&gt;push(current);</span><br><span class="line">        &#125;</span><br><span class="line">        return current;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bool empty()</span><br><span class="line">    &#123;</span><br><span class="line">        return q1-&gt;empty() &amp;&amp; q2-&gt;empty();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="两个栈实现队列"><a href="#两个栈实现队列" class="headerlink" title="两个栈实现队列"></a>两个栈实现队列</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class MyQueue</span><br><span class="line">&#123;</span><br><span class="line">public:</span><br><span class="line">    void push(int node) &#123;</span><br><span class="line">        stack1.push(node);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int pop() &#123;</span><br><span class="line">        int result=0;</span><br><span class="line">        int temp=0;</span><br><span class="line">        if(stack2.empty())</span><br><span class="line">        &#123;</span><br><span class="line">            while(!stack1.empty())</span><br><span class="line">            &#123;</span><br><span class="line">                temp = stack1.top();</span><br><span class="line">                stack1.pop();</span><br><span class="line">                stack2.push(temp);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        result = stack2.top();</span><br><span class="line">        stack2.pop();</span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">private:</span><br><span class="line">    stack&lt;int&gt; stack1;</span><br><span class="line">    stack&lt;int&gt; stack2;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;弹栈压栈&quot;&gt;&lt;a href=&quot;#弹栈压栈&quot; class=&quot;headerlink&quot; title=&quot;弹栈压栈&quot;&gt;&lt;/a&gt;弹栈压栈&lt;/h1&gt;&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>请叫我调参工程师</title>
    <link href="https://lingyixia.github.io/2019/07/11/parameters/"/>
    <id>https://lingyixia.github.io/2019/07/11/parameters/</id>
    <published>2019-07-11T11:50:42.000Z</published>
    <updated>2021-09-19T18:01:25.962Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>这篇博客会一直更新.<br>对于初学者而言，兴趣往往都在模型构成上面,但是，只有真正成为调参工程师才能深刻体会到调参对于模型的重要性，虽然我也只能算入门级,但已经被坑了好几次了。。。。。说多了都是泪啊。<br>总的来说,训练神经网络模型的超参数一般可以分为两类:</p><ul><li>训练参数:学习率,正则项系数,epoch数,batchsize</li><li>模型参数:模型层数,隐藏层参数</li></ul></blockquote></blockquote><h1 id="调参技巧"><a href="#调参技巧" class="headerlink" title="调参技巧"></a>调参技巧</h1><ol><li><p>当你想实现一个模型的时候,一定要用最精简的结构去实现它,尽量不要在任何地方做其他tip,任何tip都要尽量在保证模型无误的条件下进行，否则你永远不知道你所谓的一个小tip对你的模型有多大影响(血的教训,当初在做一个文本生成模型的时候所有激活函数都用的relu,这个bug让我一顿好找。。。)</p></li><li><p>保证每次随机种子不变,才能让实验更有效的对比</p></li><li><h1 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h1><h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2>作为一个调参工程师，私以为,学习率是<strong>最重要</strong>的参数,对于初学者来说，往往最容易忽视学习率的作用.举一个我最开始被坑的例子,当初是做了一个NER模型,写完之后各项指标增长速度特别特别慢,慢到令人发指(一小时从1%到2%),但问题是确实是不断增长，由于刚刚接触也不知道增长速度应该是怎样就一直等着，凉了一天，发现还是那个速度,等不及了开始检查模型问题,由于刚刚接触TF，对自己没把握就一直找一直找，几乎找了一个星期愣是没改出来.不得已，要不调调参数把,还好第一个改的就是学习率,其实原来是0.01我以为已经够小了,改成了0.001,刚一运行，200个step后precision直接到了20%,我他娘的就。。。。原来问题在这。所以以后我在设置学习率的时候都会从一个特别小的数开始，比如0.0001，看看指标的变化，在增大一点学习率比如0.001，再看看变化，确定模型没问题，然后在开始。当然，学习率的设置还有很多方式,比如模拟退火方式,Transform中的学习率代码就使用了这种:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):</span><br><span class="line">    def __init__(self, d_model, warmup_steps=4000):</span><br><span class="line">        super(CustomSchedule, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_model = tf.cast(self.d_model, tf.float32)</span><br><span class="line">        self.warmup_steps = warmup_steps</span><br><span class="line"></span><br><span class="line">    def __call__(self, step):</span><br><span class="line">        arg1 = tf.math.rsqrt(step)</span><br><span class="line">        arg2 = step * (self.warmup_steps ** -1.5)</span><br><span class="line">        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)</span><br></pre></td></tr></table></figure></li></ol><p>也就是让学习率先快诉增大，当step达到warmup_steps后后在慢慢减小<br>bert中的学习率是这样设置的:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = tf.train.polynomial_decay(</span><br><span class="line">      learning_rate,</span><br><span class="line">      global_step,</span><br><span class="line">      num_train_steps,</span><br><span class="line">      end_learning_rate=0.0,</span><br><span class="line">      power=1.0,</span><br><span class="line">      cycle=False)</span><br><span class="line">  if num_warmup_steps:</span><br><span class="line">    global_steps_int = tf.cast(global_step, tf.int32)</span><br><span class="line">    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)</span><br><span class="line">    global_steps_float = tf.cast(global_steps_int, tf.float32)</span><br><span class="line">    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)</span><br><span class="line">    warmup_percent_done = global_steps_float / warmup_steps_float</span><br><span class="line">    warmup_learning_rate = init_lr * warmup_percent_done</span><br><span class="line">    is_warmup = tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32)</span><br><span class="line">    learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)</span><br></pre></td></tr></table></figure></p><p>它也是先让学习率快速增大，当step达到warmup_steps时，在安装多项式方式衰减 顺便提一下，<code>learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)</code>这行代码值得学习</p><h2 id="batchsize"><a href="#batchsize" class="headerlink" title="batchsize"></a>batchsize</h2><ol><li>batch 和 batch 之间差别太大,训练难以收敛,形成震荡</li><li>batchsize 增大会使梯度优化方向更准</li><li>随着 batch_size 增大，处理相同数据量的速度越快。</li><li>随着 batch_size 增大，达到相同精度所需要的 epoch 数量越来越多。</li><li>由于上述两种因素的矛盾，batch_size 增大到某个时候，达到时间上的最优。</li><li>增大 batchsize 能够有效的利用GPU并行能力</li><li>GPU对batchsize为2的整数次幂效果更好</li></ol><h1 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h1><p>首先需要知道的是，对于隐藏层和层数刚开始的设置要紧盯训练样本的量，要保证模型的参数量不高于样本量的一半，有权威称$\frac{1}{10}$最好.反正你不要写完模型后参数太大,你想想用一千条数据去训练好几百万的参数能学到点啥？下面给出个统计模型参数的tf代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def __get_parametres(self):</span><br><span class="line">       total_parameters = 0</span><br><span class="line">       for variable in tf.trainable_variables():</span><br><span class="line">           shape = variable.get_shape()</span><br><span class="line">           variable_parameters = 1</span><br><span class="line">           for dim in shape:</span><br><span class="line">               variable_parameters *= dim.value</span><br><span class="line">           total_parameters += variable_parameters</span><br><span class="line">       tf.logging.info(&quot;总参数量为:&#123;total_parameters&#125;&quot;.format(total_parameters=total_parameters))</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;这篇博客会一直更新.&lt;br&gt;对于初学者而言，兴趣往往都在模型构成上面,但是，只有真正成为调参工程师才能深刻体会到调参对于模型的重要性，虽然我也只能算入门级,但已经被坑了好几次了。。。。。说多了都是泪啊。&lt;br&gt;总的来说,训
      
    
    </summary>
    
      <category term="深度学习" scheme="https://lingyixia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>总有一个算法让你惊艳</title>
    <link href="https://lingyixia.github.io/2019/07/06/interestAlgorighm/"/>
    <id>https://lingyixia.github.io/2019/07/06/interestAlgorighm/</id>
    <published>2019-07-06T05:34:05.000Z</published>
    <updated>2021-09-20T03:42:25.852Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>总有一个算法让你惊艳</p></blockquote></blockquote><h1 id="随机洗牌"><a href="#随机洗牌" class="headerlink" title="随机洗牌"></a>随机洗牌</h1><p>问:一副牌54张，如何洗牌才能让最公平。<br>什么叫公平?就是每张牌在每个位置的概率都一样就是公平, Knuth 老爷子给出来这样的算法:</p><pre><code>1. 初始化任意顺序2. 从最后一张牌开始,设为第K张,然后从[1,K]张中任选一张与其交换3. 从[1,K-1]张牌中任选一张和第K-1张交换... ...</code></pre><p>伪代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for(int i =n;i&gt;=1;i--)</span><br><span class="line">&#123;</span><br><span class="line">    swap(array[i],random(1,i))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>是不是贼简单？那为啥这个算法能做到呢？下面证明一下:<br>假设现在有5张牌，初始顺序为$1,2,3,4,5$<br>首先，从1~5(这是下标)中任选一张比如选到了2,那么用2和5交换得到$1,5,3,4,2$,也就是说，第一次交换2在最后一个位置的概率是1/5(也可以说任意一个数字在最后一个位置的概率是1/5),那么我们进行第二次交换,从1~4(这是下标)中任选一个和第4个交换,比如我们选到了1,在此之前要保证1没有在之前的步骤被选中也就是4/5,现在选中1的概率是1/4,那么两者相乘得到1/5,也就是1在第4个位置的概率是1/5也可以说任意一个数字在最后一个位置的概率是1/5),后面的不用多说了吧。<br>这题和<a href="https://lingyixia.github.io/2019/04/14/PoolSampling/">蓄水池采样算法</a>由异曲同工之妙。<br>用这个算法还可以在中途随意停止,比如有54张牌,我们要找任意10张牌进行公平洗牌,那么只需要上诉步骤执行10次就可以了.</p><h1 id="轮盘赌随机算法"><a href="#轮盘赌随机算法" class="headerlink" title="轮盘赌随机算法"></a>轮盘赌随机算法</h1><p>$\qquad$俄罗斯轮盘赌（Russian roulette）是一种残忍的赌博游戏。与其他使用扑克、色子等赌具的赌博不同的是，俄罗斯轮盘赌的赌具是左轮手枪和人的性命。俄罗斯轮盘赌的规则很简单：在左轮手枪的六个弹槽中放入一颗或多颗子弹，任意旋转转轮之后，关上转轮。游戏的参加者轮流把手枪对着自己的头，扣动板机；中枪的当然是自动退出，怯场的也为输，坚持到最后的就是胜者。旁观的赌博者，则对参加者的性命压赌注。<br>$\qquad$现在我们把问题抽象化，一个饼图，上面有一个指针，我们我们如何通过一个算法来确定到底每次拨动指针后会指向哪一个？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;ctime&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line">#include &lt;cstdlib&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">#define N  999 //精度为小数点后面3位</span><br><span class="line"></span><br><span class="line">int RouletteWheelSelection()</span><br><span class="line">&#123;</span><br><span class="line">    srand((unsigned) time(NULL));</span><br><span class="line">    float m = rand() % (N + 1) / (float)(N + 1);</span><br><span class="line">    cout &lt;&lt; m &lt;&lt; endl;</span><br><span class="line">    float Probability_Total = 0;</span><br><span class="line">    int Selection = 0;</span><br><span class="line">    vector&lt;float&gt; Probabilities = &#123;0.14, 0.35, 0.2, 0.07, 0.23, 0.01&#125;;</span><br><span class="line">    for(int i = 0; i &lt; Probabilities.size(); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        Probability_Total += Probabilities[i];</span><br><span class="line">        if(Probability_Total &gt;= m)</span><br><span class="line">        &#123;</span><br><span class="line">            Selection = i;</span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return Selection;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    cout &lt;&lt; RouletteWheelSelection();</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>其实只要意识到需要最终各部分的概率与占比相同即可。</p></blockquote></blockquote><h1 id="切分句子"><a href="#切分句子" class="headerlink" title="切分句子"></a>切分句子</h1><p>在NLP中经常会有这样的需求,对于训练数据有少部分会特别长,远远超出平均长度,那么我们就需要对句子进行拆分,但是不能直接安长度切，这样很可能会切断关键词,切分方法一般是在无用的地方切分,比如标点符号,现在给出一个算法实现这个功能:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def data_cut(sentence, cut_chars, cut_length, min_length):</span><br><span class="line">    if len(sentence) &lt;= cut_length:</span><br><span class="line">        return [sentence]</span><br><span class="line">    else:</span><br><span class="line">        for char in cut_chars:</span><br><span class="line">            start = min_length  # 防止直接从头几个就找到了，这样切的太短</span><br><span class="line">            end = len(sentence) - (min_length - 1)  # 防止从最后几个找到了,这样切的也太短</span><br><span class="line">            if char in sentence[start:end]:</span><br><span class="line">                index = sentence[start:end].index(char)</span><br><span class="line">                return data_cut(sentence[:start + index], cut_chars, cut_length, min_length) + \</span><br><span class="line">                       data_cut(sentence[start + index:], cut_chars, cut_length, min_length)</span><br><span class="line">    return [sentence]  # 如果没有找到切分点那就不管句子长度直接返回</span><br></pre></td></tr></table></figure></p><blockquote><p>&gt;<br>参数说明:sentence是一个list,内容是sentence每个字符<br>       cut_chars是一个list,内容是切分字符，按优先级排序<br>       cut_length是int类型，表示切分多长的句子,这个长度以及以下的直接返回<br>       min_length:切分后子句子的最短长度<br>return:二维list,注意，每个切分后的句子不一定全小于cut_length,有些另类的句子可能没有切分点,这样的需要手动处理<br>eg:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def data_cut(sentence, cut_chars, cut_length, min_length):</span><br><span class="line">    if len(sentence) &lt;= cut_length:</span><br><span class="line">        return [sentence]</span><br><span class="line">    else:</span><br><span class="line">        for char in cut_chars:</span><br><span class="line">            start = min_length</span><br><span class="line">            end = len(sentence) - (min_length - 1)</span><br><span class="line">            if char in sentence[start:end]:</span><br><span class="line">                index = sentence[start:end].index(char)</span><br><span class="line">                return data_cut(sentence[:start + index], cut_chars, cut_length, min_length) + data_cut(</span><br><span class="line">                    sentence[start + index:], cut_chars, cut_length, min_length)</span><br><span class="line">    return [sentence]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    sentence = &quot;三种上下文特征：单词、n-gram 和字符在词嵌入文献中很常用。大多数词表征方法本质上利用了词-词的共现统计，即使用词作为上下文特征（词特征）。受语言建模问题的启发，开发者将 n-gram 特征引入了上下文中。词到词和词到 n-gram 的共现统计都被用于训练 n-gram 特征。对于中文而言，字符（即汉字）通常表达了很强的语义。为此，开发者考虑使用词-词和词-字符的共现统计来学习词向量。字符级的 n-gram 的长度范围是从 1 到 4（个字符特征）。&quot;</span><br><span class="line">    l = data_cut(list(sentence), [&apos;，&apos;, &apos;。&apos;], 150, 5)</span><br><span class="line">    print(l)</span><br><span class="line">#自己运行看看吧，不好写</span><br></pre></td></tr></table></figure></p></blockquote><h1 id="约瑟夫环"><a href="#约瑟夫环" class="headerlink" title="约瑟夫环"></a>约瑟夫环</h1><blockquote><blockquote><p>n个人拉成圈，按照1~n标号,从1开始报数,报到m的人出局，每一次有人出局后重新排号，1号是重新排号前m的下一个人.</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int JosephRing(int n, int m)</span><br><span class="line">&#123;</span><br><span class="line">    if (n == 1) return n;</span><br><span class="line">    return (JosephRing(n - 1, m) + m - 1) % n + 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>解释:对于任何一个状态而言，每个位置设为$old_i$,如果现在把该状态的第$m$个剔除掉，则重新排号后每个位置设为$new_i$,两者的关系</p><script type="math/tex; mode=display">old_i = (new +m-1)%n+1</script><p>$JosephRing(n,m)$表示有$n$个人,每次删除$m$号人最终剩下的号码，每次递归回来其实都是再算</p></blockquote></blockquote><h1 id="点和线"><a href="#点和线" class="headerlink" title="点和线"></a>点和线</h1><h2 id="一个知识点"><a href="#一个知识点" class="headerlink" title="一个知识点"></a>一个知识点</h2><p>定义: 平面上的三点$A(x1,y1),B(x2,y2),C(x3,y3)$的面积量:</p><script type="math/tex; mode=display">S(A,B,C)=\frac{1}{2}  \left|\begin{array}{}    x_1 &    y_1    & 1 \\     x_2 &    y_2   & 1\\     x_3 & y_3 & 1 \end{array}\right|</script><p>其中: 当A、B、C逆时针时S为正的,反之S为负的。<br>证明如图:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/dotandline.png" alt title>                </div>                <div class="image-caption"></div>            </figure><br>也就是说，平面三点一定能写成一个直角梯形减两个直角三角形的形式.<br>即:</p><script type="math/tex; mode=display">S(A,B,C)=\frac{(x_1y_2+x_3y_1+x_2y_3-x_1y_3-x_2y_1-x_3y_2)}{2}</script><p>正好是上诉行列式.</p><h2 id="一个应用"><a href="#一个应用" class="headerlink" title="一个应用"></a>一个应用</h2><p>令矢量的起点为A，终点为B，判断的点为C，<br>如果S(A，B，C)为正数，则C在矢量AB的左侧；<br>如果S(A，B，C)为负数，则C在矢量AB的右侧；<br>如果S(A，B，C)为0，则C在直线AB上</p><h1 id="点和矩形"><a href="#点和矩形" class="headerlink" title="点和矩形"></a>点和矩形</h1><blockquote><blockquote><p>判断坐标系内某电点是否在某个矩形内部</p></blockquote></blockquote><p>只需要将点与四个角连接，计算形成的四个三角形面积(海伦公式)和是否等于矩形面积,等于则在内部，否则在外部。</p><h1 id="判断平面内两线段相交"><a href="#判断平面内两线段相交" class="headerlink" title="判断平面内两线段相交"></a>判断平面内两线段相交</h1><ol><li>计算两线段所在的直线的的交点(如果有),然后看该交点是否在两条线段上即可</li><li>若两直线相交,则如图:,,,算了不弄图了，也简单，假设有线段AB 和 CD 若相交则必定C 和 D 在 AB 的两侧，则有$\vec{AB} \times \vec{AC} $和$\vec{AB} \times \vec{AD} $必定异号 $\vec{AB} \times \vec{AC} $和$\vec{CA} \times \vec{CD} $必定异号，判断这个即可</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;总有一个算法让你惊艳&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;随机洗牌&quot;&gt;&lt;a href=&quot;#随机洗牌&quot; class=&quot;headerlink&quot; title=&quot;随机洗牌&quot;&gt;&lt;/a&gt;随机
      
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>二进制相关</title>
    <link href="https://lingyixia.github.io/2019/06/23/Binary/"/>
    <id>https://lingyixia.github.io/2019/06/23/Binary/</id>
    <published>2019-06-23T14:49:15.000Z</published>
    <updated>2021-09-19T18:01:25.944Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数组与-2-k"><a href="#数组与-2-k" class="headerlink" title="数组与$2^k$"></a>数组与$2^k$</h1><blockquote><blockquote><p>一个整形数组，每次取的元素和必须是$2^k$,问至少多少次能取完?假设一定能完成这个任务。</p></blockquote></blockquote><p>答:只需要把所有的数字加起来，然后换为二进制，里面有几个1就有几次.(找不到标准答案，感觉应该对)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数组与-2-k&quot;&gt;&lt;a href=&quot;#数组与-2-k&quot; class=&quot;headerlink&quot; title=&quot;数组与$2^k$&quot;&gt;&lt;/a&gt;数组与$2^k$&lt;/h1&gt;&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;一个整形数组，每次取的元素和必须是$2^k$
      
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>概率计算问题</title>
    <link href="https://lingyixia.github.io/2019/06/23/machineProblem/"/>
    <id>https://lingyixia.github.io/2019/06/23/machineProblem/</id>
    <published>2019-06-23T11:14:33.000Z</published>
    <updated>2021-09-19T18:01:25.960Z</updated>
    
    <content type="html"><![CDATA[<p>若干有趣的有关概率计算的小问题<br><a id="more"></a></p><h1 id="抛硬币"><a href="#抛硬币" class="headerlink" title="抛硬币"></a>抛硬币</h1><blockquote><blockquote><p>一枚均匀硬币，平均抛多少次才能连续两次正面向上?</p></blockquote></blockquote><p>答:设<script type="math/tex">N_k</script>表示连续$k$次正面向上的随机变量,则<script type="math/tex">E(N_k)</script>表示连续$k$面朝上需要抛硬币次数的均值,则:</p><script type="math/tex; mode=display">N_k=N_{k-1}+p+(1-p)[1+N_k]</script><p>下面解释这个式子.<br>要想得到<script type="math/tex">N_k</script>次正面向上之前必须有<script type="math/tex">N_{k-1}</script>次正面向上,因此第一项的<script type="math/tex">N_{k-1}</script>就是这个意思,第二项的意思是达到$N_{k-1}$之的下一个伯努利实验如果成功,则实验到此为止。第三项的意思是如果不成功,则一切从头开始.</p><script type="math/tex; mode=display">E(N_k)=E(N_{k-1})+p+(1-p)[1+E(N_k)]</script><p>得:</p><script type="math/tex; mode=display">E(N_k) = \frac{E(N_{k-1})+1}{p}</script><p>得:</p><script type="math/tex; mode=display">\begin{align}E(N_K)&=\frac{1}{p^N}+\frac{1}{p^{N-1}}+...+\frac{1}{p} \\&=\frac{p^N-1 }{p^N(p-1)}\end{align}</script><h1 id="羊、车、门"><a href="#羊、车、门" class="headerlink" title="羊、车、门"></a>羊、车、门</h1><blockquote><blockquote><p>有3扇关闭的门，一扇门后面停着汽车，另外两扇门后是山羊，只有主持人知道每扇门后面是什么。参赛者先选择一扇门，在开启它之前，主持人会开启另外一扇门，露出门后的山羊，然后如果你是参赛者，你换不换门? </p></blockquote></blockquote><p>正常人会认为换不换几率都是1/2,但是真的是这样么?现在我们把此题用解答一下:</p><ol><li>如果换门，那么所有的情况只有三种:<ul><li>参赛者挑山羊一号，主持人挑山羊二号。转换将成功</li><li>参赛者挑山羊二号，主持人挑山羊一号。转换将成功</li><li>参赛者挑汽车，主持人挑两头山羊的任何一头。转换将失败<br>或者说不换门所有的情况只有三种:</li><li>参赛者挑山羊一号，主持人挑山羊二号。不换将失败</li><li>参赛者挑山羊二号，主持人挑山羊一号。不换则失败</li><li>参赛者挑汽车，主持人挑两头山羊的任何一头。转换将成功<br>或者这样说:<br>已知第一个步骤得到车的概率是1/3，那第二个步骤换门得到车的概率是0，—&gt; 1/3 <em> 0 =0<br>已知第一个步骤得到羊的概率是2/3，那第二个步骤换门得到车的概率是1，—&gt; 2/3 </em> 1 =2/3<br>总的来说，换门得到的概率是2/3</li></ul></li><li>现在把这个问题普遍化，问题就是计算换门和不换门两种情况下成功的概率,假设$N$扇门中有一个车，其他的都是羊<ul><li>不换门:成功的条件是第一次选择了车 设事件A=第一次选择了车 则$P(A)=\frac{1}{N}$</li><li>换门:成功的条件是第一次选择了羊 第二次选择了车   设事件B=第一次选择了羊，事件C=第二次选择了车 $P(C|B)=\frac{N-1}{N} \times \frac{1}{N-2} = \frac{N-1}{N-2} \times \frac{1}{N} &gt; \frac{1}{N}$<br>实例代码,更改N可以更改门的数量:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import time  # 导入时间库</span><br><span class="line">from random import choice</span><br><span class="line"></span><br><span class="line">TIMES = 100000  # 用来标记做了多少次选择</span><br><span class="line">che = 0  # 用来记录多少次选择了车</span><br><span class="line">yang = 0  # 用来记录多少次选了羊</span><br><span class="line">N = 5  # 标记多少个门</span><br><span class="line">time.clock()  # 开始计时</span><br><span class="line"># for 循环用于选择并判断多少次选择了车</span><br><span class="line">for i in range(TIMES):</span><br><span class="line">    l = [&apos;c&apos;] + [&apos;s&apos; + str(i + 1) for i in range(N - 1)]</span><br><span class="line">    # eg: l = [&apos;c&apos;, &apos;s1&apos;, &apos;s2&apos;, &apos;s3&apos;, &apos;s4&apos;]  # 选择内容，c代表车，sn代表第N只羊</span><br><span class="line">    first = choice(l)  # 随机从中选择一个</span><br><span class="line">    if first == &apos;c&apos;:</span><br><span class="line">        yang = yang + 1  # 当选择车时，主持人亮出一只羊，更改选择后就是选择了羊</span><br><span class="line">    else:</span><br><span class="line">        l.remove(first)  # 扔掉first选的的羊</span><br><span class="line">        sheeps = l.copy()</span><br><span class="line">        sheeps.remove(&apos;c&apos;)#主持人在剩下的羊中选择一只</span><br><span class="line">        sheep = choice(sheeps)  # 主持人选了这个羊</span><br><span class="line">        l.remove(sheep)</span><br><span class="line">        second = choice(l)</span><br><span class="line">        if second == &apos;c&apos;:#如果第二次选中了车</span><br><span class="line">            che = che + 1</span><br><span class="line"></span><br><span class="line">car = che / TIMES  # 得出选择车的概率</span><br><span class="line">sheep = yang / TIMES  # 得出选择羊的概率</span><br><span class="line">print(&quot;car =&quot;, car)</span><br><span class="line">print(&quot;sheep =&quot;, sheep)</span><br></pre></td></tr></table></figure></li></ul></li></ol><h1 id="圆上任取三点"><a href="#圆上任取三点" class="headerlink" title="圆上任取三点"></a>圆上任取三点</h1><blockquote><blockquote><p>圆上任取三点组成锐角三角形的概率是多少?此题可以转化为圆上任取三点不在同一半圆内的概率是多少?</p></blockquote></blockquote><p>假设先任取A和B，他们一定在同一半圆内,假设他们连接O组成的角$AOB$为X(x$\in[0,\pi]$)，则C点必定在OA和OB反向延长线的夹角内才能保证不在同一半圆内,该角度也必定为X,此时概率为:$\frac{x}{2 \pi}$</p><script type="math/tex; mode=display">P= \frac{1}{\pi} \times \int_0^{\pi} \frac{x}{2 \pi} dx = \frac{1}{4}</script><h1 id="生日悖论"><a href="#生日悖论" class="headerlink" title="生日悖论"></a>生日悖论</h1><p>至少有多少人才能保证有两人生日相同?<br>答案是366，一点没错，但是我们改一下题:<br>一个屋子里有23人，计算有两人生日相同的概率。</p><script type="math/tex; mode=display">P = 1- \frac{C_{365}^{23}}{365^{23}}≈1-0.493=0.507</script><p>也就是说，当有23人的时候，就有50%的概率有两人生日相同,当人70人时，在通过上诉公示可以得到概率达到了99.9%,几乎是100%，也就是说，不需要366人，只需要70人基本上就可以断定至少两人生日相同了.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;若干有趣的有关概率计算的小问题&lt;br&gt;
    
    </summary>
    
      <category term="数学" scheme="https://lingyixia.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="概率论" scheme="https://lingyixia.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>常见博弈游戏</title>
    <link href="https://lingyixia.github.io/2019/06/23/gametheory/"/>
    <id>https://lingyixia.github.io/2019/06/23/gametheory/</id>
    <published>2019-06-23T07:07:55.000Z</published>
    <updated>2021-09-19T18:01:25.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Nim-Game"><a href="#Nim-Game" class="headerlink" title="Nim Game"></a>Nim Game</h2><blockquote><blockquote><p>有n堆石子，每堆石子有若干石子，两个人轮流从某一堆取任意多的物品，规定每次至少取一个，多者不限。取走最后石子的人获胜。</p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Nim-Game&quot;&gt;&lt;a href=&quot;#Nim-Game&quot; class=&quot;headerlink&quot; title=&quot;Nim Game&quot;&gt;&lt;/a&gt;Nim Game&lt;/h2&gt;&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;有n堆石子，每堆石子有若干石子，两个人轮
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>常用分布</title>
    <link href="https://lingyixia.github.io/2019/06/23/commondistribution/"/>
    <id>https://lingyixia.github.io/2019/06/23/commondistribution/</id>
    <published>2019-06-23T07:03:04.000Z</published>
    <updated>2021-09-19T18:01:25.957Z</updated>
    
    <content type="html"><![CDATA[<p>常用分布若干知识点记录<br><a id="more"></a></p><h1 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h1><blockquote><blockquote><p>进行一次实验,实验的结果只有两种:成功的概率是$p$,则失败的概率是$1-p$。eg:一个硬币抛一次人结果。</p><script type="math/tex; mode=display">P(X=1)=p \\P(X=0)=1-p</script><p>期望:$E(X)=P$<br>方差:$D(X)=P \times (1-P)$</p></blockquote></blockquote><h1 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h1><blockquote><blockquote><p>n次伯努利实验,成功k次。g:一个硬币抛n次，k次正面朝上。</p><script type="math/tex; mode=display">P(X=k)=C_n^kp^{k}(1-p)^{n-k}</script><p>期望:</p><script type="math/tex; mode=display">\begin{aligned}E(X) &=\sum_{k=0}^nk C_n^kp^{k}(1-p)^{n-k} \\&=\sum_{k=0}^nk \frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k} \\&=n\sum_{k=0}^n \frac{(n-1)!}{(k-1)!(n-k)!} p^{k}(1-p)^{n-k} \\&= np\sum_{k=0}^n C_{n-1}^{k-1}p^{k-1}(1-p)^{(n-1)-(k-1)} \\&=np(p+(1-p))^{n-1} \\&=np\end{aligned}</script><script type="math/tex; mode=display">D(X)=np(1-p)</script><p>过两天在证明</p></blockquote></blockquote><h1 id="几何分布"><a href="#几何分布" class="headerlink" title="几何分布"></a>几何分布</h1><blockquote><blockquote><p>考虑伯努利试验，每次成功的概率为$p$,$0&lt;p&lt;1$,重复试验直到试验首次成功为止。令X表示需要试验的次数，那么:</p><script type="math/tex; mode=display">P(X=n)=(1−p)^{n−1}p</script><ul><li>$E(X)=\frac{1}{p}$</li><li>$D(X)=\frac{1-p}{p^2}$<br><a href="https://zlearning.netlify.com/math/probability/geometry-distribution.html" target="_blank" rel="noopener">参考</a></li></ul></blockquote></blockquote><h1 id="补充"><a href="#补充" class="headerlink" title="补充:"></a>补充:</h1><script type="math/tex; mode=display">\begin{aligned}D(X) &= E(X-E(X)^2) \\&=E(X^2-2XE(X)+E(X)^2) \\&=E(X^2)-2E(x)^2+E(X)^2 \\&=E(X^2)-E(X)^2\end{aligned}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;常用分布若干知识点记录&lt;br&gt;
    
    </summary>
    
      <category term="数学" scheme="https://lingyixia.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="概率论" scheme="https://lingyixia.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
</feed>
