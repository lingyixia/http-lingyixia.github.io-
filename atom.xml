<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>灵翼俠的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://lingyixia.github.io/about/"/>
  <updated>2018-10-29T02:50:04.121Z</updated>
  <id>https://lingyixia.github.io/about/</id>
  
  <author>
    <name>陈飞宇</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>股票最大利润</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/leetcode_121/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/leetcode_121/</id>
    <published>2018-12-20T08:05:09.705Z</published>
    <updated>2018-10-29T02:50:04.121Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Say you have an array for which the ith element is the price of a given stock on day i.</p></blockquote><p>If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.</p><p>Note that you cannot sell a stock before you buy one.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">//第一种方法：记录到i-1为止，最小下标，然后比较prices[i]-prices[currintMinIndex]</span><br><span class="line">int maxProfit(vector&lt;int&gt;&amp; prices) </span><br><span class="line">&#123;</span><br><span class="line">int currintMinIndex = 0;</span><br><span class="line">int currentMax = 0;</span><br><span class="line">int maxSum = 0;</span><br><span class="line">for(int i = 1;i&lt;prices.size();i++)</span><br><span class="line">&#123;</span><br><span class="line">if(prices[i-1]&lt;prices[currintMinIndex])</span><br><span class="line">&#123;</span><br><span class="line">currintMinIndex = i-1;</span><br><span class="line">&#125;</span><br><span class="line">currentMax = prices[i] - prices[currintMinIndex];</span><br><span class="line">maxSum = maxSum&gt;currentMax?maxSum:currentMax;</span><br><span class="line">&#125;     </span><br><span class="line">return maxSum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">//第二种方法：计算prices[i]-price[i-1],然后题目就可以转为连续数组最大值问题</span><br><span class="line">int maxProfit(vector&lt;int&gt;&amp; prices) </span><br><span class="line">&#123;</span><br><span class="line">int currentProfit = 0;</span><br><span class="line">int currentMaxProfit=0;</span><br><span class="line">int maxProfit = 0;//此处不能是INT_MIN，因为有可能[7,6,4,3,1]，即一直下降，此时可以选择不买不买，即利润最低为0</span><br><span class="line">for(int i = 1;i&lt;prices.size();i++)</span><br><span class="line">&#123;</span><br><span class="line">currentProfit = prices[i]-prices[i-1];</span><br><span class="line">currentMaxProfit += currentProfit;</span><br><span class="line">maxProfit = currentMaxProfit&gt;maxProfit?currentMaxProfit:maxProfit;</span><br><span class="line">currentMaxProfit = currentMaxProfit&lt;0?0:currentMaxProfit;</span><br><span class="line">&#125;</span><br><span class="line">return maxProfit&gt;0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      求股票最大利润
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/about/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="leetcode" scheme="https://lingyixia.github.io/about/tags/leetcode/"/>
    
      <category term="动态规划" scheme="https://lingyixia.github.io/about/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>背包问题</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/backpack/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/backpack/</id>
    <published>2018-12-20T08:05:09.692Z</published>
    <updated>2018-12-20T12:57:48.156Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>0-1背包、完全背包、多重背包问题简记</p></blockquote><p>1.0-1背包问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">//递推公式为：values[i][j] = max(values[i - 1][j], values[i-1][j - woods[i-1].volume] + woods[i-1].value);</span><br><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;vector&gt;</span><br><span class="line">#include&lt;algorithm&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int items[5] = &#123;0,0,0,0,0&#125;;</span><br><span class="line">struct Woods</span><br><span class="line">&#123;</span><br><span class="line">int volume;</span><br><span class="line">int value;</span><br><span class="line">&#125;;</span><br><span class="line">void backPack(vector&lt;Woods&gt; woods, int bag);</span><br><span class="line">void findItems(int** values, vector&lt;Woods&gt; woods, int i, int j);</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">vector&lt;Woods&gt; woods = &#123;Woods&#123;2,3&#125;,Woods&#123;3,4&#125;,Woods&#123;4,5&#125;,Woods&#123;5,6&#125; &#125;;</span><br><span class="line">backPack(woods,8);</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line">void backPack(vector&lt;Woods&gt; woods, int bag)</span><br><span class="line">&#123;</span><br><span class="line">int** values = new int*[woods.size()+1];</span><br><span class="line">for (int i = 0; i &lt;= woods.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">values[i] = new int[bag+1]();</span><br><span class="line">&#125;</span><br><span class="line">for (int i = 1; i &lt;= woods.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">for (int j = 1; j &lt;= bag; j++)</span><br><span class="line">&#123;</span><br><span class="line">if (j &lt; woods[i-1].volume)</span><br><span class="line">&#123;</span><br><span class="line">values[i][j] = values[i - 1][j];</span><br><span class="line">&#125;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">values[i][j] = max(values[i - 1][j], values[i-1][j - woods[i-1].volume] + woods[i-1].value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">findItems(values, woods, 4, 8);</span><br><span class="line">for (int i = 0; i &lt;= woods.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">delete [] values[i];</span><br><span class="line">&#125;</span><br><span class="line">delete values;</span><br><span class="line">&#125;</span><br><span class="line">void findItems(int** values, vector&lt;Woods&gt; woods,int i ,int j)</span><br><span class="line">&#123;</span><br><span class="line">if (i&gt;0)</span><br><span class="line">&#123;</span><br><span class="line">if (values[i][j] == values[i-1][j])</span><br><span class="line">&#123;</span><br><span class="line">items[i] = 0;</span><br><span class="line">findItems(values, woods,i-1,j);</span><br><span class="line">&#125;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">items[i] = 1;</span><br><span class="line">findItems(values, woods,i-1,j-woods[i-1].volume);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上诉方法使用二维数组保存中间值，比较消耗空间，而且我们可以看出，对于每一步要求的values[i][j]而言，只依赖于values[i-1][:],即前一行，因此我们只需要一维数组即可，循环保存前一行数据。但是这样貌似就无法找物品组成了，回头在想。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">//空间优化写法</span><br><span class="line">void backPack(vector&lt;Woods&gt; woods, int bag)</span><br><span class="line">&#123;</span><br><span class="line">int* values = new int[bag + 1]();</span><br><span class="line">for (int i = 1; i &lt;= woods.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">for (int j = bag; j &gt;= 0; j--)//倒序，保证在改完某个数据之前，它依赖的前面的数据还没改</span><br><span class="line">&#123;</span><br><span class="line">if (j - woods[i-1].volume &gt;= 0)</span><br><span class="line">&#123;</span><br><span class="line">values[j] = max(values[j], values[j - woods[i-1].volume] + woods[i-1].value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//再次改善</span><br><span class="line">void backPack(vector&lt;Woods&gt; woods, int bag)</span><br><span class="line">&#123;</span><br><span class="line">int* values = new int[bag + 1]();</span><br><span class="line">for (int i = 1; i &lt;= woods.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">for (int j = bag; j &gt;= woods[i-1].volume; j--)//倒序，保证更改某个数据之前它依赖的前面的数据未改</span><br><span class="line">&#123;</span><br><span class="line">values[j] = max(values[j], values[j - woods[i-1].volume] + woods[i-1].value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2.完全背包问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">与0-1背包唯一的不同就是递推公式：values[i][j] = max(values[i - 1][j], values[i][j - woods[i-1].volume] + woods[i-1].value);</span><br><span class="line">//不在详细说明</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//空间优化写法</span><br><span class="line">void backPack(vector&lt;Woods&gt; woods, int bag)</span><br><span class="line">&#123;</span><br><span class="line">int* values = new int[bag + 1]();</span><br><span class="line">for (int i = 1; i &lt;= woods.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">for (int j = woods[i-1].volume; j &lt;= bag; j++)//正序</span><br><span class="line">&#123;</span><br><span class="line">values[j] = max(values[j], values[j - woods[i-1].volume] + woods[i-1].value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3.多重背包问题（即每种物品的数量有上限）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">//在0-1背包的基础上增加数量循环判断,递推公式为：values[i][j] = max(values[i][j], values[i-1][j - k * woods[i - 1].volume] + k * woods[i - 1].value);</span><br><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;vector&gt;</span><br><span class="line">#include&lt;algorithm&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">struct Woods</span><br><span class="line">&#123;</span><br><span class="line">int volume;</span><br><span class="line">int value;</span><br><span class="line">int num;</span><br><span class="line">&#125;;</span><br><span class="line">void backPack(vector&lt;Woods&gt; woods, int bag);</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">vector&lt;Woods&gt; woods = &#123; Woods&#123; 80 ,20 ,4 &#125;,Woods&#123; 40 ,50, 9 &#125;,Woods&#123; 30 ,50, 7 &#125;,Woods&#123; 40 ,30 ,6 &#125;,Woods&#123; 20 ,20 ,1 &#125; &#125;;</span><br><span class="line">backPack(woods, 1000);</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line">void backPack(vector&lt;Woods&gt; woods, int bag)</span><br><span class="line">&#123;</span><br><span class="line">int** values = new int*[woods.size() + 1];</span><br><span class="line">for (int i = 0; i &lt;= woods.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">values[i] = new int[bag + 1]();</span><br><span class="line">&#125;</span><br><span class="line">for (int i = 1; i &lt;= woods.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">for (int j = 1; j &lt;= bag; j++)</span><br><span class="line">&#123;</span><br><span class="line">values[i][j] = values[i - 1][j];</span><br><span class="line">for (int k = 1; k &lt;= woods[i - 1].num; k++)</span><br><span class="line">&#123;</span><br><span class="line">if (j &lt; k * woods[i - 1].volume)</span><br><span class="line">&#123;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">values[i][j] = max(values[i][j], values[i-1][j - k * woods[i - 1].volume] + k * woods[i - 1].value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">for (int i = 0; i &lt;= woods.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">delete[] values[i];</span><br><span class="line">&#125;</span><br><span class="line">delete values;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">//空间优化写法</span><br><span class="line">void backPack(vector&lt;Woods&gt; woods, int bag)</span><br><span class="line">&#123;</span><br><span class="line">int* values = new int[bag + 1]();</span><br><span class="line">for (int i = 1; i &lt;= woods.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">for (int j = bag; j &gt;= 0; j--)</span><br><span class="line">&#123;</span><br><span class="line">for (int k = 1; k &lt;= woods[i - 1].num; k++)</span><br><span class="line">&#123;</span><br><span class="line">if (j &lt; k * woods[i - 1].volume)</span><br><span class="line">&#123;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">values[j] = max(values[j], values[j - k * woods[i - 1].volume] + k * woods[i - 1].value);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">delete[] values;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>二进制写法先不想了</p>]]></content>
    
    <summary type="html">
    
      三种背包问题
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/about/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="动态规划" scheme="https://lingyixia.github.io/about/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
      <category term="递归" scheme="https://lingyixia.github.io/about/tags/%E9%80%92%E5%BD%92/"/>
    
      <category term="背包" scheme="https://lingyixia.github.io/about/tags/%E8%83%8C%E5%8C%85/"/>
    
  </entry>
  
  <entry>
    <title>连续数组最大和</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/leetcode_53/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/leetcode_53/</id>
    <published>2018-12-20T08:05:09.691Z</published>
    <updated>2018-10-29T02:50:04.120Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum.</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">//动态规划</span><br><span class="line">//递推公式为DP[i] = max&#123;DP[i-1] + A[i],A[i]&#125;</span><br><span class="line">int maxSubArray(vector&lt;int&gt;&amp; nums)</span><br><span class="line">&#123;</span><br><span class="line">int currentMax = 0;//currentMax是i处以及之前的连续最大值</span><br><span class="line">int max = INT_MIN;</span><br><span class="line">for (int i = 0; i &lt; nums.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">currentMax += nums[i];</span><br><span class="line">max = currentMax&gt;max?currentMax:max;</span><br><span class="line">currentMax = currentMax&lt;0?0:currentMax;</span><br><span class="line">//if (currentMax&gt;max)</span><br><span class="line">//&#123;</span><br><span class="line">//max = currentMax;</span><br><span class="line">//&#125;</span><br><span class="line">//if (currentMax&lt;0)//之所以不用else if是考虑譬如[-2,1]的情况</span><br><span class="line">//&#123;</span><br><span class="line">//currentMax = 0;</span><br><span class="line">//&#125;</span><br><span class="line">&#125;</span><br><span class="line">return max;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      求连续数组最大和
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/about/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="leetcode" scheme="https://lingyixia.github.io/about/tags/leetcode/"/>
    
      <category term="动态规划" scheme="https://lingyixia.github.io/about/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>C++数组长度</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/cpp-array/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/cpp-array/</id>
    <published>2018-12-20T08:05:09.690Z</published>
    <updated>2018-12-20T12:56:50.442Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录函数内获取数组长度的坑</p></blockquote><p>看代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">void fun(int* array)</span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; array &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; sizeof(array);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">int array[] = &#123; 1,2,3,4,5,6 &#125;;</span><br><span class="line">cout &lt;&lt; array &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; sizeof(array) &lt;&lt; endl;</span><br><span class="line">fun(array);</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line">结果：</span><br><span class="line">00DEFCA0</span><br><span class="line">24</span><br><span class="line">00DEFCA0</span><br><span class="line">4</span><br></pre></td></tr></table></figure></p><p>可以看出，虽然函数体内外的array所指的地址相同，sizeof后并不一样，前者是实际数组的大小，后者是纯指针的大小（编译器决定），也就是说，当数组传到函数内后，就意味着它是一个纯指针，不再有数组的意义了，要想在函数内获取数组长度，只能以参数的形式传入。<br>总之，想在函数内获取数组长度就用vector吧！！！</p>]]></content>
    
    <summary type="html">
    
      C++中获取数组长度的坑
    
    </summary>
    
      <category term="基础" scheme="https://lingyixia.github.io/about/categories/%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="C++" scheme="https://lingyixia.github.io/about/tags/C/"/>
    
      <category term="坑" scheme="https://lingyixia.github.io/about/tags/%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>github博客小记</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/jekyll_github_record/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/jekyll_github_record/</id>
    <published>2018-12-20T08:05:09.674Z</published>
    <updated>2018-12-20T12:55:58.435Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>简单记录自己github page搭建的过程，只是为了记录自己踩一些坑，不是新手教程</p></blockquote><p>本博客使用了jekyll+github page,要使用到jekyll的主题，可以先把主题下载下来，本地预览调试，在传到github上托管。也可以直接fork到自己的github中，改名字，在下载下来。我是直接fork的<a href="http://BladeMasterCoder.github.io" target="_blank" rel="noopener">作者大大的博客</a>，然后改名字即可。 </p><p>1.：github如果不自定义域名，会强制使用https协议，这样，如果某个页面中引用了http协议的连接会出错（似乎只能用一样的协议），这时可以在header中加入：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;meta http-equiv=&quot;Content-Security-Policy&quot;content=&quot;upgrade-insecure-requests&quot;&gt;</span><br></pre></td></tr></table></figure><p>作用是强制http使用https。</p><p><strong>但是！！！！！！</strong><br>当本地预览的时候地址是一定要将该句注释掉，因为本地预览是http:/127.0.0.1:4000，不支持https。</p><p>2.jekyll本地预览首先需要安装<a href="https://rubyinstaller.org/downloads/" target="_blank" rel="noopener">ruby+devkit</a>，接下来：gem install jekyll（安装jekyll）gem install  bundler（安装bundler）<br>然后到博客目录下，新建Gemfile，里面输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source &apos;https://rubygems.org&apos;</span><br><span class="line">gem &apos;github-pages&apos;, group: :jekyll_plugins</span><br></pre></td></tr></table></figure><p>然后：bundle install(需要上诉Gemfile)，最后bundle exec jekyll serve启动服务器即可</p><p>3.simple-jekyll-search:最让人头疼的是找不到json,基本步骤按照<a href="https://github.com/christian-fei/Simple-Jekyll-Search" target="_blank" rel="noopener">这里</a>即可以，<br>一定不要忘了那个<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">layout: null</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>也就是说其实这根本不是一个标准的json，我就是没加上面那部分说啥也不行,坑苦了我，本来还以为不是json的一部分。</p>]]></content>
    
    <summary type="html">
    
      简单记录搭建github page的坑
    
    </summary>
    
      <category term="github" scheme="https://lingyixia.github.io/about/categories/github/"/>
    
    
      <category term="坑" scheme="https://lingyixia.github.io/about/tags/%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>数据科学常用之Pandas</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/data_science_pandas/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/data_science_pandas/</id>
    <published>2018-12-20T08:05:09.673Z</published>
    <updated>2018-12-20T12:56:57.811Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>包含数据科学常用Pandas函数</p></blockquote><p>1.pd.set_option<br>[set_option文档地址]  (<a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html?highlight=set_option#pandas.set_option" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html?highlight=set_option#pandas.set_option</a>)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#显示所有列</span><br><span class="line">pd.set_option(&apos;display.max_columns&apos;, None)</span><br><span class="line">#显示所有行</span><br><span class="line">pd.set_option(&apos;display.max_rows&apos;, None)</span><br><span class="line">#设置value的显示长度为100，默认为50</span><br><span class="line">pd.set_option(&apos;max_colwidth&apos;,100)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      数据科学常用Pandas函数
    
    </summary>
    
      <category term="数据科学" scheme="https://lingyixia.github.io/about/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"/>
    
    
      <category term="pandas" scheme="https://lingyixia.github.io/about/tags/pandas/"/>
    
      <category term="python" scheme="https://lingyixia.github.io/about/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow中Rnn的实现</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/tensorflow_rnn/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/tensorflow_rnn/</id>
    <published>2018-12-20T08:05:09.668Z</published>
    <updated>2018-12-20T12:58:59.435Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>包含TensorFlow中BasicRNNCell,BasicLSTMCell等的实现</p></blockquote><p>1.BasicRNNCell<br>基本结构如图:<br><img src="/img/SimpleRNN.png" alt="">   </p><blockquote><blockquote><p>在TensorFlow中，BasicRNNCellm每一步输出的state和output相同，源代码如下:</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def call(self, inputs, state):</span><br><span class="line">  &quot;&quot;&quot;Most basic RNN: output = new_state = act(W * input + U * state + B).&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  gate_inputs = math_ops.matmul(</span><br><span class="line">      array_ops.concat([inputs, state], 1), self._kernel)</span><br><span class="line">  gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)</span><br><span class="line">  output = self._activation(gate_inputs)</span><br><span class="line">  return output, output</span><br></pre></td></tr></table></figure><p>公式如下:</p><script type="math/tex; mode=display">ht=tanh(W_k[x_t,h_{t-1}]+b)</script><p>或</p><script type="math/tex; mode=display">ht=tanh(W_x+Uh_{t-1}+b)</script><p>但是我个人认为应该是:</p><script type="math/tex; mode=display">ht=tanh([x_t,h_{t-1}]*W+b) \tag{1}</script><p>我也不知道为啥会都写作上面那两种形式.<br>eg:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">batch_size = 3</span><br><span class="line">input_dim = 2</span><br><span class="line">output_dim = 4</span><br><span class="line"></span><br><span class="line">inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, input_dim))</span><br><span class="line">cell = tf.contrib.rnn.BasicRNNCell(num_units=output_dim)</span><br><span class="line">previous_state = cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">output, state = cell(inputs, previous_state)</span><br><span class="line">kernel = cell.variables</span><br><span class="line">X = np.ones(shape=(batch_size, input_dim))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    output, state, kernel = sess.run([output, state,kernel], feed_dict=&#123;inputs: X&#125;)</span><br><span class="line">    print(X.shape)</span><br><span class="line">    print(kernel[0].shape)</span><br><span class="line">    print(kernel[1].shape)</span><br><span class="line">    print(previous_state.shape)</span><br><span class="line">    print(state.shape)</span><br><span class="line">    print(output.shape)</span><br><span class="line">结果为:</span><br><span class="line">(3, 2)</span><br><span class="line">(6, 4)</span><br><span class="line">(4,)</span><br><span class="line">(3, 4)</span><br><span class="line">(3, 4)</span><br><span class="line">(3, 4)</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>分析:(kernel中是所有参数的list，此处是W和bias)根据公式(1),$output = ([X,previous_state] <em> W+bias)$,即$([(3, 2),(3, 4)]</em>(6, 4)+(4,)) = (3,4)$，代码中也较容易看出.</p></blockquote></blockquote><p>2.BasicLSTMCell<br>基本结构如图:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/basiclstm.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>源码如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def call(self, inputs, state):</span><br><span class="line">   &quot;&quot;&quot;Long short-term memory cell (LSTM).</span><br><span class="line">   Args:</span><br><span class="line">     inputs: `2-D` tensor with shape `[batch_size, input_size]`.</span><br><span class="line">     state: An `LSTMStateTuple` of state tensors, each shaped</span><br><span class="line">       `[batch_size, self.state_size]`, if `state_is_tuple` has been set to</span><br><span class="line">       `True`.  Otherwise, a `Tensor` shaped</span><br><span class="line">       `[batch_size, 2 * self.state_size]`.</span><br><span class="line">   Returns:</span><br><span class="line">     A pair containing the new hidden state, and the new state (either a</span><br><span class="line">       `LSTMStateTuple` or a concatenated state, depending on</span><br><span class="line">       `state_is_tuple`).</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">   sigmoid = math_ops.sigmoid</span><br><span class="line">   one = constant_op.constant(1, dtype=dtypes.int32)</span><br><span class="line">   # Parameters of gates are concatenated into one multiply for efficiency.</span><br><span class="line">   if self._state_is_tuple:</span><br><span class="line">     c, h = state</span><br><span class="line">   else:</span><br><span class="line">     c, h = array_ops.split(value=state, num_or_size_splits=2, axis=one)</span><br><span class="line"></span><br><span class="line">   gate_inputs = math_ops.matmul(</span><br><span class="line">       array_ops.concat([inputs, h], 1), self._kernel)</span><br><span class="line">   gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)</span><br><span class="line"></span><br><span class="line">   # i = input_gate, j = new_input, f = forget_gate, o = output_gate</span><br><span class="line">   i, j, f, o = array_ops.split(</span><br><span class="line">       value=gate_inputs, num_or_size_splits=4, axis=one)</span><br><span class="line"></span><br><span class="line">   forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)</span><br><span class="line">   # Note that using `add` and `multiply` instead of `+` and `*` gives a</span><br><span class="line">   # performance improvement. So using those at the cost of readability.</span><br><span class="line">   add = math_ops.add</span><br><span class="line">   multiply = math_ops.multiply</span><br><span class="line">   new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),</span><br><span class="line">               multiply(sigmoid(i), self._activation(j)))</span><br><span class="line">   new_h = multiply(self._activation(new_c), sigmoid(o))</span><br><span class="line"></span><br><span class="line">   if self._state_is_tuple:</span><br><span class="line">     new_state = LSTMStateTuple(new_c, new_h)</span><br><span class="line">   else:</span><br><span class="line">     new_state = array_ops.concat([new_c, new_h], 1)</span><br><span class="line">   return new_h, new_state</span><br></pre></td></tr></table></figure><p>公式如下:</p><script type="math/tex; mode=display">f_t = \sigma(W_f[h_{t-1},x_t]+b_f)\\i_t = \tanh(W_i[h_{t-1,x_t}]+b_i) \\\widetilde C_t = \tanh(W_C[h_{t-1,x_t}]+b_C) \\O_t = \sigma(W_o[h_{t-1},x_t]+b_o)\\C_t = f_t*C_{t-1}+i_t*\widetilde C_t \\h_t = O_t * \tanh(C_t)</script><p>同理，我感觉应该是:</p><script type="math/tex; mode=display">f_t = \sigma([h_{t-1},x_t]*W_f+b_f)\\i_t = \tanh([h_{t-1,x_t}]*W_i+b_i) \\\widetilde C_t = \tanh([h_{t-1,x_t}]*W_C+b_C) \\O_t = \sigma([h_{t-1},x_t]*W_o+b_o)\\C_t = f_t*C_{t-1}+i_t*\widetilde C_t \\h_t = O_t * \tanh(C_t)</script><p>eg:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">batch_size = 3</span><br><span class="line">input_dim = 2</span><br><span class="line">output_dim = 4</span><br><span class="line"></span><br><span class="line">inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, input_dim))</span><br><span class="line"></span><br><span class="line">cell = tf.contrib.rnn.BasicLSTMCell(num_units=output_dim)</span><br><span class="line">previous_state = cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">output, state = cell(inputs, previous_state)</span><br><span class="line">kernel = cell.variables</span><br><span class="line">X = np.ones(shape=(batch_size, input_dim))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    output, state,kernel = sess.run([output, state, kernel], feed_dict=&#123;inputs: X&#125;)</span><br><span class="line">    print(X.shape)</span><br><span class="line">    print(previous_state[0].shape)</span><br><span class="line">    print(previous_state[1].shape)</span><br><span class="line">    print(kernel[0].shape)</span><br><span class="line">    print(kernel[1].shape)</span><br><span class="line">    print(state[0].shape)</span><br><span class="line">    print(state[1].shape)</span><br><span class="line">    print(output.shape)</span><br><span class="line">结果:</span><br><span class="line">(3, 2)</span><br><span class="line">(3, 4)</span><br><span class="line">(3, 4)</span><br><span class="line">(6, 16)</span><br><span class="line">(16,)</span><br><span class="line">(3, 4)</span><br><span class="line">(3, 4)</span><br><span class="line">(3, 4)</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><p>分析: (kernel是所有参数，即W和bias)根据上诉公式，在源码中求遗忘门:$f_t$,输入门$i_t$和$\widetilde C_t$s输出门$O_t$的代码为:</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">i, j, f, o = array_ops.split(value=gate_inputs, num_or_size_splits=4, axis=one)</span><br></pre></td></tr></table></figure><p>这里也解释了为什么eg中kernel[0]是(6,16),因为代码中是将4个W同时初始化在一起，即（6,16）中其实是有4个W，并在上诉代码中分别计算，kernal[1]同理。这样得到的i,j,f,o应该都是(3,4),在源码中可以看出计算i,j,f,o是矩阵相乘，但是计算$C_t$和$h_t$是各个元素相乘,因此得到的$C_t$和$h_t$都是(3,4).</p><p>3.GRU<br>![]/img/GRU.png)<br>源代码为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def call(self, inputs, state):</span><br><span class="line">  &quot;&quot;&quot;Gated recurrent unit (GRU) with nunits cells.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  gate_inputs = math_ops.matmul(</span><br><span class="line">      array_ops.concat([inputs, state], 1), self._gate_kernel)</span><br><span class="line">  gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias)</span><br><span class="line"></span><br><span class="line">  value = math_ops.sigmoid(gate_inputs)</span><br><span class="line">  r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)</span><br><span class="line"></span><br><span class="line">  r_state = r * state</span><br><span class="line"></span><br><span class="line">  candidate = math_ops.matmul(</span><br><span class="line">      array_ops.concat([inputs, r_state], 1), self._candidate_kernel)</span><br><span class="line">  candidate = nn_ops.bias_add(candidate, self._candidate_bias)</span><br><span class="line"></span><br><span class="line">  c = self._activation(candidate)</span><br><span class="line">  new_h = u * state + (1 - u) * c</span><br><span class="line">  return new_h, new_h</span><br></pre></td></tr></table></figure><p>eg:  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">batch_size = 3</span><br><span class="line">input_dim = 2</span><br><span class="line">output_dim = 4</span><br><span class="line"></span><br><span class="line">inputs = tf.placeholder(dtype=tf.float32, shape=(batch_size, input_dim))</span><br><span class="line"></span><br><span class="line">cell = tf.contrib.rnn.GRUCell(num_units=output_dim)</span><br><span class="line">previous_state = cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">output, state = cell(inputs, previous_state)</span><br><span class="line">kernel = cell.variables</span><br><span class="line">X = np.ones(shape=(batch_size, input_dim))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    output, state, kernel = sess.run([output, state, kernel], feed_dict=&#123;inputs: X&#125;)</span><br><span class="line">    print(X.shape)</span><br><span class="line">    print(previous_state.shape)</span><br><span class="line">    print(kernel[0].shape)</span><br><span class="line">    print(kernel[1].shape)</span><br><span class="line">    print(kernel[2].shape)</span><br><span class="line">    print(kernel[3].shape)</span><br><span class="line">    print(state.shape)</span><br><span class="line">    print(output.shape)</span><br><span class="line">结果:</span><br><span class="line">(3, 2)</span><br><span class="line">(3, 4)</span><br><span class="line">(6, 8)</span><br><span class="line">(8,)</span><br><span class="line">(6, 4)</span><br><span class="line">(4,)</span><br><span class="line">(3, 4)</span><br><span class="line">(3, 4)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow中各种rnn的实现
    
    </summary>
    
      <category term="深度学习" scheme="https://lingyixia.github.io/about/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="tensorflow" scheme="https://lingyixia.github.io/about/tags/tensorflow/"/>
    
      <category term="rnn" scheme="https://lingyixia.github.io/about/tags/rnn/"/>
    
  </entry>
  
  <entry>
    <title>C++优先队列</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/priority_queue/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/priority_queue/</id>
    <published>2018-12-20T08:05:09.664Z</published>
    <updated>2018-12-20T12:58:43.604Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>优先队列是队列和栈结合形成的一种数据结构，c++中使用堆来构建。</p></blockquote><p>首先函数在头文件<queue>中，归属于命名空间std，使用的时候需要注意。<br>有两种声明方式:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std::priority_queue&lt;T&gt; pq;</span><br><span class="line">std::priority_queue&lt;T, std::vector&lt;T&gt;, cmp&gt; pq;</span><br></pre></td></tr></table></figure></queue></p><p>第一种方式较为简单，当T是基本数据类型的时候使用<br>当T是自定义数据结构的时候(一般是struct)需要使用第二中声明方式.三个参数从左到右第一个指的是优先队列中的数据类型，第二个表示存储堆的数据结构，一般是vector即可，第三个是比较结构，是一个struct，默认是less，即小的在前，但是默认声明方式只针对基本数据结构，自定义的需要重写该结构体，less也可以改为greater。</p><blockquote><blockquote><p>greater和less是std实现的两个仿函数（就是使一个类的使用看上去像一个函数。其实现就是类中实现一个operator()，这个类就有了类似函数的行为，就是一个仿函数类了）</p></blockquote></blockquote><p>eg:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;queue&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">struct Node</span><br><span class="line">&#123;</span><br><span class="line">int val;</span><br><span class="line">Node(int val) :val(val) &#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">priority_queue&lt;Node&gt; A;                    //大根堆</span><br><span class="line">priority_queue&lt;Node, vector&lt;Node&gt;, greater&lt;Node&gt; &gt; B;    //小根堆 </span><br><span class="line">A.push(Node(1));</span><br><span class="line">A.push(Node(9));</span><br><span class="line">A.push(Node(4));</span><br><span class="line">cout &lt;&lt; A.top().val &lt;&lt; endl;</span><br><span class="line">A.pop();</span><br><span class="line">cout &lt;&lt; A.top().val &lt;&lt; endl;</span><br><span class="line">A.pop();</span><br><span class="line">cout &lt;&lt; A.top().val &lt;&lt; endl;</span><br><span class="line">A.pop();</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这样运行出错，因为priority_queue<node> A; 默认使用的是less，而less的源代码是:</node></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">struct less</span><br><span class="line">&#123;// functor for operator&lt;</span><br><span class="line">_CXX17_DEPRECATE_ADAPTOR_TYPEDEFS typedef _Ty first_argument_type;</span><br><span class="line">_CXX17_DEPRECATE_ADAPTOR_TYPEDEFS typedef _Ty second_argument_type;</span><br><span class="line">_CXX17_DEPRECATE_ADAPTOR_TYPEDEFS typedef bool result_type;</span><br><span class="line"></span><br><span class="line">constexpr bool operator()(const _Ty&amp;_Left, const _Ty&amp; _Right) const</span><br><span class="line">&#123;// apply operator&lt; to operands</span><br><span class="line">return (_Left &lt; _Right);</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>return (_Left &lt; _Right)对于一个自定义结构体而言程序不知道&lt;是什么操作，因此需要要么在结构体中重载&lt;符号，要么重写less函数，因此有两种写法。<br>第一种: </p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">struct Node</span><br><span class="line">&#123;</span><br><span class="line">int val;</span><br><span class="line">Node(int val) :val(val) &#123;&#125;</span><br><span class="line">bool operator &lt;(Node a) const &#123; return val &lt; a.val; &#125;</span><br><span class="line">bool operator &gt;(Node a) const &#123; return val &gt; a.val; &#125;</span><br><span class="line">&#125;;</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">第二种:</span><br></pre></td></tr></table></figure><h1 id="include"><a href="#include" class="headerlink" title="include"></a>include<iostream></iostream></h1><h1 id="include-1"><a href="#include-1" class="headerlink" title="include"></a>include<queue></queue></h1><p>using namespace std;<br>struct Node<br>{<br>    int val;<br>    Node(int val) :val(val) {}</p><p>};<br>struct cmp<br>{<br>    bool operator()(Node a, Node b)<br>    {<br>        return a.val &lt; b.val;<br>    }<br>};<br>int main()<br>{<br>    priority_queue<node,vector<node>,cmp&gt; A;                    //大根堆<br>    priority_queue<node, vector<node="">, less<node> &gt; B;    //小根堆<br>    A.push(Node(1));<br>    A.push(Node(9));<br>    A.push(Node(4));<br>    cout &lt;&lt; A.top().val &lt;&lt; endl;<br>    A.pop();<br>    cout &lt;&lt; A.top().val &lt;&lt; endl;<br>    A.pop();<br>    cout &lt;&lt; A.top().val &lt;&lt; endl;<br>    A.pop();<br>    return 0;<br>}<br>```</node></node,></node,vector<node></p>]]></content>
    
    <summary type="html">
    
      C++优先队列简单简单介绍
    
    </summary>
    
      <category term="基础" scheme="https://lingyixia.github.io/about/categories/%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="C++" scheme="https://lingyixia.github.io/about/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>C++参数传递方式</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/parameter_passing/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/parameter_passing/</id>
    <published>2018-12-20T08:05:09.648Z</published>
    <updated>2018-12-20T12:58:35.323Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>C++函数的三种传递方式为：值传递、指针传递和引用传递</p></blockquote><h3 id="值传递-略"><a href="#值传递-略" class="headerlink" title="值传递(略)"></a>值传递(略)</h3><h3 id="指针传递-实质也是值传递"><a href="#指针传递-实质也是值传递" class="headerlink" title="指针传递(实质也是值传递)"></a>指针传递(实质也是值传递)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">void fun(int *x)</span><br><span class="line">&#123;</span><br><span class="line">    *x += 5; //修改的是指针x指向的内存单元值</span><br><span class="line">&#125;</span><br><span class="line">void main(void)</span><br><span class="line">&#123;</span><br><span class="line">    int y = 0;</span><br><span class="line">    fun(&amp;y);</span><br><span class="line">cout&lt;&lt;&lt;&lt;\&quot;y = \&quot;&lt;&lt;y&lt;&lt;endl; //y = 5;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>实质是传递地址的值，即地址的‘值传递’</p><h3 id="引用传递"><a href="#引用传递" class="headerlink" title="引用传递"></a>引用传递</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">void fun(int &amp;x)</span><br><span class="line">&#123;</span><br><span class="line">   x += 5; //修改的是x引用的对象值 &amp;x = y;</span><br><span class="line">&#125;</span><br><span class="line">void main(void)</span><br><span class="line">&#123;</span><br><span class="line">int y = 0;</span><br><span class="line">fun(y);</span><br><span class="line">cout&lt;&lt;&lt;&lt;\&quot;y = \&quot;&lt;&lt;y&lt;&lt;endl; //y = 5;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实质是取别名，&amp;在C++中有两种作用，取别名和取地址（C中只有后者）,=号左边是引用，=号右边是取址。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int a=3;  </span><br><span class="line">int &amp;b=a;//引用              </span><br><span class="line">int *p=&amp;a; //取地址</span><br></pre></td></tr></table></figure></p></blockquote></blockquote><p>引用传递是C++的特性，值传递和指针传递是C语言中本来就有的方式。</p>]]></content>
    
    <summary type="html">
    
      C++三种参数传递方式
    
    </summary>
    
      <category term="基础" scheme="https://lingyixia.github.io/about/categories/%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="C++" scheme="https://lingyixia.github.io/about/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>为什么使用交叉熵损失函数</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/LossFunction/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/LossFunction/</id>
    <published>2018-12-20T08:05:09.639Z</published>
    <updated>2018-12-20T12:58:11.864Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>为什么使用交叉熵损失函数而不是二次代价函数</p></blockquote><p>本文基本转自<a href="https://blog.csdn.net/yuanjunliang/article/details/79394805" target="_blank" rel="noopener">这篇文章</a>，感谢作者</p><h2 id="前奏"><a href="#前奏" class="headerlink" title="前奏"></a>前奏</h2><ol><li><p>作为神经网络，应当具有自学习的能力，为了更好的模拟人学习的过程，神经网络的学习能力应当能够自我调整，当发现自己犯的错误越大时，改正的力度就越大。比如投篮：当运动员发现自己的投篮方向离正确方向越远，那么他调整的投篮角度就应该越大，篮球就更容易投进篮筐。这所谓的学习能力便体现在<strong>损失函数</strong>中，常见的损失函数有两种：<strong>二次代价函数</strong>和<strong>交叉熵损失函数</strong>，前者主要用在线性回归中，而在神经网络中主要用后者，下面我们来说明为什么。</p></li><li><p>以一个神经元的二类分类训练为例，进行两次实验,激活函数采用$sigmoid$：输入一个相同的样本数据x=1.0（该样本对应的实际分类y=0）；两次实验各自随机初始化参数，从而在各自的第一次前向传播后得到不同的输出值，形成不同的$Loss$：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/firstloss.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>  <figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/secondloss.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>   <blockquote><blockquote><p>在实验1中，随机初始化参数，使得第一次输出值为0.82（该样本对应的实际值为0）;经过300次迭代训练后，输出值由0.82降到0.09，逼近实际值。而在实验2中，第一次输出值为0.98，同样经过300迭代训练，输出值只降到了0.20。<br>从两次实验的代价曲线中可以看出：实验1的代价随着训练次数增加而快速降低，但实验2的代价在一开始下降得非常缓慢；直观上看，初始的误差越大，收敛得越缓慢。  </p></blockquote></blockquote></li></ol><p>下面计算两种损失函数,eg:$y$是真实值,$\hat y$是计算值,$z=wx+b$,$\hat y = \sigma (z)$  </p><h2 id="二次代价损失函数"><a href="#二次代价损失函数" class="headerlink" title="二次代价损失函数"></a>二次代价损失函数</h2><script type="math/tex; mode=display">C=\frac{1}{2n}\sum_x|y-\hat y |^2</script><p>以一个样本为例:  </p><script type="math/tex; mode=display">L=\frac{(y-\hat y)^2}{2}</script><p>则有</p><script type="math/tex; mode=display">\begin{align}\frac{\partial L}{\partial w} &=\frac{\partial L}{\partial \hat y} \frac{\partial \hat y}{\partial z} \frac{\partial z}{\partial w} \\&=(\hat y-y) \sigma\prime(x)x\end{align}</script><script type="math/tex; mode=display">\begin{align}\frac{\partial L}{\partial w} &=\frac{\partial L}{\partial \hat y} \frac{\partial \hat y}{\partial z} \\&=(\hat y-y) \sigma\prime(x)\end{align}</script><blockquote><blockquote><p>其中，z表示神经元的输入，表示激活函数。从以上公式可以看出，w和b的梯度跟激活函数的梯度成正比，激活函数的梯度越大，w和b的大小调整得越快，训练收敛得就越快。而神经网络常用的激活函数为sigmoid函数，该函数的曲线如下所示:  </p></blockquote></blockquote><p><img src="/img/sigmoid.jpg" alt="">  </p><blockquote><blockquote><p>如图所示，实验2的初始输出值（0.98）对应的梯度明显小于实验1的输出值（0.82），因此实验2的参数梯度下降得比实验1慢。这就是初始的代价（误差）越大，导致训练越慢的原因。与我们的期望不符，即：不能像人一样，错误越大，改正的幅度越大，从而学习得越快。  </p></blockquote></blockquote><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><script type="math/tex; mode=display">C=\frac{1}{n}\sum_x[y\ln \hat y+(1-y)\ln (1-\hat y)]</script><p>以一个样本为例:  </p><script type="math/tex; mode=display">L=-\sum_x[y\ln \hat y+(1-y)\ln (1-\hat y)]</script><p>则有</p><script type="math/tex; mode=display">\begin{align}\frac{\partial L}{\partial w}&=-\sum_x[\frac{y}{\hat y}-\frac{(1-y)}{1-\hat y}] \frac{\partial \hat y}{\partial z}\frac{\partial z}{\partial w}\\&=-\sum_x[\frac{y}{\hat y}-\frac{(1-y)}{1-\hat y}]\sigma \prime(x)x\\&=-\sum_x[\frac{y- \sigma (x)}{\sigma(x)(1-\sigma(x))}] \sigma \prime(x) x \\&= -\sum_x[y-\sigma(x)]x\end{align}</script><script type="math/tex; mode=display">\begin{align}\frac{\partial L}{\partial b}&=-\sum_x[y-\sigma(x)]\end{align}</script><blockquote><blockquote><p>因此，$w$的梯度公式中$\sigma \prime (z)$原来的被消掉了;另外，该梯度公式中的表示输出值与实际值之间的误差。所以，当误差越大，梯度就越大，参数$w$调整得越快，训练速度也就越快,$b$的梯度同理.</p></blockquote></blockquote><h2 id="交叉熵函数来源"><a href="#交叉熵函数来源" class="headerlink" title="交叉熵函数来源"></a>交叉熵函数来源</h2><p>以$w$的偏导为例:在二次代价函数中:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w} = (\hat y - y) \sigma \prime(x) x</script><p>为了消除$\sigma \prime(x)$,我们令要计算的$w$偏导为:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w} = (\hat y - y) x</script><p>而$w$偏导实际计算为:</p><script type="math/tex; mode=display">\begin{align}\frac{\partial L}{\partial w} &=\frac{\partial L}{\partial \hat y}\sigma \prime(x)x \\&=\frac{\partial L}{\partial \hat y}\hat y(1-\hat y) x \\\end{align}</script><p>则:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \hat y}\hat y(1-\hat y) x= (\hat y - y) x</script><p>x被消掉得:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \hat y} = \frac{\hat y-y}{(1-\hat y)\hat y}</script><p>求积分得:</p><script type="math/tex; mode=display">L=-[y \ln \hat y +(1-y) \ln (1- \hat y)]</script><p>即交叉熵函数.</p>]]></content>
    
    <summary type="html">
    
      为什么使用交叉熵损失函数而不是二次代价函数
    
    </summary>
    
      <category term="深度学习" scheme="https://lingyixia.github.io/about/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="熵" scheme="https://lingyixia.github.io/about/tags/%E7%86%B5/"/>
    
      <category term="损失函数" scheme="https://lingyixia.github.io/about/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>CNN理解</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/CNN/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/CNN/</id>
    <published>2018-12-20T08:05:09.630Z</published>
    <updated>2018-12-20T12:56:39.455Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>CNN的卷积层和普通神经网络全连接层对比</p></blockquote><p>为了过渡，首先看一下<a href="https://app.yinxiang.com/Home.action?login=true#n=c3ac6a3d-1140-4dca-9149-95539535fb93&amp;s=s32&amp;b=35353f67-3554-4bbc-9e1f-cad110a0c1ef&amp;ses=4&amp;sh=1&amp;sds=5&amp;" target="_blank" rel="noopener">这篇笔记</a>   </p><p>举例: 只有一个数据:$6\times6$,$Filter:3\times3$,如图所示:    </p><p><img src="/img/66image.jpg" alt=""><br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/filter1.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h3 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/fullconnect.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>  <p>参数数量为 $(6\times 6+1)\times n$</p><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/cnn.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>  <p>参数数量为 $(3\times 3+1)\times n$</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>可以很明显的看出，其实两者相差并不很多，其实feather map数量就是全连接层中的神经元数量，由于每个神经元所含参数只有9个，不能像全连接(每个神经元36个参数)那样按照矩阵相乘想乘得到一个数字，因此是用卷积的方式得到一个feather map，卷积过程中36个数据<strong>共享</strong>9个参数。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>1.很明显，通过卷积+池化+全连接层也能反向传播  </p><ol><li>其实能学到东西，关键在于初始化$Weights$，在普通全连接神经网络中:  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def add_layer(inputs, in_size, out_size, activation_function=None):</span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line">        output = tf.nn.dropout(output, keep_prob)</span><br><span class="line">    if activation_function is None:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    else:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line">    return outputs</span><br></pre></td></tr></table></figure><p>在cnn的卷积层中:   </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def conv2d(input, shape, activation_function):</span><br><span class="line">    Weights = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1))</span><br><span class="line">    biases = tf.constant(0.1, shape=[1, shape[3]])</span><br><span class="line">    convOutput = tf.nn.conv2d(input, filter=Weights, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;) + biases</span><br><span class="line">    if activation_function:</span><br><span class="line">        convOutput = activation_function(convOutput)</span><br><span class="line">    return convOutput</span><br></pre></td></tr></table></figure><p>也就是说，同一层中多个神经元的参数初始化是随机的，千万不能初始化为同一个值，这样会导致每个神经元的输出永远相同，也就是每个神经元学到的东西是相同的！！！<strong>每个神经元初始化<em>Weights</em>的不同是每个神经元学到不同特征的前提</strong>.</p>]]></content>
    
    <summary type="html">
    
      CNN的卷积层和普通神经网络全连接层对比
    
    </summary>
    
      <category term="深度学习" scheme="https://lingyixia.github.io/about/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="CNN" scheme="https://lingyixia.github.io/about/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>各种熵</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/entropy/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/entropy/</id>
    <published>2018-12-20T08:05:09.623Z</published>
    <updated>2018-12-20T12:57:01.789Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>熵(信息熵)、交叉熵和相对熵(KL散度)、条件熵</p></blockquote><h3 id="熵-信息熵"><a href="#熵-信息熵" class="headerlink" title="熵(信息熵)"></a>熵(信息熵)</h3><blockquote><p>信息熵有两个含义:1.系统包含的信息量的期望 2.定量描述该系统所需的编码长度的期望</p></blockquote><h4 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h4><h5 id="定性推导"><a href="#定性推导" class="headerlink" title="定性推导:"></a>定性推导:</h5><p>设$h(x)$为$x$包含的<strong>信息量</strong>,如果我们有俩个<strong>不相关</strong>的事件$x$和$y$,那么我们观察到的俩个事件同时发生时获得的<strong>信息量</strong>应该等于观察到的事件各自发生时获得的<strong>信息量</strong>之和,即:$h(x,y) = h(x) + h(y)$,由于$x$,$y$是俩个不相关的事件，则满足$p(x,y) = p(x)*p(y)$.那么要想让$h(x,y) = h(x) + h(y)$,$h(x)$就只能是$\log p(x)$,为了让信息量为非负,我们在其前面加负号，得到信息量公式:</p><script type="math/tex; mode=display">h(x) = -\log p(x)</script><h5 id="定量推导"><a href="#定量推导" class="headerlink" title="定量推导:"></a>定量推导:</h5><p><a href="https://blog.csdn.net/stpeace/article/details/79052689" target="_blank" rel="noopener">参考该博客</a><br>这个很牛逼！！！！！开始推导：<br>首先说明信息量的定义，谨记x是个概率值，不是事件:</p><p>1.信息量是概率的递减函数，记为$f(x)$,$x\in[0,1]$<br>2.$f(1)=0,f(0)=+∞$<br>3.独立事件(概率之积)的信息量等于各自信息量之和:$f  (x_1*x_2)=f(x_1)+f(x_2),x_1,x_2\in[0,1]$  </p><script type="math/tex; mode=display">\begin{align}f(x)^\prime &= \lim_{\Delta x \to 0}\frac{f(x+\Delta x)-f(x)}{\Delta x} \\&= \lim_{\Delta x \to 0}\frac{f(\frac{x+\Delta x}{x}*x)-f(x)}{\Delta x} \\&= \lim_{\Delta x \to 0}\frac{f(x+\Delta x)+f(x)-f(x)}{\Delta x} \\&=\lim_{\Delta x \to 0}\frac{f(\frac{x+\Delta x}{x})}{\Delta x} \\&= \frac{1}{x}\lim_{\Delta x \to 0}\frac{f(1+\frac{\Delta x}{x})}{\frac{\Delta x}{x}} \\&=\frac{1}{x}f(1)^\prime\end{align}</script><p>积分得:</p><script type="math/tex; mode=display">f(x) = f(1)^\prime\ln|x|+C \qquad x\in[0,1]</script><p>令$x=1$,得$C=0$,故</p><script type="math/tex; mode=display">\begin{align}f(x) &= f(1)^\prime\ln x \\     &=f(1)^\prime\frac{\log_ax}{\log_ae} \\     &= \frac{f(1)^\prime}{\log_ae} *\log_ax\end{align}</script><p>而$\frac{f(1)^\prime}{\log_ae}$是个常数，令为1，则</p><script type="math/tex; mode=display">\begin{align}f(x) &= \log_ax \\     &= -\log_a\frac{1}{x}\end{align}</script><p>证毕!!!  </p><h5 id="编码推导"><a href="#编码推导" class="headerlink" title="编码推导:"></a>编码推导:</h5><p><a href="https://blog.csdn.net/AckClinkz/article/details/78740427" target="_blank" rel="noopener">参考此处</a><br>首先需要知道的是Karft不等式:$\sum r^{-l_i}\le1$,其中$r$是进制数，一般取二进制,$l_i$表示第$i$个信息的码长.问题可以转化为:  </p><script type="math/tex; mode=display">\min_{l_i}\sum p_il_i \\s.t.\quad \sum r^{-l_i}\le1</script><p>由拉格朗日乘数法:  </p><script type="math/tex; mode=display">L(l_i,\lambda)=\sum p_il_i+\lambda(\sum r^{-l_i}-1)</script><p>根据拉格朗日法则求极值得:   </p><script type="math/tex; mode=display">\begin{cases}p_i-\lambda*r^{-l_i}\ln r =0 \\\sum r^{-l_i}-1=0 \\\end{cases}</script><p>由上面公式得:  </p><script type="math/tex; mode=display">\begin{cases}r^{-l_i}= \frac{p_i}{\lambda * \ln r} \\\sum r^{-l_i}=1 \\\end{cases}</script><p>一式带入二式得:$\sum r^{-l_i} = \sum \frac{p_i}{\lambda <em> \ln r} = \frac{1}{\lambda </em> \ln r} = 1$  </p><p>因此:$\lambda = \frac{1}{\ln r}$</p><p>最终得:$l_i = \log r \frac{\lambda * \ln r}{p_i}=\log r\frac{1}{p_i}$，正好是熵!!!</p><p>则公式信息熵: </p><script type="math/tex; mode=display">H(x) = -p(x)\log p(x)</script><p>自然是系统信息量的期望，或称为编码长度的期望.</p><h3 id="交叉熵和相对熵-KL散度"><a href="#交叉熵和相对熵-KL散度" class="headerlink" title="交叉熵和相对熵(KL散度)"></a>交叉熵和相对熵(KL散度)</h3><p>现在有两个分布，真实分布p和非真实分布q，我们的样本来自真实分布p,按照真实分布p来编码样本所需的编码长度的期望为(<strong>信息熵</strong>):  </p><script type="math/tex; mode=display">H(p) = \sum -p\log p</script><p>按照不真实分布q来编码样本所需的编码长度的期望为(<strong>交叉熵</strong>):   </p><script type="math/tex; mode=display">H(p,q)= \sum -p\log q</script><p>注意:$H(p,q)≠H(q,p)!!!$而<strong>KL散度(相对熵)</strong>为:</p><script type="math/tex; mode=display">H(p||q)=H(p,q)-H(p)</script><p>它表示两个分布的差异，差异越大，相对熵越大。</p><h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>条件熵针对得是某个特征范围内来计算，而不是整个样本，即熵是整个样本的不确定性，而条件熵是特定特征条件下样本的不确定性。<a href="https://blog.csdn.net/xwd18280820053/article/details/70739368" target="_blank" rel="noopener">案例见此</a>,公式表达为:</p><script type="math/tex; mode=display">\begin{align}H(Y|X) &=\sum_{x\in X}p(X)H(Y|X=x) \\       &=\sum _{x,y}-p(x,y)\log p(y|x)\end{align}</script><p>X表示总体样本的某个特征，条件熵和其他熵最大的不同就是条件熵和特征有关，其他熵只和Label有关，和特征无关。</p><h3 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h3><p>表示X和Y是同一个分布中的两个特征，没有特征和Label之分.</p><script type="math/tex; mode=display">H(X,Y) = \sum_{x,y}-p(x,y)\log p(x,y)</script><p>联合熵和条件熵的关系:  </p><script type="math/tex; mode=display">\begin{align}H(X,Y) - H(Y|X) &= -\sum_{x,y}p(x,y)\log p(x,y) + \sum_{x,y}p(x,y)\log p(y|x) \\&= -\sum_{x,y}p(x,y)\log p(x) \\&=-\sum_xp(x)\log p(x) \\&=H(X)\end{align}</script><p>注意，要和条件熵的符号区分。</p><h3 id="互信息-信息增益"><a href="#互信息-信息增益" class="headerlink" title="互信息(信息增益)"></a>互信息(信息增益)</h3><script type="math/tex; mode=display">\begin{align}I(X,Y) &= H(X)-H(X|Y) \\       &= H(Y)-H(Y|X) \\       &= H(X)+H(Y) - H(X,Y) \\       &= H(X,Y) - H(X|Y) -H(Y|X)\end{align}</script>]]></content>
    
    <summary type="html">
    
      各种熵总结
    
    </summary>
    
      <category term="机器学习" scheme="https://lingyixia.github.io/about/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="熵" scheme="https://lingyixia.github.io/about/tags/%E7%86%B5/"/>
    
  </entry>
  
  <entry>
    <title>线性回归和逻辑回归对比</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/regression/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/regression/</id>
    <published>2018-12-20T08:05:09.597Z</published>
    <updated>2018-12-20T12:57:30.411Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>温故而知新啊，今天复习线性回归和逻辑回归，发现了以前没想过的东西,即为什么逻辑回归要用交叉熵函数。</p></blockquote><h3 id="简单对比"><a href="#简单对比" class="headerlink" title="简单对比"></a>简单对比</h3><p>1.最终函数</p><p>线性回归:</p><script type="math/tex; mode=display">f(x)_{w,b}=wx+b</script><p>逻辑回归:</p><script type="math/tex; mode=display">P(Y=1|X)=\frac{e^{wx+b}}{1+e^{wx+b}}\\P(Y=0|X)=\frac{1}{1+e^{wx+b}}</script><p>令:</p><script type="math/tex; mode=display">f(x)_{w,b}=P(1|X)=\frac{e^{wx+b}}{1+e^{wx+b}}</script><p>2.损失函数</p><p>线性回归(注意谁减谁):</p><script type="math/tex; mode=display">L(w,b)=\frac{1}{2}\sum_{i=1}^N(y_i-f(x_i))^2</script><p>逻辑回归(注意谁前谁后):</p><script type="math/tex; mode=display">L(w,b)=\sum_{i=1}^NC(f(x_i),y_i)</script><p>3.梯度计算:</p><p>线性回归:</p><script type="math/tex; mode=display">\nabla_wL(w,b)=\sum_{i=1}^N(y_i-f(x_i))(-x_i)\\\nabla_bL(w,b)=\sum_{i=1}^N(y_i-f(x_i))</script><p>逻辑回归:</p><script type="math/tex; mode=display">\begin{align}\sum_{i=1}^NC(f(x_i),y_i) &= \sum_{i=1}^N-[y_i\ln(f(x_i))+(1-y_i)\ln(1-f(x_i))]\\                          &= -\sum_{i=1}^N[y_i\ln(\frac{f(x_i)}{1-f(x_i)})-\ln(1-f(x_i))]\\                          &= -\sum_{i=1}^N[y_i(wx_i+b)-\ln(1-f(x_i))]\\                          &= -\sum_{i=1}^N[y_i(wx_i+b)-\ln(1+e^{wx+b})]\end{align}</script><script type="math/tex; mode=display">\nabla_wL(w,b)=-\sum_{i=1}^N(y_i-f(x_i))x_i\\\nabla_bL(w,b)=-\sum_{i=1}^N(y_i-f(x_i))</script><p>至此发现线性回归和逻辑回归的参数偏导公式完全相同，然后梯度上升或下降即可(上升还是下降取决于线性回归谁减谁，逻辑回归交叉熵谁先谁后)。</p><h3 id="交叉熵含义"><a href="#交叉熵含义" class="headerlink" title="交叉熵含义"></a>交叉熵含义</h3><p>对于</p><script type="math/tex; mode=display">T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}</script><p>要想让得到回归函数$f(x)$最符合要求,只需使<strong>后验概率概率</strong>最大即可:</p><script type="math/tex; mode=display">\prod_{i=1}^N[f(x_i)]^{y_i}[1-f(x_i)]^{1-y_i}</script><p>其中,$y_i$是标签为1的数据，这其实是个似然函数,然后取$\log$:</p><script type="math/tex; mode=display">\begin{align}L(w,b) &= \sum_{i=i}^N[y_i\log(f(x_i))+(1-y_i)\log(1-f(x_i))]\\       &= \sum_{i=i}^N[y_i(wx_i+b)-\ln(1+e^{wx+b})]\end{align}</script><p>发现$L(w,b)=\sum_{i=1}^NC(f(x_i),y_i)$,因此，<strong>交叉熵的含义其实就是后验概率最大化</strong>。</p>]]></content>
    
    <summary type="html">
    
      线性回归和逻辑回归对比
    
    </summary>
    
      <category term="机器学习" scheme="https://lingyixia.github.io/about/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="分类" scheme="https://lingyixia.github.io/about/tags/%E5%88%86%E7%B1%BB/"/>
    
      <category term="回归" scheme="https://lingyixia.github.io/about/tags/%E5%9B%9E%E5%BD%92/"/>
    
      <category term="有监督" scheme="https://lingyixia.github.io/about/tags/%E6%9C%89%E7%9B%91%E7%9D%A3/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯</title>
    <link href="https://lingyixia.github.io/about/2018/12/20/NaiveBayes/"/>
    <id>https://lingyixia.github.io/about/2018/12/20/NaiveBayes/</id>
    <published>2018-12-20T07:36:51.000Z</published>
    <updated>2018-12-21T05:52:31.338Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>朴素贝叶斯简单推导</p></blockquote><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>设输入空间$\chi\subseteq R^n$，为n维空间的集合，输出空间为类标记集合$\gamma={c_1,c_2,…,c_K}$，输入为特征向量$x\subset\chi$，输出为类标记$y\subset\gamma$，$X$是输入控件$\chi$上的随机向量,$Y$是定义在输出空间$\gamma$,$P(X,Y)$是$X$和$Y$的联合分布，训练数据集:</p><script type="math/tex; mode=display">T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}</script><p>由$P(X,Y)$独立同分布产生(谨记：$(x_1,y_1)$中的$x_1\subset\chi$，即$x_1$是n维)。<br>现在需要用T训练贝叶斯模型，并判断$x’$的标签。</p><h3 id="问题探究"><a href="#问题探究" class="headerlink" title="问题探究"></a>问题探究</h3><p>显然，$x’$的标签可以$y_1$~$y_N$中任何一个，若一定要判断标签也等同于哪个标签的概率最高，即计算:</p><script type="math/tex; mode=display">y=arg\max_{c_k}P(Y=c_k|X=x) \qquad k=1,2,3,...,K \tag{1}</script><p>已知:</p><script type="math/tex; mode=display">P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\Sigma_kP(X=x|Y=c_k)P(Y=c_k)} \quad k=1,2,3,...K \tag{2}</script><p>其中，分母对于每个$c_k$都是相同的，略去不计算，只需计算:<br>$P(X=x|Y=c_k)P(Y=c_k)$<br>，即:</p><script type="math/tex; mode=display">y=arg\max_{c_k}P(X=x|Y=c_k)P(Y=c_k) \qquad  k=1,2,3,...K \tag{3}</script><p>对于$P(Y=c_k)$只需要在给出的$T$中数出来即可，因此现在只需要计算$P(X=x|Y=c_k)$，这也是整个朴素贝叶斯过程最困难的一步，为什么说困难呢？<br>$P(X=x|y=c_k)$完整表示为$P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},X^{(3)}=x^{(3)},….,X^{(N)}=x^{(N)}|Y=c_k)$<br>若和$P(Y=c_k)$一样只有一个维度，很容易数出来，但这里需要同时考虑N个维度，数的过程是相当复杂的，因此，朴素贝叶斯在此处做了条件<strong>独立性假设</strong>：</p><script type="math/tex; mode=display">P(X=x|y=c_k)=\prod_{i=1}^n(X^{(i)}=x^{(i)}|Y=c_k) \qquad k=1,2,3,...K \tag{4}</script><p>这样就只需要同时考虑一个维度，简单了许多，最后将$(4)$带入$(3)$中得:</p><script type="math/tex; mode=display">y=arg\max_{c_k}\prod_{i=1}^n(X^{(i)}=x^{(i)}|Y=c_k)P(Y=c_k) \qquad k=1,2,3,...K \tag{5}</script><p>至此朴素贝叶斯过程结束。</p><h3 id="名词谨记"><a href="#名词谨记" class="headerlink" title="名词谨记"></a>名词谨记</h3><ol><li><p>先验概率:<br>$P(Y=c_k)$，即不管$X$，直接在$T$中数出来的概率</p></li><li><p>后验概率:<br>$arg\max_{c_k}P(Y=c_k|X=x)$，即考虑$X$的约束下，数出来的概率（注意：两个概率都是针对$Y$的概率）</p></li></ol><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>其实前面已经可以说是整个朴素贝叶斯的过程，但是还要继续说明一个问题：<br>$(1)$式是整个朴素贝叶斯要计算的基本公式，我们可以看出，其实该公式就是后验概率，也就是说，朴素贝叶斯是一个<strong>后验概率最大化</strong>的过程。</p><h4 id="后验概率最大化的含义"><a href="#后验概率最大化的含义" class="headerlink" title="后验概率最大化的含义"></a>后验概率最大化的含义</h4><p>先说结论：<strong>后验概率最大化</strong>的含义就是选择$0-1$损失函数时的<strong>期望风险最小化</strong>，下面证明这个结论：<br>首先$0-1$损失函数：</p><script type="math/tex; mode=display">L(Y,f(X))= \begin{cases}1 & Y ≠ f(X)\\0 & Y=f(X) \\\end{cases}</script><p>其中$Y$和$f(X)$分别是$X$的实际标签和计算所的标签，则期望风险函数为：</p><script type="math/tex; mode=display">\begin{align}R_{exp}(f(X)) &= E[L(Y,f(X))] \\              &=\sum_{x=1}^N\sum_{k=1}^K[L(Y=c_k,f(X)]P(Y=c_k,X=x_i)\\              &=E_X(\sum_{k=1}^K[L(Y=c_k,f(X)]P(Y=c_k|X))\end{align}</script><p>意思就是先对确定$X=x$求<strong>条件期望</strong>,其实就是两层而来来算。<br>这样，针对确定的$X=x$而言：</p><script type="math/tex; mode=display">\begin{align}f(x) &= arg\min_{y\subset\chi}\sum_{k=1}^KL(Y=c_k,f(X)=y)P(Y=c_k,|X=x) \\     &= arg\min_{y\subset\chi}\sum_{k=1}^KP(Y≠c_k,|X=x) \\\\     &= arg\min_{y\subset\chi}\sum_{k=1}^K(1-P(Y=c_k,|X=x)) \\     &= arg\max_{y\subset\chi}\sum_{k=1}^KP(Y=c_k,|X=x)\end{align}</script><p>由此，我们从<strong>期望风险最小化</strong>入手，最终得到了<strong>后验概率最小化</strong>，即朴素贝叶斯的原理。</p>]]></content>
    
    <summary type="html">
    
      朴素贝叶斯简介
    
    </summary>
    
      <category term="机器学习" scheme="https://lingyixia.github.io/about/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="分类" scheme="https://lingyixia.github.io/about/tags/%E5%88%86%E7%B1%BB/"/>
    
      <category term="无监督" scheme="https://lingyixia.github.io/about/tags/%E6%97%A0%E7%9B%91%E7%9D%A3/"/>
    
  </entry>
  
</feed>
