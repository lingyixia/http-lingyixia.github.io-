<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>灵翼俠的个人博客</title>
  
  <subtitle>不做搬运工</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://lingyixia.github.io/"/>
  <updated>2021-11-28T09:45:20.537Z</updated>
  <id>https://lingyixia.github.io/</id>
  
  <author>
    <name>陈飞宇</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于知识的多模态对话生成</title>
    <link href="https://lingyixia.github.io/2021/11/20/dbd/"/>
    <id>https://lingyixia.github.io/2021/11/20/dbd/</id>
    <published>2021-11-20T10:50:23.000Z</published>
    <updated>2021-11-28T09:45:20.537Z</updated>
    
    <content type="html"><![CDATA[<h1 id="研究意义"><a href="#研究意义" class="headerlink" title="研究意义"></a>研究意义</h1><p>对话问题是NLP领域一个常见问题，重点研究如何让用户和机器进行顺畅的交流，从功能上可分为任务式对话、聊天式对话对话。目前研究任务型对话相对较为成熟，当前市面产品例如苹果Siri、微软Cortana、百度小度等等在完成用户指令方面都比较出色，显现了对话系统极大的潜力，但闲聊问题比较复杂难度较大，但事实上大多数人类的对话都会集中在闲聊、社交、个人情感上，比如在推特上80%都是关于个人情感、想法。因此针对该问题的研究很有价值。<br>对话问题从领域上可分为开放领域对话和基于知识对话，当前开放领域研究较多，常见手机聊天软件也都是开放领域，虽然此类对话系统对也可以很好的满足人们的一些常见情感需求，但是却有很大的限制，例如：Q：儿子，你不要娶了媳妇忘了娘呀！A：放心，我会一直娘下去的。之所以Agent会给出这样的回答是因为没有”娶了媳妇忘了娘”这句俗语的背景知识。如果对话系统有这种学习能力，其作用会更加广泛，例如让掌握商品情况，然后User对其提问Agent自动回答实现自动化购买，甚至给出教材让Agent模拟教师备课，由Agent提出有意义的问题和学生对话，同时检测学生的知识掌握情况并提出指导性建议。<br>因此，基于知识的对话研究非常有必要，本课题拟所涉及到的的是多模态的知识，因为学习的内容一般不仅仅包含文字，同时也少不了图像、视频等，这些都对模型学习背景知识提供很大的帮助，该领域常被称为Knowledge Grounded Conversation (KGC),旨在研究对于一定的知识背景，User和Agent根据其内容进行多轮对话，同时Agent的每次回答不仅会考虑User的提问以及对话历史，还会考虑文档内容，但是对于开放领域的知识会较少考虑，可以看做增添额外的信息与约束的对话生成。</p><h1 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h1><p>对话系统从NLP起步阶段就一直是热点问题，从模型角度讲可以分为两大类别：检索式和生成式，检索式对话是一种比较经典的方式，将对话问题抽象为搜索问题，先经过query理解，然后进一步进行召回、排序，等人在Multi-view Response Selection中认为在对话模型中将单词作为单元很难捕捉到话语层面的信息和依赖关系，因此提出了一种多视图响应模型，将单词级别和话语级别进行联合建模。 等人在SMN中认为全部话语拼接是不利的，并且在抽象的高层语义中直接匹配是不可取的，因此提出当前回应和每个历史话语分别匹配，再过交替CNN和MaxPooling，最后RNN累积匹配信息的处理方法和流程。等人在DUA中对SMN进一步改进，提出用最后一个话语来区别前面历史话语中词重要程度的方式。针对生成式对话生成中，Iulian V. Serban等人建议训练时考虑让答复引入新信息（Deep Reinforcement Learning for Dialogue Generation），保证语义连贯性等因素， 等人提出一种多层生成式模型（Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models），利用层级RNN把之前多轮对话背景加以考虑。等人认为，对话里面的句子一个是token等局部特征组成了句子级别的特征，另外一个是存在于句子之间的隐藏特征，为了利用这些隐藏特征，作者提出了一种包含隐藏变量的神经网络生成模型，并利用了VAE的生成能力来编码隐藏变量。<br>除了开放领域的对话生成，基于知识的对话生成也是近些年研究热点，针对背景文档过长的问题，等人提出TMN模型，从文档中筛选关键句子然后以学习这些句子来建模，等人提出GLKS，模型在对输入信息（对话和文档）进行交互后采用一种全局指导局部的思路，使用对话历史从文档中筛选出重要的n-gram信息来指导后续的生成过程。由于背景知识大多不只有文字，还有图片等等，只用文字必然会损失信息，因此多模态对话也是当前一个研究热点，等人在OpenViDial中提出三种模型，分别通过纯文本、粗粒度视觉信息、细粒度视觉信息的对比，证明视觉对对话任务的增强作用。有等人提出Maria模型，该模型包含三个组件：图像检索模型、视觉概念检测模型和基于视觉知识的对话生成模型，利用三个组件检索出的视觉信息增强对话知识（Maria: A Visual Experience Powered Conversational Agent）。等人提出一种响应驱动的视觉状态估计器，通过响应驱动的注意力更新以及视觉信息的条件融合机制分别增强语言编码能力和视觉融合能力（Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue-commentCZ）。</p><h1 id="研究难点以及拟解决方案"><a href="#研究难点以及拟解决方案" class="headerlink" title="研究难点以及拟解决方案"></a>研究难点以及拟解决方案</h1><p>数据集严重匮乏<br>    本课题拟研究多模态背景知识下多模态的对话，当前相关数据集大多是广泛领域的对话，基于背景知识的对话数据集并不多，仅存在的几个数据集例如CMUDoG、Movie-chats、Persona都是完全基于语言的数据集，最近的OpenViDial虽然背景知识是多模态，但对话内容完全基于语言，因此首要问题是数据集的创建。</p><ul><li><p>多模态知识的有效利用</p><p>  和开放领域的对话不同，本课题的对话需要基于背景知识，因此如何让生成的对话仅仅围绕所给背景知识至关重要，例如背景知识是电影简介，对话内容应当围绕电影的情节、角色进行。同时，作为多轮对话模型，不仅仅要考虑到背景知识、当前语句，还需要考虑到上一轮次，而且不同于常见的对话模型，本课题还需要考虑到多模态融合。<br>  本课题拟使用基于pretrain-finetune的方式，其中pretrain阶段可以看作一个多任务的自监督多模态模型，用于学习背景知识，finetune阶段则可以看作对话生成阶段。<br>  pretrain模型类似图示：</p><p>  <div align="center">  <img src="/img/dialogue_pretrain.png" alt width="650">  <center>pretrain</center>  </div><br>  该预训练模型包含三种损失函数：</p><ul><li>Text-Image Contrasive Loss: Image Transformer的输入是文档中的图片，并将图片分为多个Patch，Text Transformer 的输入是文档中的文字，该Loss相当于有监督对比学习，在同一个训练Batch，只有一组Image和Text来自同一个文档，Batch内其他Image和Text的组合分别来自不同文档。</li><li>Text-Image Align Loss: Fusion Transformer输入序列是整个文档，对于输入数据，随机替换若干Image或Text为其他文档的数据，</li><li><p>MLM Loss: Fusion Transformer的输入包含两种Mask数据</p><ul><li>某个文档的图片和文字，并采取和Bert一样的方式进行Mask。</li><li>某个文档的图片和文字，并对某段文字或某个图片整个Mask掉，旨在强制模型学习“背诵课文“。</li></ul><p>finetune模型类似图示<br><div align="center"><img src="/img/finetune.png"><center>finetune</center></div><br>整体相当于Encoder-Decoder结构，Encoder阶输入背景知识[CLS]和某一条对话前所有的对话(包含User和Agent)，并对User和Agent前面各自加上特殊标记[U]和[A],并用[Seq]标记隔开，同时，考虑Transformer复杂度为O(n^2)，随着到对话的加长有可能Encoder阶段速度会越来越慢，因此本文引入双层Self-Attention机制，对每个句子单独做Inner-Self-Attention，然后对整个序列做Global-Self—Attention,从而大幅度降低序列长度。</p></li></ul></li></ul><p>对话质量的自动评价<br>    当前业内针对对话系统的评价指标往往是和其他生成类任务相同都采用类似BLEU词重叠的方法，但是对于对话系统来说这些评判标准往往比较死板，甚至与人类评判相关性很差。基于知识的对话系统基本评价指标包含两个：其一是正确性，其二是和人类语言的相似程度。针对前者可以采取关键词匹配的方式，后者比较难解决，本课题拟尝试基于Gan的评价指标，在Gan模型中，Generator用于将隐变量编码为某一个概率分布，Discriminator用于判断该概率分布是否符合人类语言，因此理论上Discriminator可以用来当作指标，但该方案存在的问题是作为指标，本质上应当是固定的，而该方式不同研究者需要训练指标模型，因此不确定性较大。<br>MRC一般可以采用准确率、F1等，但文本生成的自动评价指标（如PPL）无法反映对话质量的好坏，而人工评价方法代价过高。我们需要一种代价低廉，自动评对话有趣程度、信息含量、一致性等问题的评价方式；</p><ol><li>将文字、图片、视频等多模态资源作为外部知识加入到对话中，同时，各模态的存在形态也不尽相同，例如文字不仅可以表现为句子、段落，同时还可以表现为表格，相关数据集的数量和规模还有待提高；</li><li>终身学习问题。<br>对话系统还有一个和其他生成任务最大的区别，当User和Agent在对话的时候，User很有可能会在对话中添加背景以外的知识，我们希望在对话的进行中，模型能够持续地利用两者的交互进行自身的更新和优化，能利用已得到的“技能”融合不同结构的资源，这是本课题的另一个研究重点，即终身学习(life-long问题)。<br>该问题最大的难点是Catastropic forget，由于用交流的过程有时序，模型训练的数据顺序需要严格而不能打散， 判断神经元输出归一化后绝对值越接近0越不重要<h1 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h1>本课题针对基于知识的多模态对话生成拟提出一种finetune方式，对多模态知识进行统一建模，同时设计了多种损失函数，针对对话生成阶段可能序列过长的问题，提出一种双层Self-Attention结构，</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;研究意义&quot;&gt;&lt;a href=&quot;#研究意义&quot; class=&quot;headerlink&quot; title=&quot;研究意义&quot;&gt;&lt;/a&gt;研究意义&lt;/h1&gt;&lt;p&gt;对话问题是NLP领域一个常见问题，重点研究如何让用户和机器进行顺畅的交流，从功能上可分为任务式对话、聊天式对话对话。目前研究
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>双层自注意力Transformer</title>
    <link href="https://lingyixia.github.io/2021/11/13/shuffleLM/"/>
    <id>https://lingyixia.github.io/2021/11/13/shuffleLM/</id>
    <published>2021-11-13T15:44:44.000Z</published>
    <updated>2021-11-28T09:45:17.554Z</updated>
    
    <content type="html"><![CDATA[<h1 id="立题依据"><a href="#立题依据" class="headerlink" title="立题依据"></a>立题依据</h1><h2 id="研究意义"><a href="#研究意义" class="headerlink" title="研究意义"></a>研究意义</h2><p>Transformer<a href="#refer-anchor"><sup>[1]</sup></a>从2017年被提出至今，一直是NLP领域重点研究对象，各种SOTA模型的诞生几乎都离不开它的加持，如今甚至在NLP领域已经成为标配，有完全取代CNN和RNN的趋势，在CV领域也有席卷的迹象。尽管Transformer<a href="#refer-anchor"><sup>[1]</sup></a>取得这么大成功，但是它有一个致命缺陷，其时空复杂度均为$O(n^2)$,其中$n$代表序列长度,这也是限制其在CV领域发展的一个重要原因，正因如此，不少工作被迫妥协，经典模型Bert<a href="#refer-anchor"><sup>[2]</sup></a>最长长度只有512，对于短文本而言还可以接受，但对长文本例如一篇文章一般几千个token，这种长度的序列直接训练会导致显存不足因此只能被迫截断，这样必然损失精度，因此研究如何降低复杂度至关重要。</p><h2 id="国内外研究现状"><a href="#国内外研究现状" class="headerlink" title="国内外研究现状"></a>国内外研究现状</h2><p>针对Transformer<a href="#refer-anchor"><sup>[1]</sup></a>时空复杂度过大的问题，业内已经有研究人员提出提出一些解决方案，Qipeng Guo等人提出Star-Transformer<a href="#refer-anchor"><sup>[3]</sup></a>，将密集Attention结构改为星形结构，基本思想是将计算两两token之间注意力分数改为计算各个token和中心token的注意力分数，从而降低复杂度。Rewon Child等人提出Sparse Transformer<a href="#refer-anchor"><sup>[4]</sup></a>，基本思路是对于某个token而言，只需要计算对其贡献最大的若干个注意力分数即可，通过将Transformer模型中的Attention矩阵稀疏化来达到减少内存消耗、降低计算力的方法，基本做法是在Softmax阶段只保留topK个结果，将计算复杂度从$O(n^2)$降低到$O(n^\frac{2}{3})$.Zihao Ye等人提出BP-Transformer<a href="#refer-anchor"><sup>[5]</sup></a>，采用分治法利用多层二叉树，逐步计算Attention最终归并，将计算复杂度降低到$O(n\log n)$。 Iz Beltagy等人提出Longformer<a href="#refer-anchor"><sup>[6]</sup></a>，对于每一个token只对固定窗口大小附近的token进行Local Attention，同时附近token采用了类似空洞卷积的方式增强用来捕获更长的序列长度的信息，针对特定任务的特定token做Global Attention，但值得注意的是，在原始代码中所谓的Local Attention窗口大小也达到了512,即内存量和Bert<a href="#refer-anchor"><sup>[2]</sup></a>相比并没有减少。Zihang Dai等人提出Transformer-XL<a href="#refer-anchor"><sup>[7]</sup></a>,将长文档切块，从第一个块开始做Transformer，然后以模仿RNN的方式传递到下一个块递归的训练，该方案被用在很多其他工作中，例如Xlnet<a href="#refer-anchor"><sup>[8]</sup></a>，同时XlNet<a href="#refer-anchor"><sup>[8]</sup></a>也使用了一种全排列的乱序方式，但是该乱序中加入了位置信息，不能算真正乱序。Sainbayar Sukhbaatar等人提出自适应跨度 Transformer<a href="#refer-anchor"><sup>[9]</sup></a>，主要思想是抛弃了区域Attention固定窗口的做法，使用一种能自适应选择长下文窗口大小的方案。Nikita Kitaev等人提出Reformer<a href="#refer-anchor"><sup>[10]</sup></a>，选择通过局部敏感哈希技术将每个词例的注意力范围变窄，Aurko Roy 等人提出Routing Transformer<a href="#refer-anchor"><sup>[11]</sup></a>尝试将问题建模为了一个路由问题，从而让模型学会选择词例的稀疏聚类。Siyu Ding等人在ERNIE-DOC<a href="#refer-anchor"><sup>[12]</sup></a>中提出回溯式feed机制和增强的循环机制，其基本原理是对于一篇长文，采用先略读后精读的方式，其中回溯式feed机制参考了xlnet<a href="#refer-anchor"><sup>[8]</sup></a>通过粗细两种粒度从乱序恢复正常语序，增强的循环机制借鉴了Transformer-xl<a href="#refer-anchor"><sup>[7]</sup></a>，但是将上一时刻上一层的输出改为上一时刻下一层的输出，相当于扩大感受野。</p><h1 id="研究方案"><a href="#研究方案" class="headerlink" title="研究方案"></a>研究方案</h1><h2 id="研究目标、研究内容和关键问题"><a href="#研究目标、研究内容和关键问题" class="headerlink" title="研究目标、研究内容和关键问题"></a>研究目标、研究内容和关键问题</h2><h3 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h3><p>本论文的研究目标是提出并实现一种双层Self-Attention结构的Transformer，用于降低普通Transformer时空复杂度。</p><h3 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h3><p>普通的Transformer模型时间和空间复杂度都是$O(n^2)$,需要注意的是这里的复杂度指的是乘/除法运算的复杂度,证明如下：</p><script type="math/tex; mode=display">Attention(Q_{n\times d_k},K_{n\times d_k},V_{n\times d_k})=\frac{softmax(Q_{n\times d_k}\times K_{n\times d_k}^T)}{\sqrt{d_k}} \times V_{n\times d_k}</script><p>第一步: $Q_{n\times d_k}\times K_{n\times d_k}^T$乘法计算量为$d_k\times n^2$.</p><p>第二步:$softmax(Q_{n\times d_k}\times K_{n\times d_k}^T)$乘法计算量为$n^2$.</p><p>第三步:$\frac{softmax(Q_{n\times d_k}\times K_{n\times d_k}^T)}{\sqrt{d_k}}$除法计算量为$d_k\times n^2$.</p><p>第四步:$\frac{softmax(Q_{n\times d_k}\times K_{n\times d_k}^T)}{\sqrt{d_k}} \times V_{n\times d_k}$乘法计算量为$d_k\times n^2$.</p><p>综上所述，去除常数$d_k$，可得复杂度为$O(n^2)$,如此大的复杂度在面对长文本时很难处理，时间上勉强可以接受，但是空间上很难解决，通常只能通过限制文本长度或减小batchsize来解决，此方式过于暴力，无疑会增加降低模型效果，本论文拟尝试一种双层self-Attention的Transformer模型，其基本结构如下：</p><div align="center"><img src="/img/double_layer_attention.png"><center>双层self-Attention</center></div><p>基本思想是对长序列进行分块，先通过块内计算Attention Score，然后通过块间计算全局Attention Score。图中$token_{ij}$中$i$表示第$i$个块,$j$表示块中第$j$个token，$SA_{ij}$表示第$i$层第$j$个块的$SA$。</p><h3 id="关键问题"><a href="#关键问题" class="headerlink" title="关键问题"></a>关键问题</h3><ol><li>如何设置各个分块大小？<br>常见方式是固定窗口大小，本论文拟尝试的方式如下：<br>固定分区后分块数量$m$，则平均每个块长度为$\frac{n}{m}$,然后每个块实际大小设置为$r_i=random[1,\frac{2n}{m}]$，且满足$\sum_ir_i=n$。之所以希望窗口大小随机而不是固定主要是考虑希望窗口在序列的位置信息不是固定位置而是一定范围的位置。</li><li>按照标准Transformer，backbone需要串联起来，但是上诉方式，第一层backbone之后序列长度就和之前不一样了，无法直接衔接到相同形状的下一层，在BP-Transformer中解决该问题的方案是用根节点去做类似分类任务，用叶子结点去做生成任务，但是叶子是处于第一层，理论上越深层语义信息越丰富，经典论文Transformer中做翻译任务用的就是Encoder的最后一层。本论文中该问题拟尝试的解决方案有两个:</li></ol><ul><li><p>方案一：基本思路是为使每一个backbone输入输出相同，可尝试将两层Attention结合，即第一层SA用于捕获块内注意力，第二层SA用于捕获全局注意力，然后两者拼接即可恢复输入形状，进而可以随意串联，同时每一个backbone会重新分块。该方式可以用于任何任务。</p><div align="center"><img src="/img/stack_multi_attention.png"><center>堆叠self-Attention</center></div></li><li><p>方案二：基本思路是类似多层cnn堆叠的方式，呈现金字塔形状，同时并行训练多个不同分块方式的相同模型，最后一层concat在一起，该方式可用于多对一的任务中。</p><div align="center"><img src="/img/pyramid_attention.png"><center>金字塔self-Attention</center></div></li></ul><ol><li>SA数量的增多也意味着FNN数量的增多，可能会增加参数：<br>可以尝试同层同形SA之间参数共享。</li><li>块内位置信息是否必要？<br>在Nlp领域无论什么任务都比较强调语序，因此一般都会加上位置信息，看起来很正常，但一定要如此么？有个传播已久例子：“研表究明，汉字的序顺并不定一能影阅响读，比如当你看完这句话后，才发这现里的字全是乱的。”但是如果我们把这句话改为：”影研明字，的序究顺并表不当一汉阅响，比读现你看完发这定话后，才这里能的字全是如句乱的。”正常情况下很难一眼理解句意，看起来有趣的同时也蕴含着一定的道理，语序确实很重要，但或许不是那么重要？<br>出于这种思考，本论文拟尝试一种区域内乱序的方案，意思是分块后块之间严格语序，但是块内不严格语序。基本结构可以参考双层self-Attention图，并将第一层SA改为reduce_sum/reduce_mean即可实现块内乱序。</li></ol><h2 id="可行性分析"><a href="#可行性分析" class="headerlink" title="可行性分析"></a>可行性分析</h2><p>本小节旨在证明该方法是否能够降低复杂度，以及最优复杂度是多少。</p><h3 id="命题"><a href="#命题" class="headerlink" title="命题"></a>命题</h3><p>假设一段文字长度为$n$，则其在self-Attention部分计算量为$n^2$，我们把它切成$m$段，为方便计算假设每段长度相同,即$\frac{n}{m}$，求此时双层self-Attention的计算量最低是多少。</p><h3 id="解题"><a href="#解题" class="headerlink" title="解题"></a>解题</h3><p>根据上图可得则此时的计算量为:$\frac{n^2}{m^2} \times m + m^2=\frac{n^2}{m}+m^2$(块内Attention和块间Attention相加)，令：</p><script type="math/tex; mode=display">f(m)=\frac{n^2}{m}+m^2 \quad (1<m<n)</script><p>求导：</p><script type="math/tex; mode=display">f'(m)=\frac{2m^3-n^2}{m^2}\quad (1<m<n)</script><p>令$f’(m)=0$,求得$m=(\frac{n^2}{2})^\frac{1}{3}$,$f(m)$在$(1,(\frac{n^2}{2})^\frac{1}{3}]$为减函数，$[(\frac{n^2}{2})^\frac{1}{3},n)$为增函数，因此$m=(\frac{n^2}{2})^\frac{1}{3}$时可求得$f(m)$最小值$n^{\frac{4}{3}}\times(2^{\frac{1}{3}}+2^{-\frac{2}{3}})=\frac{3n^\frac{4}{3}}{2^\frac{2}{3}}$,复杂度为$O(n^{4/3})$。<br>和Bert对比，假设n=512,则原始计算量为262144，而双层self-Attention当m=50时，计算量最优值仅为7740,远远小于262144。理论上如果和普通Bert相同的计算量，双层self-Attention可处理序列长度为7500，远大于512。<br>虽然从复杂度上看，$O(n^{4/3})$和$O(n^2)$差不了很多，但是从计算量角度还是有一定的效果，当然理论上self-Attention层越多越能减轻计算负担。</p><h1 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h1><p>本文尝试提出一种利用双层self-Attention降低Transformer复杂度的方案，利用块内自注意力和块间自注意力的结合，将普通Transformer自注意力机制的复杂度从$O(n^2)$降低到$O(n^\frac{4}{3})$。</p><h1 id="研究计划"><a href="#研究计划" class="headerlink" title="研究计划"></a>研究计划</h1><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><div id="refer-anchor"></div><p>[1] <a href="http://papers.nips.cc/paper/7181-Attention-is-all-you-%0Aneed.pdf" target="_blank" rel="noopener">Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems. 2017.</a><br>[2] <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional Transformers for language understanding.” arXiv preprint arXiv:1810.04805 (2018).</a><br>[3] <a href="https://arxiv.org/abs/1902.09113" target="_blank" rel="noopener">Guo, Qipeng, et al. “Star-Transformer.” arXiv preprint arXiv:1902.09113 (2019).</a><br>[4] <a href="https://openreview.net/forum?id=Hye87grYDH" target="_blank" rel="noopener">Zhao, Guangxiang, et al. “Sparse Transformer: Concentrated Attention Through Explicit Selection.” (2019).</a><br>[5] <a href="https://arxiv.org/abs/1911.04070" target="_blank" rel="noopener">Ye, Zihao, et al. “Bp-Transformer: Modelling long-range context via binary partitioning.” arXiv preprint arXiv:1911.04070 (2019).</a><br>[6] <a href="https://arxiv.org/abs/2004.05150" target="_blank" rel="noopener">Beltagy, Iz, Matthew E. Peters, and Arman Cohan. “Longformer: The long-document Transformer.” arXiv preprint arXiv:2004.05150 (2020).</a><br>[7] <a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener">Dai, Zihang, et al. “Transformer-xl: Attentive language models beyond a fixed-length context.” arXiv preprint arXiv:1901.02860 (2019).</a><br>[8] <a href="http://papers.nips.cc/paper/8812-xlnet-generalizedautoregressive-pretraining-for-language-understanding" target="_blank" rel="noopener">Yang, Zhilin, et al. “Xlnet: Generalized autoregressive pretraining for language understanding.” Advances in neural information processing systems 32 (2019).</a><br>[9] <a href="https://arxiv.org/abs/1905.07799" target="_blank" rel="noopener">Sukhbaatar, Sainbayar, et al. “Adaptive Attention span in Transformers.” arXiv preprint arXiv:1905.07799 (2019).</a><br>[10] <a href="https://arxiv.org/abs/2001.04451" target="_blank" rel="noopener">Kitaev, Nikita, Łukasz Kaiser, and Anselm Levskaya. “Reformer: The efficient Transformer.” arXiv preprint arXiv:2001.04451 (2020).</a><br>[11] <a href="https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00353/97776" target="_blank" rel="noopener">Roy, Aurko, et al. “Efficient content-based sparse Attention with routing Transformers.” Transactions of the Association for Computational Linguistics 9 (2021): 53-68.</a><br>[12] <a href="https://arxiv.org/abs/2012.15688" target="_blank" rel="noopener">Ding, Siyu, et al. “ERNIE-DOC: The Retrospective Long-Document Modeling Transformer.” arXiv preprint arXiv:2012.15688 (2020).</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;立题依据&quot;&gt;&lt;a href=&quot;#立题依据&quot; class=&quot;headerlink&quot; title=&quot;立题依据&quot;&gt;&lt;/a&gt;立题依据&lt;/h1&gt;&lt;h2 id=&quot;研究意义&quot;&gt;&lt;a href=&quot;#研究意义&quot; class=&quot;headerlink&quot; title=&quot;研究意义&quot;&gt;&lt;/a
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>bert-flow&amp;&amp;whiten</title>
    <link href="https://lingyixia.github.io/2021/10/31/bert-flow-whiten/"/>
    <id>https://lingyixia.github.io/2021/10/31/bert-flow-whiten/</id>
    <published>2021-10-31T06:26:47.000Z</published>
    <updated>2022-01-19T12:53:25.303Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>分享两篇论文:<a href="https://arxiv.org/abs/2011.05864" target="_blank" rel="noopener">BERT-Flow</a>和<a href="https://arxiv.org/abs/2103.15316" target="_blank" rel="noopener">BERT-whitening</a></p></blockquote><h1 id="BERT-Flow"><a href="#BERT-Flow" class="headerlink" title="BERT-Flow"></a>BERT-Flow</h1><h2 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a>提出问题</h2><p>$\qquad$论文中作者发现，Bert虽然作为上游任务在特定任务上做有监督微调的时候会有大幅度提升，但在无监督任务，即直接用Bert输出作为特征的时候，效果往往不如意，作者实验的任务无监督语义检索（句子相似度评估），通常利用Bert做寓意检索任务包含两个步骤，分别将两个句子输入Bert，直接用$[CLS]$，或者得到后几层的输出做个$Pool$或者$Average$,最后用$Cosine$计算距离，但是这样得到的结果甚至比直接用$Glove$做平均还要差，表现为任选两个句子相似度都很高。因此作者提出了两个问题：</p><ol><li>是否$Bert$得到的句子向量语义不足？</li><li>是否用$Cosine$计算相似度有问题？</li></ol><h2 id="Bert-语义问题"><a href="#Bert-语义问题" class="headerlink" title="$Bert$语义问题"></a>$Bert$语义问题</h2><p>$\qquad$这里引用了另一篇论文的内容:<em><a href="https://arxiv.org/abs/1711.03953" target="_blank" rel="noopener">《Breaking the softmax bottleneck: a high-rank rnn language model》</a></em>,这里也一并带一下,对于一个语言模型而言，通常建模可以简化成这样的形式:</p><script type="math/tex; mode=display">P_\theta(x|c)=\frac{e^{h_c^Tw_x}}{\sum_{x'}e^{h_c^Tw_{x'}}} \tag{1}</script><p>其中$c$是$x$的$context$, $h_c$和$w_x$分别是$c$和$x$的表征向量，两个向量都是关于$\theta$的函数。<br>然后，作者又引入三个矩阵:</p><script type="math/tex; mode=display">H_\theta=\begin{bmatrix}    h_{c1}^T\\    h_{c2}^T\\    h_{c3}^T \\    ...\\    h_{cN}^T  \end{bmatrix}\quad W_\theta=\begin{bmatrix}    w_{x1}^T\\    w_{x2}^T\\    w_{x3}^T \\    ...\\    w_{xM}^T  \end{bmatrix} \quad A=\begin{bmatrix}    logP^*(x_1|c_1) & logP^*(x_2|c_1) & logP^*(x_3|c_1) &\cdots&logP^*(x_M|c_1)\\    logP^*(x_1|c_2) & logP^*(x_2|c_2) & logP^*(x_3|c_2) &\cdots&logP^*(x_M|c_2)\\    logP^*(x_1|c_3) & logP^*(x_2|c_3) & logP^*(x_3|c_3) &\cdots&logP^*(x_M|c_3)\\    \vdots &\vdots&\vdots&\ddots&\vdots\\    logP^*(x_1|c_N) & logP^*(x_2|c_N) & logP^*(x_3|c_N) &...&logP^*(x_M|c_N) \tag{2}   \end{bmatrix}</script><p>其中，$N$是$context$的数量,$N$是单词的数量，而$P^*(x|c)$是训练数据的分布，。可以看出，其实$H_\theta$其实就是整个语言模型。<br>引入矩阵集合:</p><script type="math/tex; mode=display">F(A)=\{A+ \Lambda J_{N,M}|\Lambda \quad \in diagonal \} \tag{3}</script><p>其中$J_{N,M}$是一个全为1的矩阵，$\Lambda$是一个随机初始化的对角矩阵，可以看出来$F(A)$中的每个矩阵都和$A$同型，且$F(A)$实际上就是给$A$的每行加一个数，比如第一行加2，第二行加1等等。由于$\Lambda$的随机性，$F(A)$是一个无穷大的矩阵集合，该集合有两个性质：</p><ol><li>矩阵$A’ \in F(A)$ 当且仅当$Softmax(A’,axis=1)=P^*$,即$F(A)$定义了训练数据所有可能的logits,这个logits的概念可以认为是模型输出未经过$Softmax$的结果，和tensorflow的参数一样。</li><li>对任意$A \in F(A),B \in F(A),|R(A)-R(B)|\leq 1$.</li></ol><p>第一条不好理解，我解释一下:<br>我们希望$P_\theta(x|c)=P^*(x|c)$,而有：</p><script type="math/tex; mode=display">Softmax(A,axis=1)=\begin{bmatrix}    P^*(x_1|c_1) & P^*(x_2|c_1) & P^*(x_3|c_1) &\cdots&P^*(x_M|c_1)\\    P^*(x_1|c_2) & P^*(x_2|c_2) & P^*(x_3|c_2) &\cdots&P^*(x_M|c_2)\\    P^*(x_1|c_3) & P^*(x_2|c_3) & P^*(x_3|c_3) &\cdots&P^*(x_M|c_3)\\    \vdots &\vdots&\vdots&\ddots&\vdots\\    P^*(x_1|c_N) & P^*(x_2|c_N) & P^*(x_3|c_N) &...&P^*(x_M|c_N)  \end{bmatrix}     \tag{4}</script><p>$\qquad$因此就是希望$Softmax(H_\theta W_\theta,axis=1)=Softmax(A,axis=1)$,一个理想的模型是希望$H_\theta W_\theta=A$(其实$H_\theta W_\theta$矩阵就是所谓的logits)。然后，你把$A$每行加一个数字在做$Softmax$，例如第一行+a,在做$Softmax$,其实就是给$Softmax$分子分母同乘以$e^a$,结果是不变的(原文好像没有解释这一点，挺坑的)。<br>$\qquad$现在回到原始问题，从上诉等式我们可以有这条公式:</p><script type="math/tex; mode=display">h_c^Tw_x=PMI(x,c)+logP^*(x)+\lambda_c \tag{5}</script><p>$\qquad$说明训练充分的$Bert$模型能够反映$w$和$c$的共现关系，也就能反应$c_1$和$c_2$的共现关系(c是由w组成的)，而句子向量就是$c$，因此训练得到的句子向量中包含共现关系，即包含了相似性信息。<br>$\qquad$其实这个证明是并不限制$Bert$,所有以预测单词为任务的语言模型都是一样的道理。</p><h2 id="Cosine-相似度计算"><a href="#Cosine-相似度计算" class="headerlink" title="$Cosine$相似度计算"></a>$Cosine$相似度计算</h2><p>$\qquad$基本上我们遇到和位置有关的相似度计算问题，首选都是$Cosine$距离,那么，用$Cosine$计算相似度需要什么条件呢？<br>$\qquad$答案是标准正交基，假设有三个向量$\alpha:[2,1],\beta:[-2,1],\gamma:[2,-1] $,问$\beta,\gamma$哪个和$\alpha$距离近，我们看下面三个例子：</p><ul><li><p>标准正交基:</p><script type="math/tex; mode=display">\begin{bmatrix}  1&0\\  0&1\\\end{bmatrix} \times \begin{bmatrix}  2 & -2 & 2\\  1 & 1 & -1\\\end{bmatrix}=  \begin{bmatrix}  2 & -2 & 2\\  1 & 1 & -1\\\end{bmatrix} \quad cos<\alpha,\beta>=3,cos<\alpha,\gamma>=-3</script></li><li><p>坐标系1</p><script type="math/tex; mode=display">\begin{bmatrix}  1&0\\  0&2\\\end{bmatrix} \times \begin{bmatrix}  2 & -2 & 2\\  1 & 1 & -1\\\end{bmatrix}=  \begin{bmatrix}  2 & -2 & 2\\  2 & 2 & -2\\\end{bmatrix} \quad cos<\alpha,\beta>=0,cos<\alpha,\gamma>=0</script></li><li><p>坐标系2</p><script type="math/tex; mode=display">\begin{bmatrix}  1&0\\  0&4\\\end{bmatrix} \times \begin{bmatrix}  2 & -2 & 2\\  1 & 1 & -1\\\end{bmatrix}=  \begin{bmatrix}  2 & -2 & 2\\  4 & 4 & -4\\\end{bmatrix} \quad cos<\alpha,\beta>=12,cos<\alpha,\gamma>=-12</script><p>看到没有，三种坐标系下的cos距离完全不同，甚至大小排序都发生了变化。<br>$\qquad$那下一个问题就是， 之所以$Cosine$距离效果差，是不是因为$Bert$的词向量不是来自标准正基？要证明这个问题就要知道其基底是什么，但是这个无法准确得知，但是我们可以猜测，我们在选择基底的时候，会设法让词向量的各个维度独立同分布，如果$Bert$得到的各个词向量满足的话则认为是基底是正交基（并不是充要条件，只是这样猜测），而当词向量的各个维度独立同分布的时候，自然满足各向同性(协方差矩阵为$aE$的形式，a是常数，E是单位矩阵)，因此我们就看是否满足各向同性即可。<br>$\qquad$我比较疑惑的是这一点，文章经过实验发现高频距离原点近，低频距离远点远，并根据这个现象认为不满足各向同性，这不应该是正常现象么？即使标准正态分布也是这样的呀？反正作者最后认为，由于各向异性，$Cosine$距离判断相似度不准，需要修正。</p></li></ul><h2 id="flow"><a href="#flow" class="headerlink" title="flow"></a>flow</h2><p>$\qquad$<a href="https://arxiv.org/abs/2011.05864" target="_blank" rel="noopener">flow</a>是一种</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;分享两篇论文:&lt;a href=&quot;https://arxiv.org/abs/2011.05864&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BERT-Flow&lt;/a&gt;和&lt;a href=&quot;https://arxiv.org/ab
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>神经网络生成模型学习总结</title>
    <link href="https://lingyixia.github.io/2021/10/20/NetWorkGenerator/"/>
    <id>https://lingyixia.github.io/2021/10/20/NetWorkGenerator/</id>
    <published>2021-10-20T15:59:19.000Z</published>
    <updated>2021-11-06T08:48:47.936Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h1><p>$\qquad$众所周知，神经网络是个<strong>判别模型</strong>，此处所指”生成模型”指的是功能性质，即利用神经网络生成我们需要的数据，比如对话生成/图像生成/语音生成。<br>$\qquad$试想，我们现在想生成一个图片，需要怎么做呢？</p><ol><li>需要知道我们要生成一个什么样图片,比如我们想生成一个卡通头像</li><li>需要找到类似所需卡通图片的很多卡通图像</li><li>用2中的所有图像训练一个模型</li><li>给3中的模型一个输入，得到所需卡通头像</li></ol><h1 id="AutoRegression"><a href="#AutoRegression" class="headerlink" title="AutoRegression"></a>AutoRegression</h1><p><img src="/img/autoregression.png"></p><center>AutoRegression</center><p>自回归是一种Step By Step的方式, 相关论文有:<a href="https://arxiv.org/abs/1609.03499" target="_blank" rel="noopener">WaveNet</a>、<a href="https://arxiv.org/abs/1601.06759" target="_blank" rel="noopener">PixelRNN</a>、<a href="https://arxiv.org/abs/1606.05328" target="_blank" rel="noopener">PixelCNN</a><br>优点：对对话/语音这类顺序明显的类别比较友好<br>缺点：</p><ol><li>对图像而言，很难确定一个好的生成顺序</li><li>在inference阶段，当前步骤严重依赖前一步骤</li><li>慢</li></ol><h1 id="AutoEncoder"><a href="#AutoEncoder" class="headerlink" title="AutoEncoder"></a>AutoEncoder</h1><p><img src="/img/autoencoder.png"></p><center>AutoEncoder</center><p>优点:</p><ol><li>无需Step By Step，可以并行生成</li><li>生成数据更规则，一板一眼。</li></ol><p>缺点：loss是对元素级别的监督度量，全局信息关注不足，因此模型泛化不够，两个非常接近的输入，可能输出天差地别。<br>举例说明：<br><img src="/img/pixel.png"><br>你认为左图和右图中哪个更相似呢？很显然是B，但是它和输入图像差了5个pixel，而A只差了2个pixel，而按照AutoEncoder的方式训练，模型认为和A更相似。</p><h2 id="Variational-Autoencoder"><a href="#Variational-Autoencoder" class="headerlink" title="Variational Autoencoder"></a>Variational Autoencoder</h2><p>$\qquad$现在想想，我们训练了这样一个模型，要怎么用呢？最简单的用法是把$Eocnder$编码后的向量(latent vertor)，保存下来，用的时候在用$Decoder$生成，但这仅仅是压缩的作用，显然大材小用了，我们其实想要的是,<strong>对于$Encoder$编码后的向量所符合的分布中，任意取一个向量都能产生合理的图片。</strong> 这样我们就能只保存$Decoder$和分布相关参数,从而产生出我们没见过的图片。很显然，现在的问题是，我们根本无法控制$Encoder$的输出分布，也就无法判断某一个向量是否符合该分布，这种情况下$VEA$出现了。<br>$\qquad$ VAE 其实最理想的控制$Encoder$分布的方式是模型训练好之后，用所有的训练输入通过$Encoder$各自得到一个向量，然后用这些向量得到分布，但是该方法几乎不可能，我们常用的方式类似最大似然，假设一个分布，然后用数据计算参数，VAE的思想是，对$Encoder$添加约束，强制使其输出符合高斯分布，那么模型训练好之后就能从该高斯分布中随机取向量生成没见过的图片了。<br>$\quad$说起来简单，但实际上VAE是一个实现起来简单，但是理解起来很复杂的模型，后续专门写一篇，这次就作为简单理解。</p><h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><p><img src="/img/gan.png"></p><p><center>GAN</center></p><ol><li>从任意分布（如高斯分布）sample一个向量，输入Generator得到和所需图片相同size的输出</li><li>把1中的输出当作副样本，从训练数据中sample一个正样本，以二分类的形式训练Discriminator(freeze Generator)。</li><li><p>重新sample一个向量，输入Generator得到和所需图片相同size的输出，将其当作正样本输入Discriminator，计算loss 训练Generator(freeze Discriminator)<br>上诉三个步骤是一个轮次。<br>这里面有两个问题：</p></li><li><p>Generator可不可以自己学？<br>理论是可以，但操作起来不行，如果这样做的话训练应该是这样一个流程：<br>从任意分布（如高斯分布）sample一个向量，输入Generator希望得到和真实数据一样的分布，即$Generator$应该满足：</p><script type="math/tex; mode=display">G=arg\max_G\sum_i logp_g(x_i) \tag{1}</script><p>这个很容易理解，其实就是一个最大似然估计而已，但是很显然，$G(x_i)$我们无从得知。因为我们在概率论中计算这种情况都是假设$G(x)$是某种分布再计算，但是神经网络非常复杂，根本不是假设某个分布就能做到的。</p></li><li>Discriminator可不可以自己学？<br>可以，如果这样做的话训练应该是这样一个流程：<br>首选sample一部分真实图片（正样本），随机初始化一些乱七八糟的图片（负样本），训练Descriminator，然后在求:<script type="math/tex; mode=display">x=arg\max_xD(x) \tag{2}</script>令$x$为负样本在反复计算，知道收敛。 但问题是，公式（2）的计算也是很困难的。</li></ol><h2 id="GAN损失函数"><a href="#GAN损失函数" class="headerlink" title="GAN损失函数"></a>GAN损失函数</h2><p>论文中给出的损失函数是这样的：</p><script type="math/tex; mode=display">\min_G\max_DV(D,G)=E_{x \sim p_{data}(x)}[logD(x)]+E_{z \sim p_z(z)}[log(1-D(G(z)))] \tag{3}</script><p>论文中说，这个损失函数相当于$JS(p_{data}(x)||p_g(x))$，现在我们来证明一下。</p><script type="math/tex; mode=display">JS(p||q)=\frac{1}{2} KL(p||\frac{p+q}{2})+\frac{1}{2}KL(q||\frac{p+q}{2}) \tag{4}</script><blockquote><blockquote><p>首先需要意识到的是，$p_{data}(x)$、$p_g(x)$、$p_z(z)$是三个分布，指的是括号中变量的概率值，即可以看作一个函数。</p></blockquote></blockquote><script type="math/tex; mode=display">\min_G\max_DV(D,G) = \int_x p_{data}(x)\times logD(x)+\int_zp_z(z)\times[log(1-D(G(z)))]dz \tag{5}</script><p>我们对(3)中的第二项进行积分换元：<br>假设$G(x)$是个可逆函数(该假设一般不成立)，则有：</p><script type="math/tex; mode=display">z=G^{-1}(x) \qquad dz=G^{-1}(x)'dx \qquad p_g(x)=p_z(G^{-1}(x))G^{-1}(x)'dx \tag{6}</script><p>因此，带入公式(3)中可得:</p><script type="math/tex; mode=display">\min_G\max_DV(D,G) = \int_x p_{data}(x)\times logD(x)+ p_g(x)\times[log(1-D(x))]dx \tag{7}</script><p>固定$G(x)$，让$D(x)$最大,令偏导为0:</p><script type="math/tex; mode=display">\frac{\partial}{\partial D(x)}=\frac{p_{data}(x)}{D(x)}-\frac{p_g(x)}{1-D(x)}=0 \tag{8}</script><p>最终可得:</p><script type="math/tex; mode=display">D^*_G(x) = \frac{p_{data}(x)}{p_{data}(x)+p_g(x)} \tag{9}</script><p>因此，当$G(x)$固定时，$D^*_G(x) $最优解为公式(7)<br>而又有:</p><script type="math/tex; mode=display">\begin{align}JS(p_{data}(x)||p_g(x))&=\frac{1}{2}\int_x p_{data}(x)log\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}dx+\frac{1}{2}\int_x p_g(x)\log\frac{p_g(x)}{p_{data}(x)+p_g(x)}dx \\&=log2+\frac{1}{2}\int_x p_{data}(x)\times logD^*_G(x)+ p_g(x)\times[log(1-D^*_G(x))]dx \\&=log2+\frac{1}{2}\min_G\max_DV(D,G)\end{align} \tag{10}</script><p>这就得到了GAN损失函数和JS散度的关系。<br><a href="https://github.com/lingyixia/sample_demos/blob/main/Gan.py" target="_blank" rel="noopener">实例代码地址</a></p><h1 id="Flow"><a href="#Flow" class="headerlink" title="Flow"></a>Flow</h1><p>$\qquad$ 无论哪种生成模型，我们努力想要得到的无非两个东西，其一是$latent \quad vector$的分布，其二是$Generator$,这两者是成对的。基本原理就是如果从$latent \quad vector$输入$Generator$得到的结果分布$p_g$和真实数据分布$p_{data}$一致，那么我们就认为这个$latent \quad vector$的分布和$Generator$足够好.<br>$\qquad$ 在这个原理下就引入了两个难题，其一众所周知，数据分布是无法准确得知的，因此一般我们解决方式是强制固定分布来训练$Generator$，其二是如何衡量$Generator$的输出$p_g$和真实数据分布$p_{data}$足够近呢？两个分布距离一般用$KL$散度或$JS$散度，但不知道具体分布同样无法计算，$Gan$的方案是引入$Discriminator$，最后发现，诶，这个损失函数和$JS$散度等价，大功告成。<br>$\qquad$ 而Flow说，GAN算不了分布，我能算。<br>$\qquad$ 对于一个$Generator$而言，我们希望让其输出和真实数据分布越接近越好，即公式（1），但难点是$p_g(x_i)$未知，Flow的思想就是想方设法把这个分布用公式表达出来。<br>其基本原理是这样的，假设$z \sim \pi(z)$，经过$x=f(z)$函数的映射，得到$x\sim p(x)$,其中$f(z)$是可逆函数则有:</p><script type="math/tex; mode=display">p(x)=\pi(z)\times |\det(J_f)| \tag{11}</script><p><a href="https://www.youtube.com/watch?v=i7LjDvsLWCg&amp;t=32s" target="_blank" rel="noopener">详见此处</a>，所以，我们可以把公式(1)改写为：</p><script type="math/tex; mode=display">G=arg\max_G\sum_i log \pi(G^{-1}(x))\times|\det(J_{G^{-1}})| \tag{12}</script><p>那现在的问题是，我们无法保证一个普通的神经网络是可逆的，因此$G(x)$需要精心设计，而不能随便用。<br>首先需要确定的是，这里的雅可比矩阵必须是方阵(否则怎么算行列式？),因此$z$和$x$必须同形，flow的基本结构称为coupling layer，如图所示:</p><div align="center"><img src="/img/couple.png" height="70%" width="70%"></div><center>coupling layer</center>图中$F$和$H$可以是任意神经网络层。$$[x_1 \sim x_d] =[z_1 \sim z_d] \tag{13}$$$$[x_{d+1}\sim x_D]=F([x_1 \sim x_d]) \odot [x_{d+1} \sim x_D] \oplus H([x_1 \sim x_d]) \tag{14}$$根据这两条关系，我们很容易从计算$z$到$x$，即$G^{-1}(x)$$$G^{-1}(x)=concat[[x_1 \sim x_d],\frac{[x_{d+1}\sim x_D]-[b_{d+1}\sim b_D]}{[a_{d+1}\sim a_D]}] \tag{15}$$还需要计算$|\det(J_{G^{-1}})|$:<div align="center"><img src="/img/JG.png" height="70%" width="70%"></div><p>左上角是单位矩阵，右上角是0矩阵，左下角无关紧要，右下角是对角矩阵。</p><script type="math/tex; mode=display">|\det(J_{G^{-1}})|=\frac{1}{|\det(dignoal)|}=\frac{1}{|\prod_{d+1}^Da_i|}\tag{16}</script><p>因此$G$即为所求。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;生成模型&quot;&gt;&lt;a href=&quot;#生成模型&quot; class=&quot;headerlink&quot; title=&quot;生成模型&quot;&gt;&lt;/a&gt;生成模型&lt;/h1&gt;&lt;p&gt;$\qquad$众所周知，神经网络是个&lt;strong&gt;判别模型&lt;/strong&gt;，此处所指”生成模型”指的是功能性质，即利用
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Lipschitz连续条件</title>
    <link href="https://lingyixia.github.io/2021/09/05/Lipschitz/"/>
    <id>https://lingyixia.github.io/2021/09/05/Lipschitz/</id>
    <published>2021-09-05T08:00:20.000Z</published>
    <updated>2021-09-19T18:03:58.740Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lipschitz约束"><a href="#Lipschitz约束" class="headerlink" title="Lipschitz约束"></a>Lipschitz约束</h2><p>$\qquad$如果对于函数$f_w(x)$的定义域内任意输入$(x_1,f_w(x_1)),(x_2,f_w(x_2))$都满足存在一个常数$L_w$使得：</p><script type="math/tex; mode=display">||f_w(x_1)-f_w(x_2)|| \leq L_w||x_1-x_2|| \tag{1}</script><p>$\qquad$则称$f_w(x)$满足利普希茨连续条件,其中，最小的$L_w$叫做Lipschitz常数。可以看到，Lipschitz连续条件约束的是函数的<strong>范数</strong>，当$f_w(x)$是实值函数时，该范数即是绝对值，该公示可以简单理解为，一个函数的一阶导数有界。</p><h1 id="模型鲁棒性"><a href="#模型鲁棒性" class="headerlink" title="模型鲁棒性"></a>模型鲁棒性</h1><p>$\qquad$做算法的会经常看到鲁棒性这个词，用来描述算法的抗干扰能力，这个性质就是说一个模型，对于两个很接近的输入，其输出也必须很接近，就说其鲁棒性强，用数学语言描述：</p><script type="math/tex; mode=display"> \lim\limits_{\Delta x\to+0} f_w(x+\Delta x) -f_w(x)\rightarrow 0 \tag{2}</script><h1 id="Lipschitz与激活函数"><a href="#Lipschitz与激活函数" class="headerlink" title="Lipschitz与激活函数"></a>Lipschitz与激活函数</h1><p>$\qquad$大家知道,我们常用的激活函数有这些$sigmoid$、$relu$,$tanh$,如果问为什么需要激活函数，回到应该是非线性化，那如果问为什么常用的激活函数是这三个呢？$x^3$行不行？<br>$\qquad$ 答案是不行，从Lipschitz的角度分析，这三个激活函数不仅仅是能够做到非线性化，而且其一阶导函数是有界的，如果使用$x^3$，当输入一个很大的数据$x$ 时，其一阶导函数必然非常大，则$f_w(x+\Delta x)-f_w(x)$也必然很大，则在模型看来，两个很接近的数据其输出却差这么多，这样学下去模型很容易废掉。<br>$\qquad$ 而这三个激活函数不仅保证一阶函数有界，而且在最大导数点在原点，还能更方便的和Normalization结合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lipschitz约束&quot;&gt;&lt;a href=&quot;#Lipschitz约束&quot; class=&quot;headerlink&quot; title=&quot;Lipschitz约束&quot;&gt;&lt;/a&gt;Lipschitz约束&lt;/h2&gt;&lt;p&gt;$\qquad$如果对于函数$f_w(x)$的定义域内任意输入$(x
      
    
    </summary>
    
      <category term="数学" scheme="https://lingyixia.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>n维空间任意两向量几乎正交</title>
    <link href="https://lingyixia.github.io/2021/08/26/ndimvectorangle/"/>
    <id>https://lingyixia.github.io/2021/08/26/ndimvectorangle/</id>
    <published>2021-08-26T06:29:23.000Z</published>
    <updated>2021-10-26T03:56:27.746Z</updated>
    
    <content type="html"><![CDATA[<h1 id="命题"><a href="#命题" class="headerlink" title="命题"></a>命题</h1><p>本文要证明的是高<strong>n维空间中，任意两个向量都几乎正交</strong>，注意：不是两个向量正交的概率大，而是”几乎正交，即夹角接近$\frac{π}{2}$。基本思路就是考虑两个随机向量的夹角$\theta$分布，然后求导得到概率密度，就可以看出在$\theta$在哪个范围内最大。</p><h1 id="命题重定义"><a href="#命题重定义" class="headerlink" title="命题重定义"></a>命题重定义</h1><p>随机两个向量不好求，我们可以先固定一个，让另一个随机即可，假设固定向量为：</p><script type="math/tex; mode=display">x=(1,0,...,0) \tag{1}</script><p>随机向量为：</p><script type="math/tex; mode=display">y=(y_1,y_2,...,y_n) \tag{2}</script><p>现在我们把原命题重新定义为<strong>n维单位超球面上，任意一个点与原点组成的单位向量和$(1,0,0,…,0)$向量都几乎正交</strong><br>直接计算可以得到:</p><script type="math/tex; mode=display">cos \langle x,y \rangle=\frac{x_1}{\sqrt(x_1^2+x_2^2+...x_n^2)} \tag{3}</script><p>现在要求的就是公式(3)的概率分布和密度，到这里还是一筹莫展。</p><h1 id="球坐标系"><a href="#球坐标系" class="headerlink" title="球坐标系"></a>球坐标系</h1><p>将$y$直角坐标转为球坐标系后为：</p><script type="math/tex; mode=display">\left\{    \begin{aligned}    y_1 & =  cos(\varphi_1) \\    y_2 & =  sin(\varphi_1)cos(\varphi_2) \\    y_3 &= sin(\varphi_1)sin(\varphi_2) cos(\varphi_3) \\    .    .    .\\    y_{n-1} &= sin(\varphi_1)sin(\varphi_2)...sina(\varphi_{n-2}) cos(\varphi_{n-1}) \\    y_{n} &= sin(\varphi_1)sin(\varphi_2)...sina(\varphi_{n-2}) sin(\varphi_{n-1})    \end{aligned}\right. \tag{4}</script><p>其中，$\varphi_{n-1} \in[0,2π]$,  $\varphi_{0…n-2} \in[0,π]$<br>此时,公式(3)中$cos \langle x,y \rangle$恰好等于$\varphi_1$,即两者之间的角度就是$\varphi_1$</p><script type="math/tex; mode=display">P_n(\varphi_1<\theta)=\frac{n维超球面上\varphi_1<\theta 部分面积(可以想像为三维球体的环带)}{n维超球体表面积} \tag{5}</script><p>n维超球面上的积分微元是$\sin^{n−2}(\varphi_1)\sin^{n−3}(\varphi_2)⋯sin(\varphi_{n−2})d\varphi1d\varphi2⋯d\varphi_{n−2}d\varphi_{n−1}$<br>因此n维球面积积分为:</p><script type="math/tex; mode=display">\begin{align*}S_n&=\int_0^{2π}d\varphi_{n−1}\int_0^πsin(\varphi_{n−2})d\varphi_{n−2}...\int_0^π\sin^{n−3}(\varphi_2)d\varphi_2\int_0^π\sin^{n−2}(\varphi_1)d\varphi_1 \tag{6}\end{align*}</script><p>故:</p><script type="math/tex; mode=display">\begin{align*}P_n(\varphi_1<\theta) &= \frac{n维超球面上\varphi_1<\theta 部分面积}{n维超球体表面积} \\&=\frac{\int_0^{2π}\int_0^π...\int_0^π \int_0^\theta \sin^{n−2}(\varphi_1)\sin^{n−3}(\varphi_2)⋯sin(\varphi_{n−2})d\varphi1d\varphi2⋯d\varphi_{n−2}d\varphi_{n−1}}{\int_0^{2π}\int_0^π...\int_0^π \int_0^π\sin^{n−2}(\varphi_1)\sin^{n−3}(\varphi_2)⋯sin(\varphi_{n−2})d\varphi_1d\varphi_2⋯d\varphi_{n−2}d\varphi_{n−1}} \\&=\frac{\int_0^{2π}d\varphi_{n−1}\int_0^πsin(\varphi_{n−2})d\varphi_{n−2}...\int_0^π\sin^{n−3}(\varphi_2)d\varphi_2\int_0^\theta\sin^{n−2}(\varphi_1)d\varphi_1}{\int_0^{2π}d\varphi_{n−1}\int_0^πsin(\varphi_{n−2})d\varphi_{n−2}...\int_0^π\sin^{n−3}(\varphi_2)d\varphi_2\int_0^π\sin^{n−2}(\varphi_1)d\varphi_1} \\&=\frac{(n−1)维单位超球的表面积 \times \int_0^\theta\sin^{n−2}(\varphi_1)d\varphi_1}{n维单位超球的表面积} \\&=\frac{\Gamma(\frac{n}{2})}{\Gamma(\frac{n-1}{2})\times \sqrt(π)} \times \int_0^\theta\sin^{n−2}(\varphi_1)d\varphi_1 \tag{7}\end{align*}</script><blockquote><blockquote><p>小插曲:<br>n维球体积:$V_n=\frac{\pi^{\frac{n}{2}}}{\Gamma(n/2)}r^{n-1}$,n维球面积$S_n=\frac{2\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}r^{n-1}=V_n’$</p></blockquote></blockquote><p>因此,  概率密度为:</p><script type="math/tex; mode=display">p_n(\theta)=P_n(\varphi_1<\theta)'=\frac{\Gamma(\frac{n}{2})}{\Gamma(\frac{n-1}{2})\times \sqrt(π)} \times \sin^{n−2} \tag{8}\theta</script><h1 id="密度函数图像"><a href="#密度函数图像" class="headerlink" title="密度函数图像"></a>密度函数图像</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding=utf-8</span><br><span class="line"></span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import seaborn as sns</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fun(n, x):</span><br><span class="line">    return (math.gamma(n / 2) * math.pow(math.sin(x), n - 2)) / (math.sqrt(math.pi) * math.gamma((n - 1) / 2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    datas_dict = dict()</span><br><span class="line">    xs = np.arange(0, 3, 0.01)</span><br><span class="line">    ys = list()</span><br><span class="line">    for n in [2, 3, 5, 10, 50, 100]:</span><br><span class="line">        for x in xs:</span><br><span class="line">            ys.append(fun(n, x))</span><br><span class="line">        datas_dict[str(n)] = ys.copy()</span><br><span class="line">        ys.clear()</span><br><span class="line">    data = pd.DataFrame(datas_dict, index=list(xs))</span><br><span class="line">    sns.lineplot(data=data)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/nvector.jpeg" alt title>                </div>                <div class="image-caption"></div>            </figure><p>参考：<br><a href="https://spaces.ac.cn/archives/7076" target="_blank" rel="noopener">https://spaces.ac.cn/archives/7076</a><br><a href="https://zhuanlan.zhihu.com/p/379317902" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/379317902</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;命题&quot;&gt;&lt;a href=&quot;#命题&quot; class=&quot;headerlink&quot; title=&quot;命题&quot;&gt;&lt;/a&gt;命题&lt;/h1&gt;&lt;p&gt;本文要证明的是高&lt;strong&gt;n维空间中，任意两个向量都几乎正交&lt;/strong&gt;，注意：不是两个向量正交的概率大，而是”几乎正交，即夹角
      
    
    </summary>
    
      <category term="数学" scheme="https://lingyixia.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>SpatialPyramidPooling</title>
    <link href="https://lingyixia.github.io/2021/06/25/SpatialPyramidPooling/"/>
    <id>https://lingyixia.github.io/2021/06/25/SpatialPyramidPooling/</id>
    <published>2021-06-25T02:27:27.000Z</published>
    <updated>2021-09-19T18:01:25.948Z</updated>
    
    <content type="html"><![CDATA[<p>中文名：空间金字塔卷积，通常卷积之后会跟一个全链接层，这就造成如果输入图片size不同，卷积之后size也不同，输入全链接层的feature就不同，这样压根不能运行，通常的解决方案是通过clip/wrap预先将图片处理成相同size，现在可以用该方法来代替。<br><a id="more"></a></p><h1 id="赘述"><a href="#赘述" class="headerlink" title="赘述"></a>赘述</h1><p>通常的CNN结构是这样的：</p><script type="math/tex; mode=display">inputs \rightarrow clip/wrap \rightarrow CNN \rightarrow flat \rightarrow dense</script><p>加入ssp后结构是这样的:</p><script type="math/tex; mode=display">inputs \rightarrow CNN \rightarrow ssp \rightarrow dense</script><h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><ul><li><p>原始：<br>$8 \times 204 \times 196 \times 3 \stackrel{cnn}{\rightarrow}  8 \times 13 \times 14 \times 256 \stackrel{flat}{\rightarrow} 8 \times 46592$<br>$8 \times 302 \times 197 \times 3 \stackrel{cnn}{\rightarrow}  8 \times 19 \times 14 \times 256  \stackrel{flat}{\rightarrow} 8 \times 68096$</p><blockquote><blockquote><p>显然两者最后的输出不可能输入在同一个dense中训练。</p></blockquote></blockquote></li><li><p>clip/warp<br>$8 \times 204 \times 196 \times 3  \stackrel{clip}{\rightarrow} 8 \times 224 \times 224 \times 3  \stackrel{cnn}{\rightarrow} 8 \times 13 \times 13 \times 256  \stackrel{flat}{\rightarrow} 8 \times 43264$<br>$8 \times 302 \times 197 \times 3  \stackrel{clip}{\rightarrow} 8 \times 224 \times 224 \times 3  \stackrel{cnn}{\rightarrow} 8 \times 13 \times 13 \times 256  \stackrel{flat}{\rightarrow} 8 \times 43264$</p></li><li>ssp<br>$8 \times 204 \times 196 \times 3 \stackrel{cnn}{\rightarrow}  8 \times 13 \times 14 \times 256 \stackrel{ssp}{\rightarrow} 8 \times 5376$<br>$8 \times 302 \times 197 \times 3 \stackrel{cnn}{\rightarrow}  8 \times 19 \times 14 \times 256  \stackrel{ssp}{\rightarrow} 8 \times 5376$</li></ul><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def SpatialPyramidPooling(previous_conv, out_pool_size_list):</span><br><span class="line">    b, w, h, c = previous_conv.shape</span><br><span class="line">    for index, pool_size in enumerate(out_pool_size_list):</span><br><span class="line">        w_wid = tf.cast(tf.math.ceil(w / pool_size), tf.int64)</span><br><span class="line">        h_wid = tf.cast(tf.math.ceil(h / pool_size), tf.int64)</span><br><span class="line">        max_pooling = tf.keras.layers.MaxPooling2D(pool_size=(w_wid, h_wid), strides=(w_wid, h_wid), padding=&apos;same&apos;)</span><br><span class="line">        result = tf.reshape(max_pooling(previous_conv), (b, -1))</span><br><span class="line">        spp = result if index == 0 else tf.concat([spp, result], axis=-1)</span><br><span class="line">    return spp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    inputs = tf.random.normal(shape=(8, 19, 14, 256))</span><br><span class="line">    result = SpatialPyramidPooling(inputs, out_pool_size_list=[4, 2, 1])</span><br><span class="line">    print(result.shape)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;中文名：空间金字塔卷积，通常卷积之后会跟一个全链接层，这就造成如果输入图片size不同，卷积之后size也不同，输入全链接层的feature就不同，这样压根不能运行，通常的解决方案是通过clip/wrap预先将图片处理成相同size，现在可以用该方法来代替。&lt;br&gt;
    
    </summary>
    
      <category term="神经网络" scheme="https://lingyixia.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="卷积" scheme="https://lingyixia.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>相对编码</title>
    <link href="https://lingyixia.github.io/2021/04/04/realtive-position-transformer/"/>
    <id>https://lingyixia.github.io/2021/04/04/realtive-position-transformer/</id>
    <published>2021-04-04T15:16:54.000Z</published>
    <updated>2021-09-19T18:01:25.963Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Relative-Position-Embedding"><a href="#Relative-Position-Embedding" class="headerlink" title="Relative Position Embedding"></a>Relative Position Embedding</h1><p><a href="https://arxiv.org/pdf/1803.02155.pdf" target="_blank" rel="noopener">论文地址</a><br><a href="https://wyydsb.xin/other/relativepositionembed.html" target="_blank" rel="noopener">参考博客1</a><br><a href="https://blog.csdn.net/weixin_41089007/article/details/91477253" target="_blank" rel="noopener">参考博客1</a></p><p>该论文的考虑出发点为原始的编码方式仅仅考虑了位置的<strong>距离</strong>关系，没有考虑位置的<strong>先后</strong>关系，本抛弃了<code>vanilla transformer</code>中静态位置编码，使用一个可训练的相对位置矩阵来表示位置信息。<br>公示很简单：<br>原始Attention：</p><script type="math/tex; mode=display">e_{ij}=\frac{x_iW_q x_iW_k^T}{\sqrt{d_{model}}}  \\a_{ij} = softmax(w_ij) \\z_i = \sum_{j=1}^n a_{ij}x_jW_v</script><p>Relative Position Attention:</p><script type="math/tex; mode=display">e_{ij}=\frac{x_iW_q (x_iW_k+a_{ij}^k)^T}{\sqrt{d_{model}}}  \\a_{ij} = softmax(w_{ij}) \\z_i = \sum_{j=1}^n a_{ij}(x_jW_v+a_{ij}^v)</script><blockquote><p>其中，$a_{ij}^k$和$a_{ij}^v$分别表示两个可学习的位置信息,至于为什么加在这两个地方，自然是因为这两个地方计算了相对位置。<br>同时，作者发现如果两个单词距离超过某个阈值$k$提升不大，因此在此限制了位置最大距离，即超过$k$的距离也按照$k$距离的位置信息计算。<br>位置信息本质是在训练两个矩阵$W^K=(w_{-k}^K,…,w_k^K)$和$W^V=(w_{-k}^V,…,w_k^V)$</p><script type="math/tex; mode=display">a_{ij}^K=W_{clip(j-i,k)}^K \\a_{ij}^V=W_{clip(j-i,k)}^V \\clip(x,k)=max(-k,min(k,x))</script></blockquote><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">torch.manual_seed(2021)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RelativePosition(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, num_units, max_relative_position):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_units = num_units</span><br><span class="line">        self.max_relative_position = max_relative_position</span><br><span class="line">        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))</span><br><span class="line">        nn.init.xavier_uniform_(self.embeddings_table)</span><br><span class="line"></span><br><span class="line">    def forward(self, length_q, length_k):</span><br><span class="line">        range_vec_q = torch.arange(length_q)</span><br><span class="line">        range_vec_k = torch.arange(length_k)</span><br><span class="line">        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]</span><br><span class="line">        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)</span><br><span class="line">        final_mat = distance_mat_clipped + self.max_relative_position</span><br><span class="line">        final_mat = torch.LongTensor(final_mat)</span><br><span class="line">        embeddings = self.embeddings_table[final_mat]</span><br><span class="line"></span><br><span class="line">        return embeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MultiHeadAttentionLayer(nn.Module):</span><br><span class="line">    def __init__(self, hid_dim, n_heads, dropout, device):</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        assert hid_dim % n_heads == 0</span><br><span class="line"></span><br><span class="line">        self.hid_dim = hid_dim</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.head_dim = hid_dim // n_heads</span><br><span class="line">        self.max_relative_position = 2</span><br><span class="line"></span><br><span class="line">        self.relative_position_k = RelativePosition(self.head_dim, self.max_relative_position)</span><br><span class="line">        self.relative_position_v = RelativePosition(self.head_dim, self.max_relative_position)</span><br><span class="line"></span><br><span class="line">        self.fc_q = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc_k = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc_v = nn.Linear(hid_dim, hid_dim)</span><br><span class="line"></span><br><span class="line">        self.fc_o = nn.Linear(hid_dim, hid_dim)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)</span><br><span class="line"></span><br><span class="line">    def forward(self, query, key, value, mask=None):</span><br><span class="line">        # query = [batch size, query len, hid dim]</span><br><span class="line">        # key = [batch size, key len, hid dim]</span><br><span class="line">        # value = [batch size, value len, hid dim]</span><br><span class="line">        batch_size = query.shape[0]</span><br><span class="line">        len_k = key.shape[1]</span><br><span class="line">        len_q = query.shape[1]</span><br><span class="line">        len_v = value.shape[1]</span><br><span class="line"></span><br><span class="line">        query = self.fc_q(query)</span><br><span class="line">        key = self.fc_k(key)</span><br><span class="line">        value = self.fc_v(value)</span><br><span class="line"></span><br><span class="line">        r_q1 = query.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        r_k1 = key.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2))  # q对k元素的attention</span><br><span class="line"></span><br><span class="line">        r_k2 = self.relative_position_k(len_q, len_k)</span><br><span class="line">        attn2 = torch.einsum(&apos;bhqe,qke-&gt;bhqk&apos;, r_q1, r_k2)  # q对k位置的attention</span><br><span class="line">        attn = (attn1 + attn2) / self.scale</span><br><span class="line">        if mask is not None:</span><br><span class="line">            attn = attn.masked_fill(mask == 0, -1e10)</span><br><span class="line">        attn = self.dropout(torch.softmax(attn, dim=-1))</span><br><span class="line">        r_v1 = value.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)</span><br><span class="line">        weight1 = torch.matmul(attn, r_v1)  # qk对v元素的attention</span><br><span class="line">        r_v2 = self.relative_position_v(len_q, len_v)</span><br><span class="line">        weight2 = torch.einsum(&apos;bhav,ave-&gt;bhae&apos;, attn, r_v2)  # qk对v位置的attention</span><br><span class="line">        x = weight1 + weight2</span><br><span class="line">        x = x.permute(0, 2, 1, 3).contiguous()</span><br><span class="line">        x = x.view(batch_size, -1, self.hid_dim)</span><br><span class="line">        x = self.fc_o(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    multiHeadAttentionLayer = MultiHeadAttentionLayer(128, 8, 0.5, &apos;cpu&apos;)</span><br><span class="line">    x = torch.randn(4, 43, 128)</span><br><span class="line">    result = multiHeadAttentionLayer(x, x, x)</span><br><span class="line">    print(result)</span><br><span class="line">    # x = torch.randn(64, 8, 43, 16)</span><br><span class="line">    # y = torch.randn(43, 43, 16)</span><br><span class="line">    # print(torch.einsum(&apos;bhqe,qke-&gt;bhqk&apos;, [x, y]))</span><br></pre></td></tr></table></figure><h2 id="TENER"><a href="#TENER" class="headerlink" title="TENER"></a>TENER</h2><blockquote><p>本来应该先写Transformer_xl,但还没完全懂，用这个做过度。</p></blockquote><p><a href="https://arxiv.org/pdf/1911.04474.pdf" target="_blank" rel="noopener">论文地址</a><br><a href="https://blog.csdn.net/Rock_y/article/details/109123472" target="_blank" rel="noopener">参考博客1</a><br><a href="https://www.cnblogs.com/shiyublog/p/11236212.html" target="_blank" rel="noopener">参考博客2</a><br>该论文用在<code>NER</code>任务中，同样的，主要目的也是解决普通<code>Transformer</code>只有<strong>距离</strong>没有<strong>先后</strong>的问题，主要思路是回归了<code>vanilla transformer</code>的编码方式，但是将静态<strong>相对位置</strong>信息加入其中，其实是一个不可训练的相对位置矩阵(具体可看代码，其实表示思路和第一篇相似)。<br>相对位置矩阵为：</p><script type="math/tex; mode=display">R_{t-j}=[...,sin(\frac{t-j}{10000^{2i/d_{model}}}),cos(\frac{t-j}{10000^{2i/d_{model}}})]</script><blockquote><p>可以看到，对于sin而言，正负号是有影响的，但是cos无影响。</p></blockquote><p>原始<code>Transformer</code>中<code>Attention score</code>计算公示为:</p><script type="math/tex; mode=display">A_{i,j}=\underbrace{E_i^TW_qW_kE_j}_a+\underbrace{E_i^TW_qW_kU_j}_b+ \underbrace{U_i^TW_qW_kE_j}_c+\underbrace{U_i^TW_qW_kU_j}_d</script><blockquote><blockquote><p>解释：<br>a:第i个单词<strong>内容</strong>对第j个单词<strong>内容</strong>的score<br>b:第i个单词<strong>内容</strong>对第j个单词<strong>位置</strong>的score<br>c:第i个单词<strong>位置</strong>对第j个单词<strong>内容</strong>的score<br>d:第i个单词<strong>位置</strong>对第j个单词<strong>位置</strong>的score</p></blockquote></blockquote><p>更改后<code>Attention score</code>计算公示为：</p><script type="math/tex; mode=display">A_{i,j}=Q_tK_j^T+Q_tR_{i-j}^T+uK_j^T+vR_{i-j}</script><p>这个看着不方便，其实就是Transformer_xl的公示，应该为：</p><script type="math/tex; mode=display">A_{i,j}=\underbrace{E_i^TW_qW_{k,E}E_j}_a+\underbrace{E_i^TW_qW_{k,R}R_{i-j}}_b+ \underbrace{u^TW_kE_j}_c+\underbrace{v^TW_kU_j}_d</script><blockquote><blockquote><p>不同于之前，这里的四项是分开计算的，其中a不变，b其实就是第一篇论文的$e_{ij}$计算部分，很显然，第一篇论文少了c和d</p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Relative-Position-Embedding&quot;&gt;&lt;a href=&quot;#Relative-Position-Embedding&quot; class=&quot;headerlink&quot; title=&quot;Relative Position Embedding&quot;&gt;&lt;/a&gt;Relat
      
    
    </summary>
    
      <category term="深度学习" scheme="https://lingyixia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Attention" scheme="https://lingyixia.github.io/tags/Attention/"/>
    
      <category term="Transformer" scheme="https://lingyixia.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>泊松分布和指数分布</title>
    <link href="https://lingyixia.github.io/2020/09/20/possionAndexp/"/>
    <id>https://lingyixia.github.io/2020/09/20/possionAndexp/</id>
    <published>2020-09-20T08:38:57.000Z</published>
    <updated>2021-09-19T18:01:25.963Z</updated>
    
    <content type="html"><![CDATA[<p>一直不理解一些分布是干什么用的，怎么来的，看了<a href="https://www.matongxue.com/search/?q=%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83" target="_blank" rel="noopener">马同学高数</a>才有了一些了解，目前的感觉是，似乎常见的分布都来自于二项分布。此处加以记录。</p><a id="more"></a><h1 id="从二项分布到泊松分布"><a href="#从二项分布到泊松分布" class="headerlink" title="从二项分布到泊松分布"></a><a href="https://blog.csdn.net/ccnt_2012/article/details/81114920" target="_blank" rel="noopener">从二项分布到泊松分布</a></h1><ul><li>已知：馒头老板统计了一周内每天卖的馒头数量</li><li>问题：一个馒头店每天需要准备多少馒头？<br>最简单的办法是求平均值，每天就准备这么多馒头就行了，但是问题是如果每天卖出馒头数量方差较大，很容易有好几天准备不足或准备过剩。</li><li>分析：将一天销售时间均分为$n$个阶段，似的每一阶段只卖出一个馒头，那么，每一阶段卖出与不卖出馒头就是一个<strong>伯努利分布</strong>，$n$个阶段就是<strong>二项分布</strong>。设每天卖出馒头为随机变量X，则一天销售时间卖出$k$个馒头的概率为：<script type="math/tex; mode=display">P(X=K)=C_n^kp^k(1-p)^{n-k} \tag{1}</script>很显然，时间是连续的，不能这样分，但是当$n \to+\infty$的时候，可以认为是连续的。<br>上诉二项分布$p$怎么求呢？由$np=\mu$得$p=\frac{\mu}{p}$,$\mu$可由一周内均值近似，则有：<script type="math/tex; mode=display">\begin{aligned}P(X=k)&=\lim_{n \to +\infty} C_n^k(\frac{\mu}{n})^k(1-\frac{\mu}{n})^{n-k}\\&=\lim_{x \to +\infty} \frac{n(n-1)...(n-k+1)}{k!}\frac{\mu^k}{n^k}(1-\frac{\mu}{n})^{n-k} \\&=\lim_{x \to +\infty}\frac{\mu^k}{k!} \frac{n}{n}\frac{n-1}{n}...\frac{n-k+1}{n}(1-\frac{\mu}{n})^{-k}(1-\frac{\mu}{n})^n\end{aligned}</script>其中$\frac{n}{n} \frac{n-1}{n}…\frac{n-k+1}{n}$和$(1-\frac{\mu}{n})^{-k}$在$n \to +\infty$下都是1，对于最后一个因式:<script type="math/tex; mode=display">\lim_{n \to +\infty}(1-\frac{\mu}{n})^n=\lim_{n\to+\infty}((1+(-\frac{1}{\frac{n}{\mu}}))^{-\frac{n}{\mu}})^{-\mu} = e^{-\mu}</script>故而原式可化为$P(X=k)=\frac{\mu^k}{k!}e^{-\mu}$,即泊松分布的公式。<br>因此，可以认为，泊松分布是描述某段时间内，卖出多少馒头的分布，其中$\mu$代表这段时间内卖出馒头的期望。<h1 id="从泊松分布到指数分布"><a href="#从泊松分布到指数分布" class="headerlink" title="从泊松分布到指数分布"></a>从泊松分布到指数分布</h1>上诉泊松分布只告诉了我们确定时间段内的分布，改造该公式如下：<script type="math/tex; mode=display">P(X=k,t)=\frac{(\mu t)^k}{k!}e^{-\mu t}</script>称之为<strong>波松过程</strong>，可以看出，当$t=1$时就是普通的泊松分布，当$t=n$的时候表示时间间隔为$n$内卖出馒头的分布。<br>对于馒头店老板而言，不仅仅需要知道每天要准备多少馒头，还需要知道卖出馒头的时间间隔，好以此适时调整服务员人数。<br>设随机变量X表示两次卖出馒头之间的间隔，则有：<script type="math/tex; mode=display">P(Y>t)=P(X=0,0)=\frac{(\mu t)^0}{0!}e^{-\mu t}=e^{-\mu t}</script><blockquote><blockquote><p>$P(X=k,t)$表示时间段t内卖出k个馒头的概率，P(Y&gt;t)表示两次卖出馒头时间间隔大于t的概率，则就是时间t内卖出0个馒头的概率。<br>则:</p><script type="math/tex; mode=display">P(Y\le t)=1-e^{-\mu t}</script><p>即指数分布</p></blockquote></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一直不理解一些分布是干什么用的，怎么来的，看了&lt;a href=&quot;https://www.matongxue.com/search/?q=%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;马同学高数&lt;/a&gt;才有了一些了解，目前的感觉是，似乎常见的分布都来自于二项分布。此处加以记录。&lt;/p&gt;
    
    </summary>
    
      <category term="概率论" scheme="https://lingyixia.github.io/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="泊松分布" scheme="https://lingyixia.github.io/tags/%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83/"/>
    
      <category term="指数分布" scheme="https://lingyixia.github.io/tags/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83/"/>
    
  </entry>
  
  <entry>
    <title>神经网络到底保存了什么</title>
    <link href="https://lingyixia.github.io/2020/03/18/NetWorkStorage/"/>
    <id>https://lingyixia.github.io/2020/03/18/NetWorkStorage/</id>
    <published>2020-03-18T06:36:26.000Z</published>
    <updated>2021-09-21T06:29:00.303Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>都知道，无论使用什么框架，神经网络都是非常消耗显存的，那么，这些消耗的显存到底保存了什么？</p></blockquote><h1 id="从一个例子开始"><a href="#从一个例子开始" class="headerlink" title="从一个例子开始"></a>从一个例子开始</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/fullnet.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>标注解释:<br>$z_i^k$:第$k$层第$i$个$feature$的输出(不经过激活函数))<br>$a_i^k$：其值为$g(z_i^k)$,$g(x)$为激活函数，这里把输入层数据$x_i$看作第0层。<br>$w_{ij}^k$: 从第$k-1$层到第$k$层的传播权重<br>这里去掉了偏置权重$b$</p><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播:"></a>正向传播:</h2><script type="math/tex; mode=display">\left\{    \begin{aligned}    z_1^1=w_{11}^1a_1^0+w_{21}^1a_2^0 \\    z_2^1=w_{12}^1a_1^0+w_{22}^1a_2^0 \\    z_3^1=w_{13}^1a_1^0+w_{23}^1a_2^0    \end{aligned}\right. \tag{1}</script><script type="math/tex; mode=display">a_i^1=g(z_i^1) \tag{2}</script><script type="math/tex; mode=display">\left\{    \begin{aligned}    z_1^2=w_{11}^2a_1^1+w_{21}^2a_2^1+ w_{31}^2a_3^1\\    z_2^2=w_{12}^2a_1^1+w_{22}^2a_2^1+ w_{32}^2a_3^1\\    \end{aligned}\right. \tag{3}</script><script type="math/tex; mode=display">a_i^2=g(z_i^2) \tag{4}</script><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>假设损失函数为MSE损失:</p><script type="math/tex; mode=display">Loss=\frac{1}{2}\sum_{i=0}^n(y_i-a_i^2)^2 \tag{5}</script><p>下面开始计算对各个参数$w_{ij}^k$的偏导。<br>从最近的开始:</p><script type="math/tex; mode=display">\left\{    \begin{aligned}    \frac{\partial L}{\partial w_{11}^2}= \frac{\partial L}{\partial z_1^2} \frac{\partial z_1^2}{w_{11}^2}=\frac{\partial L}{\partial z_1^2} a_1^1 \\    \frac{\partial L}{\partial w_{21}^2}= \frac{\partial L}{\partial z_1^2} \frac{\partial z_1^2}{w_{21}^2}=\frac{\partial L}{\partial z_1^2} a_2^1 \\    \frac{\partial L}{\partial w_{31}^2}= \frac{\partial L}{\partial z_1^2} \frac{\partial z_1^2}{w_{31}^2}=\frac{\partial L}{\partial z_1^2} a_3^1 \\\\    \frac{\partial L}{\partial w_{12}^2}= \frac{\partial L}{\partial z_2^2} \frac{\partial z_2^2}{w_{12}^2}=\frac{\partial L}{\partial z_2^2} a_1^1 \\    \frac{\partial L}{\partial w_{22}^2}= \frac{\partial L}{\partial z_2^2} \frac{\partial z_2^2}{w_{22}^2}=\frac{\partial L}{\partial z_2^2} a_2^1 \\    \frac{\partial L}{\partial w_{32}^2}= \frac{\partial L}{\partial z_2^2} \frac{\partial z_2^2}{w_{32}^2}=\frac{\partial L}{\partial z_3^2} a_3^1    \end{aligned}\right. \tag{6}</script><script type="math/tex; mode=display">\left\{    \begin{aligned}    \frac{\partial L}{\partial w_{11}^1}= \frac{\partial L}{\partial z_1^1} \frac{\partial z_1^1}{\partial w_{11}^1}=\frac{\partial L}{\partial z_1^1} a_1^0 \\    \frac{\partial L}{\partial w_{21}^1}= \frac{\partial L}{\partial z_1^1} \frac{\partial z_1^1}{\partial w_{21}^1}=\frac{\partial L}{\partial z_1^1} a_2^0 \\\\    \frac{\partial L}{\partial w_{12}^1}= \frac{\partial L}{\partial z_1^1} \frac{\partial z_2^1}{\partial w_{12}^1}=\frac{\partial L}{\partial z_2^1} a_1^0 \\    \frac{\partial L}{\partial w_{22}^1}= \frac{\partial L}{\partial z_2^1} \frac{\partial z_2^1}{\partial w_{22}^1}=\frac{\partial L}{\partial z_1^1} a_2^0 \\\\    \frac{\partial L}{\partial w_{13}^1}= \frac{\partial L}{\partial z_3^1} \frac{\partial z_3^1}{\partial w_{13}^1}=\frac{\partial L}{\partial z_3^1} a_1^0 \\    \frac{\partial L}{\partial w_{23}^1}= \frac{\partial L}{\partial z_3^1} \frac{\partial z_3^1}{\partial w_{23}^1}=\frac{\partial L}{\partial z_3^1} a_2^0    \end{aligned}\right. \tag{7}</script><p>由公示(6)(7)可以总结出来:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w_{ij}^k}=\frac{\partial L}{\partial z_j^k}a_i^{k-1} \tag{8}</script><p>其中$a_i^{k-1}$在正向传播中已经算出，下面计算$\frac{\partial L}{\partial z_j^k}$:</p><script type="math/tex; mode=display">\left\{    \begin{aligned}    \frac{\partial L}{\partial z_1^2}=\frac{\partial L}{\partial a_1^2}g'(z_1^2) \\    \frac{\partial L}{\partial z_2^2}=\frac{\partial L}{\partial a_2^2}g'(z_2^2)     \end{aligned}\right. \tag{9}</script><script type="math/tex; mode=display">\left\{    \begin{aligned}    \frac{\partial L}{\partial z_1^1}=\frac{\partial L}{\partial a_1^1}g'(z_1^1)=(\frac{\partial L}{\partial z_1^2}w_{11}^2+\frac{\partial L}{\partial z_2^2}w_{12}^2)g'(z_1^1) \\    \frac{\partial L}{\partial z_2^1}=\frac{\partial L}{\partial a_2^1}g'(z_2^1)=(\frac{\partial L}{\partial z_1^2}w_{21}^2+\frac{\partial L}{\partial z_2^2}w_{22}^2)g'(z_2^1) \\    \frac{\partial L}{\partial z_3^1}=\frac{\partial L}{\partial a_3^1}g'(z_3^1)=(\frac{\partial L}{\partial z_1^2}w_{31}^2+\frac{\partial L}{\partial z_2^2}w_{32}^2)g'(z_3^1)    \end{aligned}\right. \tag{10}</script><p>由(9)(10)可知，要求得$\frac{\partial L}{\partial z_j^k}$，需要$w_{ij}^k$和$z_i^k$。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>$\qquad$现在可以得出结论，在整个神经网络训练过程中，出于反向传播的需要，我们需要在正向传播的时候在显存中记录$a_i^k$、$w_{ij}^k$和$z_i^k$。但是由于我们常用的激活函数都是可逆的，即知道$a_i^k$能反算出$z_i^k$，因此也可以只保存$a_i^k$、$w_{ij}^k$。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;都知道，无论使用什么框架，神经网络都是非常消耗显存的，那么，这些消耗的显存到底保存了什么？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;从一个例子开始&quot;&gt;&lt;a href=&quot;#从一个例子开始&quot; class=&quot;headerlink&quot; title
      
    
    </summary>
    
      <category term="深度学习" scheme="https://lingyixia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>差分法</title>
    <link href="https://lingyixia.github.io/2019/09/23/difference/"/>
    <id>https://lingyixia.github.io/2019/09/23/difference/</id>
    <published>2019-09-23T00:20:44.000Z</published>
    <updated>2021-10-23T12:36:58.352Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从一道例题开始"><a href="#从一道例题开始" class="headerlink" title="从一道例题开始"></a>从一道例题开始</h1><blockquote><p>有一片大海，大海中隐藏着若干陆地，当海平面下降的时候漏出的连续陆地会组成一个小岛，问题：当海平面处于多高的时候才能使漏出的小岛数量最多？数量是多少？</p></blockquote><h1 id="差分法解释"><a href="#差分法解释" class="headerlink" title="差分法解释"></a>差分法解释</h1><p>一个数组$A[1,2,3,4,5,6,7,8,9]$他的差分数组就是$A[i]-A[i-1]$，为方便计算我们给数组$A$头部插入一个0，变为$A[0,1,2,3,4,5,6,7,8,9]$，则其差分数组为$B[0,1,1,1,1,1,1,1,1,1]$(头部0也是单独插入的)，差分数组又一个很重要的性质,$\sum_0^nB[i]=A[n]$，即$B$的前缀和等于$A$的相应下表的值，这有什么用呢？例如我们想给$A[2:6]$区间+1，除了直接在$A$中循环，还可以把$B$更改为$[0,1,2,1,1,1,1,0,1,1]$,即$B[2]+1,B[7]-1$,然后在按照前缀和的形式加回去，也能算出来，这就是差分法，上题就是一个经典案例。</p><h1 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a>暴力法</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line">#include &lt;algorithm&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    vector&lt;int&gt; nums = &#123;6, 5, 7, 1, 4, 2, 4&#125;;</span><br><span class="line">    nums.insert(nums.begin(), 0);</span><br><span class="line">    nums.push_back(0);</span><br><span class="line">    int max_value = *max_element(nums.begin(), nums.end());</span><br><span class="line">    vector&lt;int&gt; levels(max_value + 1);//海平面</span><br><span class="line">    for(int i = 1; i &lt; nums.size() - 1; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        if(nums[i] &gt; nums[i - 1])</span><br><span class="line">        &#123;</span><br><span class="line">            for(int j = nums[i - 1] + 1; j &lt;= nums[i]; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                levels[j]++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; *max_element(levels.begin(), levels.end());</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>很明显，复杂度$O(n^2)$</p></blockquote></blockquote><h1 id="差分法"><a href="#差分法" class="headerlink" title="差分法"></a>差分法</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line">#include &lt;algorithm&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">void add_diff(vector&lt;int&gt;&amp; diff, int low, int high)</span><br><span class="line">&#123;</span><br><span class="line">    diff[low]++;</span><br><span class="line">    diff[high + 1]--;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    vector&lt;int&gt; nums = &#123;6, 5, 7, 1, 4, 2, 4&#125;;</span><br><span class="line">    nums.insert(nums.begin(), 0);</span><br><span class="line">    nums.push_back(0);</span><br><span class="line">    int max_value = *max_element(nums.begin(), nums.end());</span><br><span class="line">    vector&lt;int&gt; diff(max_value + 1);//差分数组</span><br><span class="line">    for(int i = 1; i &lt; nums.size() - 1; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        if(nums[i] &gt; nums[i - 1])</span><br><span class="line">        &#123;</span><br><span class="line">            add_diff(diff, nums[i - 1] + 1, nums[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    int maxn = 0, pre = 0;</span><br><span class="line">    for(int p = 1; p &lt;= max_value; p++)</span><br><span class="line">    &#123;</span><br><span class="line">        pre += diff[p];</span><br><span class="line">        maxn = max(maxn, pre);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; maxn;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>复杂度$O(n)$</p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;从一道例题开始&quot;&gt;&lt;a href=&quot;#从一道例题开始&quot; class=&quot;headerlink&quot; title=&quot;从一道例题开始&quot;&gt;&lt;/a&gt;从一道例题开始&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;有一片大海，大海中隐藏着若干陆地，当海平面下降的时候漏出的连续陆地会组成一
      
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="差分法" scheme="https://lingyixia.github.io/tags/%E5%B7%AE%E5%88%86%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>BPE 和 WordPiece</title>
    <link href="https://lingyixia.github.io/2019/08/10/BPE/"/>
    <id>https://lingyixia.github.io/2019/08/10/BPE/</id>
    <published>2019-08-10T09:37:51.000Z</published>
    <updated>2021-09-19T18:01:25.943Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>WordPiece 是从 BPE(byte pair encoder) 发展而来的一种处理词的技术，目的是解决 OOV 问题,以翻译模型为例,原理是抽取公共二元串(bigram),首先看下BPE(Transformer的官方代码也是使用的这种方式):</p></blockquote></blockquote><h1 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h1><h2 id="关键代码"><a href="#关键代码" class="headerlink" title="关键代码"></a>关键代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">import re, collections</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_pairs(word):</span><br><span class="line">    &quot;&quot;&quot;Return set of symbol pairs in a word.</span><br><span class="line">    Word is represented as a tuple of symbols (symbols being variable-length strings).</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    pairs = set()</span><br><span class="line">    prev_char = word[0]</span><br><span class="line">    for char in word[1:]:</span><br><span class="line">        pairs.add((prev_char, char))</span><br><span class="line">        prev_char = char</span><br><span class="line">    return pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def encode(orig):</span><br><span class="line">    &quot;&quot;&quot;Encode word based on list of BPE merge operations, which are applied consecutively&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    word = tuple(orig) + (&apos;&lt;/w&gt;&apos;,)</span><br><span class="line">    print(&quot;__word split into characters:__ &lt;tt&gt;&#123;&#125;&lt;/tt&gt;&quot;.format(word))</span><br><span class="line"></span><br><span class="line">    pairs = get_pairs(word)</span><br><span class="line"></span><br><span class="line">    if not pairs:</span><br><span class="line">        return orig</span><br><span class="line"></span><br><span class="line">    iteration = 0</span><br><span class="line">    while True:</span><br><span class="line">        iteration += 1</span><br><span class="line">        print(&quot;__Iteration &#123;&#125;:__&quot;.format(iteration))</span><br><span class="line">        print(&quot;bigrams in the word: &#123;&#125;&quot;.format(pairs))</span><br><span class="line">        print(pairs)</span><br><span class="line">        bigram = min(pairs, key=lambda pair: bpe_codes.get(pair, float(&apos;inf&apos;)))</span><br><span class="line">        print(&quot;candidate for merging: &#123;&#125;&quot;.format(bigram))</span><br><span class="line">        if bigram not in bpe_codes:</span><br><span class="line">            print(&quot;__Candidate not in BPE merges, algorithm stops.__&quot;)</span><br><span class="line">            break</span><br><span class="line">        first, second = bigram</span><br><span class="line">        new_word = []</span><br><span class="line">        i = 0</span><br><span class="line">        while i &lt; len(word):</span><br><span class="line">            try:</span><br><span class="line">                j = word.index(first, i)</span><br><span class="line">                new_word.extend(word[i:j])</span><br><span class="line">                i = j</span><br><span class="line">            except:</span><br><span class="line">                new_word.extend(word[i:])</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">            if word[i] == first and i &lt; len(word) - 1 and word[i + 1] == second:</span><br><span class="line">                new_word.append(first + second)</span><br><span class="line">                i += 2</span><br><span class="line">            else:</span><br><span class="line">                new_word.append(word[i])</span><br><span class="line">                i += 1</span><br><span class="line">        new_word = tuple(new_word)</span><br><span class="line">        word = new_word</span><br><span class="line">        print(&quot;word after merging: &#123;&#125;&quot;.format(word))</span><br><span class="line">        if len(word) == 1:</span><br><span class="line">            break</span><br><span class="line">        else:</span><br><span class="line">            pairs = get_pairs(word)</span><br><span class="line"></span><br><span class="line">    # don&apos;t print end-of-word symbols</span><br><span class="line">    if word[-1] == &apos;&lt;/w&gt;&apos;:</span><br><span class="line">        word = word[:-1]</span><br><span class="line">    elif word[-1].endswith(&apos;&lt;/w&gt;&apos;):</span><br><span class="line">        word = word[:-1] + (word[-1].replace(&apos;&lt;/w&gt;&apos;, &apos;&apos;),)</span><br><span class="line"></span><br><span class="line">    return word</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_stats(vocab):</span><br><span class="line">    pairs = collections.defaultdict(int)</span><br><span class="line">    for word, freq in vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        for i in range(len(symbols) - 1):</span><br><span class="line">            pairs[symbols[i], symbols[i + 1]] += freq</span><br><span class="line">    return pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def merge_vocab(pair, v_in):</span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(&apos; &apos;.join(pair))</span><br><span class="line">    p = re.compile(r&apos;(?&lt;!\S)&apos; + bigram + r&apos;(?!\S)&apos;)</span><br><span class="line">    for word in v_in:</span><br><span class="line">        w_out = p.sub(&apos;&apos;.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    return v_out</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    train_data = &#123;&apos;l o w&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 5,</span><br><span class="line">                  &apos;l o w e r&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 2,</span><br><span class="line">                  &apos;n e w e s t&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 6,</span><br><span class="line">                  &apos;w i d e s t&apos; + re.escape(&apos;&lt;/w&gt;&apos;): 3&#125;</span><br><span class="line">    bpe_codes = &#123;&#125;</span><br><span class="line">    bpe_codes_reverse = &#123;&#125;</span><br><span class="line">    num_merges = 1000</span><br><span class="line">    for i in range(num_merges):</span><br><span class="line">        pairs = get_stats(train_data)</span><br><span class="line">        if not pairs:</span><br><span class="line">            break</span><br><span class="line">        print(&quot;Iteration &#123;&#125;&quot;.format(i + 1))</span><br><span class="line">        best = max(pairs, key=pairs.get)</span><br><span class="line">        train_data = merge_vocab(best, train_data)</span><br><span class="line">        bpe_codes[best] = i</span><br><span class="line">        bpe_codes_reverse[best[0] + best[1]] = best</span><br><span class="line">        print(&quot;new merge: &#123;&#125;&quot;.format(best))</span><br><span class="line">        print(&quot;train data: &#123;&#125;&quot;.format(train_data))</span><br></pre></td></tr></table></figure><p>输出结果:</p><blockquote><blockquote><p>Iteration 1<br>new merge: (‘e’, ‘s’)<br>train data: {‘l o w&lt;/w&gt;’: 5, ‘l o w e r&lt;/w&gt;’: 2, ‘n e w es t&lt;/w&gt;’: 6, ‘w i d es t&lt;/w&gt;’: 3}<br>Iteration 2<br>new merge: (‘es’, ‘t&lt;/w&gt;’)<br>train data: {‘l o w&lt;/w&gt;’: 5, ‘l o w e r&lt;/w&gt;’: 2, ‘n e w est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 3<br>new merge: (‘l’, ‘o’)<br>train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘n e w est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 4<br>new merge: (‘n’, ‘e’)<br>train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘ne w est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 5<br>new merge: (‘ne’, ‘w’)<br>train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘new est&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 6<br>new merge: (‘new’, ‘est&lt;/w&gt;’)<br>train data: {‘lo w&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 7<br>new merge: (‘lo’, ‘w&lt;/w&gt;’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘w i d est&lt;/w&gt;’: 3}<br>Iteration 8<br>new merge: (‘w’, ‘i’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘wi d est&lt;/w&gt;’: 3}<br>Iteration 9<br>new merge: (‘wi’, ‘d’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘wid est&lt;/w&gt;’: 3}<br>Iteration 10<br>new merge: (‘wid’, ‘est&lt;/w&gt;’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lo w e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}<br>Iteration 11<br>new merge: (‘lo’, ‘w’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘low e r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}<br>Iteration 12<br>new merge: (‘low’, ‘e’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lowe r&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}<br>Iteration 13<br>new merge: (‘lowe’, ‘r&lt;/w&gt;’)<br>train data: {‘low&lt;/w&gt;’: 5, ‘lower&lt;/w&gt;’: 2, ‘newest&lt;/w&gt;’: 6, ‘widest&lt;/w&gt;’: 3}</p><p>可以看到，首先输入是词典{单词:词频}的形式,在每一个轮次都会寻找一个最大的子串，上诉第一次频率最大的子串就是(‘e’, ‘s’),然后把字典中所有的(‘e’, ‘s’)合并就得到了{‘l o w &lt;\/w&gt;’: 5, ‘l o w e r &lt;\/w&gt;’: 2, ‘n e w es t &lt;\/w&gt;’: 6, ‘w i d es &lt;\/w&gt;’: 3, ‘f o l l o w &lt;\/w&gt;’: 1},后面以此类推,直到最大的词频小于某个阈值为止，上面设置的是2，最终得到的词表是:train data: {‘low&lt;\/w&gt;’: 5, ‘lower&lt;\/w&gt;’: 2, ‘newest&lt;\/w&gt;’: 6, ‘wides&lt;\/w&gt;’: 3, ‘f o l low&lt;\/w&gt;’: 1}，</p></blockquote></blockquote><p>这就是处理原语料的过程，在训练的时候，首先用上诉的<code>encode</code>代码把训练数据根据<code>code.file</code>映射到’voc.txt’中的词，然后进行训练(label方面的处理方式是独立的，也不一定需要BPE处理)</p><h2 id="subword-nmt使用"><a href="#subword-nmt使用" class="headerlink" title="subword-nmt使用"></a>subword-nmt使用</h2><p>数据准备类似：<a href="https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/tests/data/corpus.en" target="_blank" rel="noopener">https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/tests/data/corpus.en</a></p><ol><li><p>subword-nmt learn-bpe -s {num_operations} &lt; {train_file} &gt; {codes_file}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作用：生成分词器</span><br><span class="line">eg: subword-nmt learn-bpe -s 30000 &lt; corpus.en &gt; codes_file</span><br><span class="line">codes_file生成的就是接下来用到的分词器，其实就是一个词对组成的文件，其中每一行都是当时预料中词对中频率最高的一个。（上诉代码对应这部分）</span><br></pre></td></tr></table></figure></li><li><p>subword-nmt apply-bpe -c {codes_file} &lt; {test_file} &gt; {out_file}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">作用：用分词器处理预料</span><br><span class="line">eg:subword-nmt apply-bpe -c codes_file &lt; corpus.en &gt; out_file</span><br><span class="line">out_file中就是靠分词器生成的语料</span><br><span class="line">这里的操作单元是对原始预料的各个单词，比如&apos;cement&apos;，分为&apos;c e m e n t&lt;/w&gt;&apos;</span><br><span class="line">1. 词对为:[&lt;c,e&gt;,&lt;e,m&gt;,&lt;m,e&gt;,&lt;e,n&gt;,&lt;n,t&lt;/w&gt;&gt;],其中[&lt;e,m&gt;,&lt;m,e&gt;,&lt;e,n&gt;]在codes_file,并且&lt;e,n&gt;在codes_file排名靠前(语料中词频高),合并结果为:&apos;c e m en t&lt;/w&gt;&apos;</span><br><span class="line">2. 词对为:[&lt;c,e&gt;,&lt;e,m&gt;,&lt;m,en&gt;,&lt;en,t&lt;/w&gt;&gt;],其中[&lt;e,m&gt;,&lt;en,t&lt;/w&gt;&gt;]在codes_file,并且&lt;en,t&lt;/w&gt;&gt;在codes_file中排名靠前，合并结果为:&apos;c e m ent&lt;/w&gt;&apos;</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">最终合并结果为:&apos;c ement&lt;/w&gt;&apos;此时只有一个词对&lt;c,ement&lt;/w&gt;&gt;,并且不再codes_file中，因此合并停止，该词分为两个子词:c,ement,在预料中为:c@@ ement</span><br></pre></td></tr></table></figure></li><li><p>subword-nmt get-vocab —train_file {train_file} —vocab_file {vocab_file}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">作用：生成词典（训练模型要用）</span><br><span class="line">eg: subword-nmt get-vocab --input out_file --output vocab_file</span><br><span class="line">vocab_file就是预料对应的词典(把out_file 中的单词set一便即可)，即接下来用vocab_file作为词典，out_file作为语料训练模型即可</span><br></pre></td></tr></table></figure></li><li><p>模型训练完成后，在具体场景使用的时候，必定会有@（因为词典中有@，用来区分该单词是前缀还是独立单词），因此要对后缀是@的单词跟下一个单词合并。</p></li></ol><h1 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h1><blockquote><p>WordPiece是Bert使用的处理方式,这个过两天在写吧，有点事。。。</p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考:"></a>参考:</h1><ol><li><a href="https://github.com/wszlong/sb-nmt" target="_blank" rel="noopener">https://github.com/wszlong/sb-nmt</a></li><li><a href="https://blog.csdn.net/u013453936/article/details/80878412" target="_blank" rel="noopener">https://blog.csdn.net/u013453936/article/details/80878412</a></li><li><a href="http://ufal.mff.cuni.cz/~helcl/courses/npfl116/ipython/byte_pair_encoding.html" target="_blank" rel="noopener">http://ufal.mff.cuni.cz/~helcl/courses/npfl116/ipython/byte_pair_encoding.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;WordPiece 是从 BPE(byte pair encoder) 发展而来的一种处理词的技术，目的是解决 OOV 问题,以翻译模型为例,原理是抽取公共二元串(bigram),首先看下BPE(Transformer的官
      
    
    </summary>
    
      <category term="NLP" scheme="https://lingyixia.github.io/categories/NLP/"/>
    
    
      <category term="算法" scheme="https://lingyixia.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>顿悟最大熵</title>
    <link href="https://lingyixia.github.io/2019/07/28/maxlikehood/"/>
    <id>https://lingyixia.github.io/2019/07/28/maxlikehood/</id>
    <published>2019-07-28T07:27:59.000Z</published>
    <updated>2021-10-28T07:17:51.826Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>虽然以前也了解最大熵模型，甚至以为自己完全理解了最大熵,知道今天才发现我以前都回了点啥。。。。今天发现自己顿悟了一下，特此记录，希望能给看到的人一点帮助，再次膜拜孔圣人的远见”温故而知新，可以为师矣.”(未完待续)</p></blockquote></blockquote><h1 id="从最大熵思想开始"><a href="#从最大熵思想开始" class="headerlink" title="从最大熵思想开始"></a>从最大熵思想开始</h1><p>其实在我看来，所谓的最大熵思想就是对已知的条件<strong>充分考虑</strong>,对未知的条件<strong>不做任何假设</strong>,这就是最大熵的真谛.</p><h1 id="这就是最大熵模型"><a href="#这就是最大熵模型" class="headerlink" title="这就是最大熵模型"></a>这就是最大熵模型</h1><p>5个海贼团$A、B、C、D、E$去争夺罗杰的宝藏,宝藏只有一份,问谁能得到?<br>我们给他建立的模型是<strong>均匀模型</strong>$P(X=A)=\frac{1}{6},P(X=B)=\frac{1}{6},P(X=C)=\frac{1}{6},P(X=D)=\frac{1}{6},P(X=E)=\frac{1}{6},P(X=6)=\frac{1}{6}$<br>我们为毛会建立这样的模型呢?因为均匀模型的<strong>熵最大</strong>,我们对这个5个海贼团一无所知,因此只能创建<strong>均匀模型</strong>来保证<strong>熵最大</strong>，这就是<strong>最大熵</strong>的一个最简单应用</p><p>5个海贼团$A、B、C、D、E$去争夺罗杰的宝藏,我们已知:$A$得到宝藏的概率和$B$一样,都是$\frac{1}{10}$,$C$和$D$一样,都是$\frac{3}{10}$,而$E$就比较牛逼了,他的概率是$\frac{5}{10},因为他叫路飞$,宝藏只有一份,问谁能得到?<br>现在我们有了条件，就不能简单的创建<strong>均匀模型</strong>了,因为此时我们要找的模型不是$P(X,\theta)$,而是$P(Y|X,\theta)$,我们需要的是$P(Y|X,\theta)$的熵最大，这便是<strong>最大熵模型</strong></p><h1 id="看看公式"><a href="#看看公式" class="headerlink" title="看看公式"></a>看看公式</h1><p>已知:$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N) }$<br>目标:运用最大熵原理找最大熵模型<br>由上诉<strong>最大熵模型</strong>的由来我们很容易得出公式,不是要找一个模型,在$X$的条件下这个模型的$P(Y|X,\theta)$的熵最大么,很明显这是个条件熵呀,那么这个模型在这份数据上的条件熵的公式:</p><script type="math/tex; mode=display">H(Y|X) = \sum_{x \in X} \widetilde{P}(x)H(Y|X=x) \tag{1}</script><p>其中$\widetilde{P}(x)=\frac{N_x}{N}$<br>现在我们的目标确定了,我们要从<script type="math/tex">P(Y|X,\theta_1)，P(Y|X,\theta_2)，P(Y|X,\theta_3)，P(Y|X,\theta_4),,,</script>很多的模型中找到一个模型,这个模型<strong>充分考虑</strong>了已知数据$T$,而且对未知<strong>不做任何假设</strong>，<strong>不做任何假设</strong>解决了,我们让公式$(1)$要尽量大即可,那么<strong>充分考虑</strong>已知数据要怎样表示呢?说到这有人可能会有些思绪了,其实就是从$T$中找一个用来表示<strong>充分考虑</strong>已知条件的约束,让公式$(1)$在这个约数下尽量大.找到这个约束,一个最优化问题就出现了,现在我们的目的就是找这个约束。</p><h1 id="充分考虑"><a href="#充分考虑" class="headerlink" title="充分考虑"></a>充分考虑</h1><p>现在我们想想，什么是<strong>充分考虑</strong>了已知数据呢?比如我们找到了一个模型$P(Y|X，\theta)$,要考量它是对已知数据的考虑程度,把这句话数学化就是考量这个模型对原始数据<strong>特征</strong>的考虑程度,怎样表示原始数据的特征呢?<strong>特征函数</strong>出现了！！！<br>我们一般用这样一个<strong>特征函数</strong>来描述<strong>特征</strong>:</p><script type="math/tex; mode=display">f(x,y)=\begin{cases}1 这个x和y满足某一事实\\0 这个x和y不满足某一事实\\\end{cases}</script><p>那么这分数据特征的总值是多少呢?找期望呗</p><script type="math/tex; mode=display">E_{\widetilde p}(f)=\sum_{x,y}\widetilde P(x,y)f(x,y) \tag{2}</script><p>其中$\widetilde P(x,y)=\frac{N_{xy}}{N}$，谨记:$E_{\widetilde p}(f)$是原始数据的特征总值.<br>对于我们找的的模型$P(Y|X，\theta)$在这份数据的特征总值是多少呢?</p><script type="math/tex; mode=display">E_{p}(f)=\sum_{x,y}\widetilde P(x)P(y|x,\theta)f(x,y) \tag{3}</script><p>要想让$P(Y|X,\theta)$<strong>充分考虑</strong>原始数据咋做呢?两个特征总值相等呗</p><script type="math/tex; mode=display">E_{\widetilde p}(f)=E_{p}(f) \tag{4}</script><p>这就是约束条件了.</p><h1 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h1><p>现在我们的目标确定了,我们要让公式$(1)$在约束条件为公式$(4)$的条件下越大越好,现在我们把公式$(1)$展开:</p><script type="math/tex; mode=display">\begin{align}H(Y|X) &= \sum_{x \in X} \widetilde{P}(x)H(Y|X=x)) \\&=-\sum_{x \in X}\widetilde P(x)\sum_{y \in Y}P(y|X=x) \log P(Y|X=x)\\&= -\sum_{x \in X}\sum_{y \in Y}\widetilde P(x)P(y|X=x) \log P(Y|X=x)\\&=-\sum_{x ,y}\widetilde P(x)P(y|x) \log P(y|x)\end{align} \tag{5}</script><p>所以最终公式为:</p><script type="math/tex; mode=display">\max \quad \quad \quad -\sum_{x ,y}\widetilde P(x)P(y|x) \log P(y|x) \\s.t. \quad \quad \quad \quad E_{\widetilde p}(f_i)=E_{p}(f_i)  \quad i=1,2,3... \\ \quad \quad \quad \quad \sum_{y}P(y|x)=1(隐含条件) \tag{6}</script><p>其中i代表第i个特征函数<br>完毕散花!!!!卧槽写完后发现写的好清楚…不信你看了还不明白！</p><h1 id="最大熵模型求解"><a href="#最大熵模型求解" class="headerlink" title="最大熵模型求解"></a>最大熵模型求解</h1><p>1.首先引入拉格朗日乘子$w_0,w_1…$</p><script type="math/tex; mode=display">\begin{align}L(P(y|x),w)&=-\sum_{x ,y}\widetilde P(x)P(y|x) \log P(y|x)+w_0(1-\sum_{y}P(y|x))+\sum_{i=0}^{n}w_i(E_{\widetilde p}(f_i)-E_{p}(f_i))\\&=\sum_{x ,y}\widetilde P(x)P(y|x) \log P(y|x)+w_0(1-\sum_{y}P(y|x))+\sum_{i=1}^n w_i(\sum_{x,y} w_i\widetilde P(x,y)f_i(x,y)-\sum_{x,y}\widetilde P(y|x)f_i(x,y))\end{align} \tag{7}</script><p>原始问题是:</p><script type="math/tex; mode=display">\min_{P(y|x)} \max_wL(P(y|x),w) \tag{8}</script><p>对偶问题是:</p><script type="math/tex; mode=display">\max_{w} \min_{P(y|x)}L(P(y|x),w) \tag{9}</script><p>令:</p><script type="math/tex; mode=display">\Psi(w)=\min_{P(y|x)}L(P(y|x),w) \tag{10}</script><p>首先求内部:</p><script type="math/tex; mode=display">\begin{align}\frac{\partial P(y|x,w)}{\partial P(y|x)}&=\sum_{x,y}\widetilde P(x)(\log P(y|x)+1)-\sum_y w_0 - \sum_{x,y}(\widetilde P(x)\sum_{i=1}^n w_if_i(x,y)) \\&=\sum_{x,y}\widetilde P(x)(\log P(y|x)+1-w_0-\sum_{i=1}^n w_if_i(x,y))\end{align} \tag{11}</script><p>令偏导数为0，得:</p><script type="math/tex; mode=display">P(y|x)=e^{\sum_{i=1}^nw_if_i(x,y)+w_0-1}=\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{e^{1-w_0}} \tag{12}</script><p>因为有:</p><script type="math/tex; mode=display">\sum_{y}P(y|x)=1 \tag{13}</script><p>因此:</p><script type="math/tex; mode=display">e^{1-w_0}=\sum_y e^{\sum_{i=1}^n w_if_i(x,y)}=Z_w(x) \tag{14}</script><p>因此，最终求得:</p><script type="math/tex; mode=display">P(y|x,\theta)=e^{\sum_{i=1}^nw_if_i(x,y)+w_0-1}=\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{\sum_y e^{\sum_{i=1}^n w_if_i(x,y)}} \tag{15}</script><p><strong>注意看这里，这不就是Softmax么!!!!!!!!!!!!!!!!!!</strong><br>最后一步在求<br>$\max_w$即可.</p><h1 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h1><p>最大似然估计的一般公式为:</p><script type="math/tex; mode=display">L(P_w)=log \prod_{x,y}P(y|x)^{\widetilde P(x,y)}=\sum_{x,y}\widetilde P(x,y)logP(y|x) \tag{16}</script><p>我们求得的最大熵模型为:</p><script type="math/tex; mode=display">\Psi(w)=\sum_{x ,y}\widetilde P(x)P(y|x) \log P(y|x)+w_0(1-\sum_{y}P(y|x))+\sum_{i=1}^n w_i(\sum_{x,y} w_i\widetilde P(x,y)f_i(x,y)-\sum_{x,y}\widetilde P(y|x)f_i(x,y)) \tag{17}</script><p>我们要证明这两个式子求得得结果是相同的,即:</p><script type="math/tex; mode=display">P(y|x)=\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{e^{1-w_0}} \tag{18}</script><p>带入得:</p><script type="math/tex; mode=display">\begin{align}L(P_w)&=\sum_{x,y}\widetilde P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x,y}\widetilde P(x,y)\log Z_w(x) \\&=\sum_{x,y}\widetilde P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x \widetilde P(x)\log Z_w(x) \end{align} \tag{19}</script><p>得:</p><script type="math/tex; mode=display">\begin{align}\Psi(w)&=\sum_{x,y}^n\widetilde P(x,y)\sum_{i=1}^nw_if_i(x,y)+\sum_{x,y}\widetilde P(x)P(y|x)(\log P(y|x)-\sum_{i=1}w_if_i(x,y))\\&=\sum_{x,y}^n\widetilde P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x,y}\widetilde P(x)P_w(y|x)\log Z_w(x)\\&=\sum_{x,y}^n\widetilde P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x}\widetilde P(x)\log Z_w(x)\end{align} \tag{20}</script><p>因此，当带入$P(y|x,\theta)$时，得到得公式是相同得，因此求得的$\max_w$也一定相同.</p><h1 id="从最大熵模型到逻辑回归"><a href="#从最大熵模型到逻辑回归" class="headerlink" title="从最大熵模型到逻辑回归"></a>从最大熵模型到逻辑回归</h1><p>二元逻辑回归的似然函数为:</p><script type="math/tex; mode=display">L(\theta)=\prod_{i}^n[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i} \tag{21}</script><p>在这里，其实$\pi(x_i)$就是$P(Y=1|X=x_i)$而且$y_i$完全可以写成数据中$Y=1$的数量你说对也不对。。其实在细想还可以写成数据中$X=x_i,Y=1$的数量.所以可以写成:</p><script type="math/tex; mode=display">L(\theta)=P(Y=1|X=x_i)^{N_{1x_i}}P(Y=0|X=x_i)^{N_{0x_i}} \tag{22}</script><p>在开个N(N是数据总量)次方得:</p><script type="math/tex; mode=display">L(\theta)=P(Y=1|X=x_i)^\frac{N_{1x_i}}{N}P(Y=0|X=x_i)^\frac{N_{0x_i}}{N} \tag{23}</script><p>想想$\frac{N_{0x_i}}{N}$是啥,,,这不就是$\widetilde{P}(x,y)$么,,,而且开个N次方对优化也没有影响,拿这个公式和$公式(3)$去掉log,即$P(y|x)^{\widetilde{p}(x,y)}$对比一下，这不一样么.哈哈哈，有没有恍然大明白的感觉。</p><h1 id="再到最大似然估计"><a href="#再到最大似然估计" class="headerlink" title="再到最大似然估计"></a>再到最大似然估计</h1><p>概率论与数理统计中的最大似然概率为公式为:</p><script type="math/tex; mode=display">\begin{align}L(\theta)&=log\prod_iP(x_i;\theta) \\&=log\prod_iP(x_i;\theta)^{Nx_i} \\&=\sum_iN{x_i}logP(x_i;\theta) \\&=N\sum_i\frac{Nx_i}{N}logP(x_i;\theta)\\&=N\sum_i\widetilde{p}(x_i)logP(x_i;\theta)\\&=Nlog\prod_iP(x_i;\theta)^{\widetilde{P}(x_i)}\end{align} \tag{24}</script><p>这里只是一元$x$,怎么对应到二元$(x,y)$呢？其实两者同理，需要正确的理解<strong>无论是几元，这里的$p(x_i;\theta)$的真正意义是变量特征函数的联合分布</strong>，对应到二元就是:</p><script type="math/tex; mode=display">L(\theta)=Nlog\prod_{i,j}P(x_i,y_j;\theta)^{\widetilde{P}(x_i,y_j)} \tag{25}</script><p>也就是说概率论与数理统计中的最大似然估计其实完全针对的是<strong>联合分布</strong>，特征和标签的地位完全一样,得到的似然函数公式其实是一个<strong>信息熵</strong>。但是我们的目的得到输入任何$x$后$y$的分布，是个<strong>边缘分布</strong>,则该边缘分布的熵就是个<strong>条件熵</strong>：</p><script type="math/tex; mode=display">\begin{align}H(Y|x)&={P}(x) \sum_{y\in Y}P(y|x;\theta)logP(y|x;\theta) \\&=\sum_{y\in Y}P(x,y;\theta)logP(y|x;\theta) \\&=\prod_y logP(y|x;\theta)^{P(x,y;\theta)} \\&=\prod_y logP(y|x;\theta)^{\widetilde P(x,y)} (近似用数出来的数据代替P(x,y;\theta))\\\end{align} \tag{26}</script><p>我觉得这里贴一道考研题比较合适：<br>设总体$X$的概率分布为：</p><div class="table-container"><table><thead><tr><th style="text-align:center">X</th><th style="text-align:center">0</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th></tr></thead><tbody><tr><td style="text-align:center"><strong>P</strong></td><td style="text-align:center">$\theta^2$</td><td style="text-align:center">$2\theta(1-\theta)$</td><td style="text-align:center">$\theta^2$</td><td style="text-align:center">$1-2\theta$</td></tr></tbody></table></div><p>其中$\theta(0 \lt \theta \lt\frac{1}{2})$是待求参数，当前总体样本为:$3,1,3,0,1,2,3$,求$\theta$的最大似然估计。<br>第一步需要找到似然函数，$Ln(\theta)=ln{(1-2\theta)\times(2\theta(1-\theta)\times(1-2\theta) \times \theta^2 \times  2\theta(1-\theta) \times \theta^2 \times  1-2\theta}$，这其实就是公示(16),注意，此时是单变量，此时只有$X$,没有$Y$，即某个特征只对应一种label，公示(16)是普遍表示，即$P(y|x)=P(x),\widetilde P(x,y)=\widetilde P(x)$。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;虽然以前也了解最大熵模型，甚至以为自己完全理解了最大熵,知道今天才发现我以前都回了点啥。。。。今天发现自己顿悟了一下，特此记录，希望能给看到的人一点帮助，再次膜拜孔圣人的远见”温故而知新，可以为师矣.”(未完待续)&lt;/p&gt;
      
    
    </summary>
    
      <category term="数学" scheme="https://lingyixia.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="机器学习" scheme="https://lingyixia.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>贪心</title>
    <link href="https://lingyixia.github.io/2019/07/27/greedy/"/>
    <id>https://lingyixia.github.io/2019/07/27/greedy/</id>
    <published>2019-07-27T02:37:31.000Z</published>
    <updated>2021-09-19T18:01:25.958Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Jump-Game"><a href="#Jump-Game" class="headerlink" title="Jump Game"></a><a href="https://leetcode.com/problems/jump-game/" target="_blank" rel="noopener">Jump Game</a></h1><p>贪心一:</p><blockquote><blockquote><p>每到一个i,如果i&lt;=reach意味着[0,i-1]的坐标能达到reach,如果i&gt;reach,则意味着根本就到不了这里,无需继续。</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">bool canJump(vector&lt;int&gt;&amp; nums) </span><br><span class="line">&#123;</span><br><span class="line">int reach = 0;</span><br><span class="line">for (int i =0; i &lt; nums.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">if (i &gt; reach || i &gt;= nums.size() - 1) break;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">reach = max(reach, i + nums[i]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">return reach &gt;= nums.size() - 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>贪心二:</p><blockquote><blockquote><p>和上诉形式不一样,思想差不多</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bool canJump(vector&lt;int&gt;&amp; nums) </span><br><span class="line">&#123;</span><br><span class="line">int len = nums.size();</span><br><span class="line">int curMax = nums[0];</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt;= curMax; i++)</span><br><span class="line">&#123;</span><br><span class="line">if (nums[i] + i &gt;= len - 1) return true;</span><br><span class="line">curMax = max(curMax, nums[i] + i);</span><br><span class="line">&#125;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>动态规划:</p><blockquote><blockquote><p>dp[i]表示到达i时候最多还剩下多少步</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bool canJump(vector&lt;int&gt;&amp; nums) </span><br><span class="line">&#123;</span><br><span class="line">vector&lt;int&gt; dp(nums.size(), 0);</span><br><span class="line">for (int i = 1; i &lt; nums.size(); ++i) </span><br><span class="line">&#123;</span><br><span class="line">dp[i] = max(dp[i - 1], nums[i - 1]) - 1;</span><br><span class="line">if (dp[i] &lt; 0) return false;</span><br><span class="line">&#125;</span><br><span class="line">return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="推销员"><a href="#推销员" class="headerlink" title="推销员"></a>推销员</h1><blockquote><blockquote><p>阿明是一名推销员，他奉命到螺丝街推销他们公司的产品。螺丝街是一条死胡同，出口与入口是同一个，街道的一侧是围墙，另一侧是住户。螺丝街一共有 N 家住户，第 i 家住户到入口的距离为 Si 米。由于同一栋房子里可以有多家住户，所以可能有多家住户与入口的距离相等。阿明会从入口进入，依次向螺丝街的 X 家住户推销产品，然后再原路走出去。<br>阿明每走 1 米就会积累 1 点疲劳值，向第 i 家住户推销产品会积累 Ai 点疲劳值。阿明是工作狂，他想知道，对于不同的 X，在不走多余的路的前提下，他最多可以积累多少点疲劳值。</p></blockquote></blockquote><p>暂存</p><p>#</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Jump-Game&quot;&gt;&lt;a href=&quot;#Jump-Game&quot; class=&quot;headerlink&quot; title=&quot;Jump Game&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://leetcode.com/problems/jump-game/&quot; target=&quot;
      
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Bert</title>
    <link href="https://lingyixia.github.io/2019/07/22/Bert/"/>
    <id>https://lingyixia.github.io/2019/07/22/Bert/</id>
    <published>2019-07-22T13:55:40.000Z</published>
    <updated>2021-09-19T18:01:25.943Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文不讲Bert原理,拟进行Bert源码分析和应用</p></blockquote><h1 id="Bert源码分析"><a href="#Bert源码分析" class="headerlink" title="Bert源码分析"></a>Bert源码分析</h1><h2 id="组织结构"><a href="#组织结构" class="headerlink" title="组织结构"></a>组织结构</h2><blockquote><blockquote><p><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">这是Bert Github地址</a>,打开后会看到这样的结构:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/Bert/bert.jpeg" alt title>                </div>                <div class="image-caption"></div>            </figure><br>下面我将逐个分析上诉图片中加框的文件,其他的文件不是源码,不用分析.</p></blockquote></blockquote><h2 id="modeling-py"><a href="#modeling-py" class="headerlink" title="modeling.py"></a>modeling.py</h2><blockquote><blockquote><p>该文件是整个Bert模型源码,包含两个类:</p><ul><li>BertConfig:Bert配置类</li><li>BertModel:Bert模型类</li><li>embedding_lookup:用来返回函数token embedding词向量</li><li>embedding_postprocessor:得到token embedding+segment embedding+position embedding</li><li>create_attention_mask_from_input_mask得到mask,用来attention该attention的部分</li><li>transformer_model和attention_layer:Transform的ender部分,也就是self-attention,不解释了，看太多遍了.</li></ul></blockquote></blockquote><p><strong>注意上面的顺序,不是乱写的,是按照BertModel调用顺序组织的.</strong></p><h3 id="BertConfig"><a href="#BertConfig" class="headerlink" title="BertConfig"></a>BertConfig</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">class BertConfig(object):</span><br><span class="line">    def __init__(self,</span><br><span class="line">                 vocab_size,#词表大小</span><br><span class="line">                 hidden_size=768,#即是词向量维度又是Transform的隐藏层维度</span><br><span class="line">                 num_hidden_layers=12,#Transformer encoder中的隐藏层数,普通Transform中是6个</span><br><span class="line">                 num_attention_heads=12,multi-head attention 的head的数量,普通Transform中是8个</span><br><span class="line">                 intermediate_size=3072,encoder的“中间”隐层神经元数,普通Transform中是一个feed-forward</span><br><span class="line">                 hidden_act=&quot;gelu&quot;,#隐藏层激活函数</span><br><span class="line">                 hidden_dropout_prob=0.1,#隐层dropout率</span><br><span class="line">                 attention_probs_dropout_prob=0.1,#注意力部分的dropout</span><br><span class="line">                 max_position_embeddings=512,#最大位置编码长度,也就是序列的最大长度</span><br><span class="line">                 type_vocab_size=16,#token_type_ids的大小,所谓的token_type_ids在Bert中是0或1，也就是上句标记为0，下句标记为1，鬼知道默认为16是啥意思。。。</span><br><span class="line">                 initializer_range=0.02):随机初始化的参数</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_hidden_layers = num_hidden_layers</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        self.hidden_act = hidden_act</span><br><span class="line">        self.intermediate_size = intermediate_size</span><br><span class="line">        self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">        self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">        self.max_position_embeddings = max_position_embeddings</span><br><span class="line">        self.type_vocab_size = type_vocab_size</span><br><span class="line">        self.initializer_range = initializer_range</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_dict(cls, json_object):</span><br><span class="line">        config = BertConfig(vocab_size=None)</span><br><span class="line">        for (key, value) in six.iteritems(json_object):</span><br><span class="line">            config.__dict__[key] = value</span><br><span class="line">        return config</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_json_file(cls, json_file):</span><br><span class="line">        with tf.gfile.GFile(json_file, &quot;r&quot;) as reader:</span><br><span class="line">            text = reader.read()</span><br><span class="line">        return cls.from_dict(json.loads(text))</span><br><span class="line"></span><br><span class="line">    def to_dict(self):</span><br><span class="line">        output = copy.deepcopy(self.__dict__)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">    def to_json_string(self):</span><br><span class="line">        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + &quot;\n&quot;</span><br></pre></td></tr></table></figure><h3 id="BertModel"><a href="#BertModel" class="headerlink" title="BertModel"></a>BertModel</h3><blockquote><blockquote><p>现在进入正题,开始分析Bert模型源码</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">class BertModel(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None,use_one_hot_embeddings=False, scope=None):</span><br><span class="line">        config = copy.deepcopy(config)</span><br><span class="line">        if not is_training:</span><br><span class="line">            config.hidden_dropout_prob = 0.0</span><br><span class="line">            config.attention_probs_dropout_prob = 0.0</span><br><span class="line">        input_shape = get_shape_list(input_ids, expected_rank=2)</span><br><span class="line">        batch_size = input_shape[0]</span><br><span class="line">        seq_length = input_shape[1]</span><br><span class="line">        if input_mask is None:</span><br><span class="line">            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">        if token_type_ids is None:</span><br><span class="line">            token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">        with tf.variable_scope(scope, default_name=&quot;bert&quot;):</span><br><span class="line">            with tf.variable_scope(&quot;embeddings&quot;):</span><br><span class="line">                (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">                    input_ids=input_ids,</span><br><span class="line">                    vocab_size=config.vocab_size,</span><br><span class="line">                    embedding_size=config.hidden_size,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">                    use_one_hot_embeddings=use_one_hot_embeddings)#调用embedding_lookup得到初始词向量</span><br><span class="line">                self.embedding_output = embedding_postprocessor(</span><br><span class="line">                    input_tensor=self.embedding_output,</span><br><span class="line">                    use_token_type=True,</span><br><span class="line">                    token_type_ids=token_type_ids,</span><br><span class="line">                    token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">                    token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">                    use_position_embeddings=True,</span><br><span class="line">                    position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">                    dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">            with tf.variable_scope(&quot;encoder&quot;):</span><br><span class="line">                attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask)</span><br><span class="line">                self.all_encoder_layers = transformer_model(</span><br><span class="line">                    input_tensor=self.embedding_output,</span><br><span class="line">                    attention_mask=attention_mask,</span><br><span class="line">                    hidden_size=config.hidden_size,</span><br><span class="line">                    num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">                    num_attention_heads=config.num_attention_heads,</span><br><span class="line">                    intermediate_size=config.intermediate_size,</span><br><span class="line">                    intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">                    hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">                    initializer_range=config.initializer_range,</span><br><span class="line">                    do_return_all_layers=True)</span><br><span class="line"></span><br><span class="line">            self.sequence_output = self.all_encoder_layers[-1]</span><br><span class="line">            with tf.variable_scope(&quot;pooler&quot;):</span><br><span class="line">                first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)</span><br><span class="line">                self.pooled_output = tf.layers.dense(</span><br><span class="line">                    first_token_tensor,</span><br><span class="line">                    config.hidden_size,</span><br><span class="line">                    activation=tf.tanh,</span><br><span class="line">                    kernel_initializer=create_initializer(config.initializer_range))</span><br><span class="line">    def get_pooled_output(self):</span><br><span class="line">        return self.pooled_output</span><br><span class="line">    def get_sequence_output(self):</span><br><span class="line">        return self.sequence_output</span><br><span class="line"></span><br><span class="line">    def get_all_encoder_layers(self):</span><br><span class="line">        return self.all_encoder_layers</span><br><span class="line"></span><br><span class="line">    def get_embedding_output(self):</span><br><span class="line">        return self.embedding_output</span><br><span class="line"></span><br><span class="line">    def get_embedding_table(self):</span><br><span class="line">        return self.embedding_table</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明:</p><ul><li><code>config</code>:一个<code>BertConfig</code>实例</li><li><code>is_training</code>:<code>bool</code>类型,是否是训练流程,用类控制是否dropout</li><li><code>input_ids</code>:输入<code>Tensor</code>, <code>shape</code>是<code>[batch_size, seq_length]</code>.</li><li><code>input_mask</code>:<code>shape</code>是<code>[batch_size, seq_length]</code>无需细讲</li><li><code>token_type_ids</code>:<code>shape</code>是<code>[batch_size, seq_length]</code>,<code>bert</code>中就是0或1</li><li><code>use_one_hot_embeddings</code>:在<code>embedding_lookup</code>返回词向量的时候使用,详细见<code>embedding_lookup</code>函数</li></ul></blockquote></blockquote><h3 id="embedding-lookup"><a href="#embedding-lookup" class="headerlink" title="embedding_lookup"></a>embedding_lookup</h3><blockquote><blockquote><p>为了得到进入模型的词向量(token embedding)</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def embedding_lookup(input_ids,vocab_size,embedding_size=128,initializer_range=0.02,word_embedding_name=&quot;word_embeddings&quot;,use_one_hot_embeddings=False):</span><br><span class="line">  if input_ids.shape.ndims == 2:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[-1])</span><br><span class="line">  embedding_table = tf.get_variable(name=word_embedding_name,shape=[vocab_size, embedding_size],initializer=create_initializer(initializer_range))</span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [-1])</span><br><span class="line">  if use_one_hot_embeddings:</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  else:</span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line">  output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size])</span><br><span class="line">  return (output, embedding_table)</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明：</p><ul><li>input_ids：[batch_size, seq_length]</li><li>vocab_size:词典大小</li><li>initializer_range：初始化参数</li><li>word_embedding_name:不解释</li><li>use_one_hot_embeddings:是否使用one_hot方式初始化(为啥我感觉这里是True还是False结果得到的结果是一样的？？？？？)如下代码.</li></ul></blockquote></blockquote><p>return:token embedding:[batch_size, seq_length, embedding_size].和embedding_table(不解释)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line">flat_input_ids = [2, 4, 5]</span><br><span class="line">embedding_table = tf.constant(value=[[1, 2, 3, 4],</span><br><span class="line">                                     [5, 6, 7, 8],</span><br><span class="line">                                     [9, 1, 2, 3],</span><br><span class="line">                                     [5, 6, 7, 8],</span><br><span class="line">                                     [6, 4, 78, 9],</span><br><span class="line">                                     [6, 8, 9, 3]],dtype=tf.float32)</span><br><span class="line">one_hot_input_ids = tf.one_hot(flat_input_ids, depth=6)</span><br><span class="line">output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">print(output)</span><br><span class="line">print(100*&apos;*&apos;)</span><br><span class="line">output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><h3 id="embedding-postprocessor"><a href="#embedding-postprocessor" class="headerlink" title="embedding_postprocessor"></a>embedding_postprocessor</h3><blockquote><blockquote><p>bert模型的输入向量有三个,embedding_lookup得到的是token embedding 我们还需要segment embedding和position embedding,这三者的维度是完全相同的(废话不相同怎么加啊。。。)本部分代码会将这三个embeddig加起来并dropout</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def embedding_postprocessor(input_tensor,</span><br><span class="line">                            use_token_type=False,</span><br><span class="line">                            token_type_ids=None,</span><br><span class="line">                            token_type_vocab_size=16,</span><br><span class="line">                            token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">                            use_position_embeddings=True,</span><br><span class="line">                            position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">                            initializer_range=0.02,</span><br><span class="line">                            max_position_embeddings=512,</span><br><span class="line">                            dropout_prob=0.1):</span><br><span class="line">    input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">    batch_size = input_shape[0]</span><br><span class="line">    seq_length = input_shape[1]</span><br><span class="line">    width = input_shape[2]</span><br><span class="line">    output = input_tensor</span><br><span class="line">    if use_token_type:</span><br><span class="line">        if token_type_ids is None:</span><br><span class="line">            raise ValueError(&quot;`token_type_ids` must be specified if&quot;&quot;`use_token_type` is True.&quot;)</span><br><span class="line">        token_type_table = tf.get_variable(name=token_type_embedding_name,</span><br><span class="line">                                           shape=[token_type_vocab_size, width],</span><br><span class="line">                                           initializer=create_initializer(initializer_range))</span><br><span class="line">        flat_token_type_ids = tf.reshape(token_type_ids, [-1])</span><br><span class="line">        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">        token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width])</span><br><span class="line">        output += token_type_embeddings</span><br><span class="line">    if use_position_embeddings:</span><br><span class="line">        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">        with tf.control_dependencies([assert_op]):</span><br><span class="line">            full_position_embeddings = tf.get_variable(name=position_embedding_name,</span><br><span class="line">                                                       shape=[max_position_embeddings, width],</span><br><span class="line">                                                       initializer=create_initializer(initializer_range))</span><br><span class="line">            position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])</span><br><span class="line">            num_dims = len(output.shape.as_list())</span><br><span class="line">            position_broadcast_shape = []</span><br><span class="line">            for _ in range(num_dims - 2):</span><br><span class="line">                position_broadcast_shape.append(1)</span><br><span class="line">            position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">            position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)</span><br><span class="line">            output += position_embeddings</span><br><span class="line">    output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">    return output</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明:</p><ul><li>input_tensor:token embedding[batch_size, seq_length, embedding_size]</li><li>use_token_type是否使用segment embedding</li><li>token_type_ids:[batch_size, seq_length],这两个参数其实就是控制生成segment embedding的,上诉代码中的<code>output += token_type_embeddings</code>就是得到token embedding+segment embedding</li><li>use_position_embeddings:是否使用位置信息</li><li>max_position_embeddings:序列最大长度<br>注:</li><li>本部分代码中的<code>width</code>其实就是词向量维度(换个<code>embedding_size</code>能死啊。。。)</li><li>可以看出位置信息跟Transform的固定方式不一样，它是训练出来的.</li><li><code>output += position_embeddings</code>就得到了三者的想加结果<br>return :token embedding+segment embedding+position_embeddings</li></ul></blockquote></blockquote><h3 id="create-attention-mask-from-input-mask"><a href="#create-attention-mask-from-input-mask" class="headerlink" title="create_attention_mask_from_input_mask"></a>create_attention_mask_from_input_mask</h3><blockquote><blockquote><p>目的是将本来shape为[batch_size, seq_length]转为[batch_size, seq_length,seq_length],为什么要这样的维度呢?因为…..算了麻烦不写了，去我的另一篇<a href="https://lingyixia.github.io/2019/04/05/transformer/">Transform</a>中看吧</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def create_attention_mask_from_input_mask(from_tensor, to_mask):</span><br><span class="line">    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])</span><br><span class="line">    batch_size = from_shape[0]</span><br><span class="line">    from_seq_length = from_shape[1]</span><br><span class="line">    to_shape = get_shape_list(to_mask, expected_rank=2)</span><br><span class="line">    to_seq_length = to_shape[1]</span><br><span class="line">    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)</span><br><span class="line">    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)</span><br><span class="line">    mask = broadcast_ones * to_mask</span><br><span class="line">    return mask</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明:</p><ul><li>from_tensor:[batch_size, seq_length].</li><li>to_mask:[batch_size, seq_length]<br>注:<code>Transform</code>中的<code>mask</code>和平常用的不太一样,这里的<code>mask</code>是为了在计算<code>attention</code>的时候”看不到不应该看到的内容”,计算方式为该看到的<code>mask</code>为0，不该看到的<code>mask</code>为一个负的很大的数字,然后两者相加(平常使用<code>mask</code>是看到的为1，看不到的为0，然后两者做点乘)，这样在计算<code>softmax</code>的时候那些负数的<code>attention</code>会非常非常小,也就基本看不到了.</li></ul></blockquote></blockquote><h3 id="transformer-model"><a href="#transformer-model" class="headerlink" title="transformer_model"></a>transformer_model</h3><blockquote><blockquote><p>这一部分是<code>Transform</code>部分,但是只有<code>encoder</code>部分,从<code>BertModel</code>中的<code>with tf.variable_scope(&quot;encoder&quot;):</code>这一部分也可以看出来</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">def transformer_model(input_tensor,</span><br><span class="line">                      attention_mask=None,</span><br><span class="line">                      hidden_size=768,</span><br><span class="line">                      num_hidden_layers=12,</span><br><span class="line">                      num_attention_heads=12,</span><br><span class="line">                      intermediate_size=3072,</span><br><span class="line">                      intermediate_act_fn=gelu,</span><br><span class="line">                      hidden_dropout_prob=0.1,</span><br><span class="line">                      attention_probs_dropout_prob=0.1,</span><br><span class="line">                      initializer_range=0.02,</span><br><span class="line">                      do_return_all_layers=False):</span><br><span class="line">    if hidden_size % num_attention_heads != 0:</span><br><span class="line">        raise ValueError(</span><br><span class="line">            &quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span><br><span class="line">            &quot;heads (%d)&quot; % (hidden_size, num_attention_heads))</span><br><span class="line">    attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">    input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">    batch_size = input_shape[0]</span><br><span class="line">    seq_length = input_shape[1]</span><br><span class="line">    input_width = input_shape[2]</span><br><span class="line">    if input_width != hidden_size:</span><br><span class="line">        raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; %</span><br><span class="line">                         (input_width, hidden_size))</span><br><span class="line">    prev_output = reshape_to_matrix(input_tensor)#这个不单独写了,就是将[batch_size, seq_length, embedding_size]的input 给reahpe为[batch_size*seq_length,embedding_size]</span><br><span class="line">    all_layer_outputs = []</span><br><span class="line">    for layer_idx in range(num_hidden_layers):</span><br><span class="line">        with tf.variable_scope(&quot;layer_%d&quot; % layer_idx):</span><br><span class="line">            layer_input = prev_output</span><br><span class="line">            with tf.variable_scope(&quot;attention&quot;):</span><br><span class="line">                attention_heads = []</span><br><span class="line">                with tf.variable_scope(&quot;self&quot;):</span><br><span class="line">                    attention_head = attention_layer(from_tensor=layer_input,</span><br><span class="line">                                                     to_tensor=layer_input,</span><br><span class="line">                                                     attention_mask=attention_mask,</span><br><span class="line">                                                     num_attention_heads=num_attention_heads,</span><br><span class="line">                                                     size_per_head=attention_head_size,</span><br><span class="line">                                                     attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">                                                     initializer_range=initializer_range,</span><br><span class="line">                                                     do_return_2d_tensor=True,</span><br><span class="line">                                                     batch_size=batch_size,</span><br><span class="line">                                                     from_seq_length=seq_length,</span><br><span class="line">                                                     to_seq_length=seq_length)</span><br><span class="line">                    attention_heads.append(attention_head)</span><br><span class="line">                attention_output = None</span><br><span class="line">                if len(attention_heads) == 1:</span><br><span class="line">                    attention_output = attention_heads[0]</span><br><span class="line">                else:</span><br><span class="line">                    attention_output = tf.concat(attention_heads, axis=-1)</span><br><span class="line">                with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">                    attention_output = tf.layers.dense(attention_output,</span><br><span class="line">                                                       hidden_size,</span><br><span class="line">                                                       kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">                    attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">                    attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line">            with tf.variable_scope(&quot;intermediate&quot;):#feed-forword部分</span><br><span class="line">                intermediate_output = tf.layers.dense(attention_output,</span><br><span class="line">                                                      intermediate_size,</span><br><span class="line">                                                      activation=intermediate_act_fn,</span><br><span class="line">                                                      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">            with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">                layer_output = tf.layers.dense(intermediate_output,</span><br><span class="line">                                               hidden_size,</span><br><span class="line">                                               kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">                layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">                layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">                prev_output = layer_output</span><br><span class="line">                all_layer_outputs.append(layer_output)</span><br><span class="line">    if do_return_all_layers:</span><br><span class="line">        final_outputs = []</span><br><span class="line">        for layer_output in all_layer_outputs:</span><br><span class="line">            final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">            final_outputs.append(final_output)</span><br><span class="line">        return final_outputs</span><br><span class="line">    else:</span><br><span class="line">        final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">        return final_output</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明:</p><ul><li>input_tensor:token embedding+segment embedding+position embedding [batch_size, seq_length, embedding_size]</li><li>attention_mask:[batch_size, seq_length,seq_length]</li><li>hidden_size:不解释</li><li>num_hidden_layers:多少个<code>ecncoder block</code></li><li>num_attention_heads:多少个<code>head</code></li><li>intermediate_size:<code>feed forward</code>隐藏层维度</li><li>intermediate_act_fn:<code>feed forward</code>激活函数<br>其他的不解释了<br>return [batch_size, seq_length, hidden_size],</li></ul></blockquote></blockquote><h3 id="attention-layer"><a href="#attention-layer" class="headerlink" title="attention_layer"></a>attention_layer</h3><blockquote><blockquote><p>其实就是<code>self-attention</code>,但是在计算的时候全都转换为了二维矩阵，按注释的意思是避免反复reshape,因为reshape在CPU/GPU上易于实现，但是在TPU上不易实现,这样可以加速训练.</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">def attention_layer(from_tensor,</span><br><span class="line">                    to_tensor,</span><br><span class="line">                    attention_mask=None,</span><br><span class="line">                    num_attention_heads=1,</span><br><span class="line">                    size_per_head=512,</span><br><span class="line">                    query_act=None,</span><br><span class="line">                    key_act=None,</span><br><span class="line">                    value_act=None,</span><br><span class="line">                    attention_probs_dropout_prob=0.0,</span><br><span class="line">                    initializer_range=0.02,</span><br><span class="line">                    do_return_2d_tensor=False,</span><br><span class="line">                    batch_size=None,</span><br><span class="line">                    from_seq_length=None,</span><br><span class="line">                    to_seq_length=None):</span><br><span class="line">    def transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, width):</span><br><span class="line">        output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line">        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])</span><br><span class="line">        return output_tensor</span><br><span class="line"></span><br><span class="line">    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])</span><br><span class="line">    to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])</span><br><span class="line">    if len(from_shape) != len(to_shape):</span><br><span class="line">        raise ValueError(&quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;)</span><br><span class="line">    if len(from_shape) == 3:</span><br><span class="line">        batch_size = from_shape[0]</span><br><span class="line">        from_seq_length = from_shape[1]</span><br><span class="line">        to_seq_length = to_shape[1]</span><br><span class="line">    elif len(from_shape) == 2:</span><br><span class="line">        if (batch_size is None or from_seq_length is None or to_seq_length is None):</span><br><span class="line">            raise ValueError(</span><br><span class="line">                &quot;When passing in rank 2 tensors to attention_layer, the values &quot;</span><br><span class="line">                &quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot;</span><br><span class="line">                &quot;must all be specified.&quot;)</span><br><span class="line">    from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">    to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line">    query_layer = tf.layers.dense(from_tensor_2d,</span><br><span class="line">                                  num_attention_heads * size_per_head,</span><br><span class="line">                                  activation=query_act,</span><br><span class="line">                                  name=&quot;query&quot;,</span><br><span class="line">                                  kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    key_layer = tf.layers.dense(to_tensor_2d,</span><br><span class="line">                                num_attention_heads * size_per_head,</span><br><span class="line">                                activation=key_act,</span><br><span class="line">                                name=&quot;key&quot;,</span><br><span class="line">                                kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    value_layer = tf.layers.dense(to_tensor_2d,</span><br><span class="line">                                  num_attention_heads * size_per_head,</span><br><span class="line">                                  activation=value_act,</span><br><span class="line">                                  name=&quot;value&quot;,</span><br><span class="line">                                  kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads, from_seq_length, size_per_head)</span><br><span class="line">    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, to_seq_length, size_per_head)</span><br><span class="line">    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)</span><br><span class="line">    attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head)))</span><br><span class="line">    if attention_mask is not None:</span><br><span class="line">        attention_mask = tf.expand_dims(attention_mask, axis=[1])</span><br><span class="line">        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0</span><br><span class="line">        attention_scores += adder#这里就是使用mask来attention该attention的部分</span><br><span class="line">    attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line">    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line">    value_layer = tf.reshape(value_layer, [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line">    value_layer = tf.transpose(value_layer, [0, 2, 1, 3])</span><br><span class="line">    context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line">    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])</span><br><span class="line">    if do_return_2d_tensor:</span><br><span class="line">        context_layer = tf.reshape(context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">    else:</span><br><span class="line">        context_layer = tf.reshape(context_layer, [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">    return context_layer</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>参数说明:</p><ul><li>from_tensor在Transform中被转为二维[batch_size*seq_length, embedding_size]</li><li>to_shape:传过来的参数跟from_tensor一毛一样,在这里没什么卵用其实,因为q和k的length是一样的</li><li>attention_mask:[batch_size, seq_length,seq_length]</li><li>num_attention_heads:head数量</li><li>size_per_head:每一个head维度,代码中是用总维度除以head数量得到的:attention_head_size = int(hidden_size / num_attention_heads)<br>return: return :[batch_size, from_seq_length,num_attention_heads * size_per_head].</li></ul></blockquote></blockquote><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def gelu(x):</span><br><span class="line">  cdf = 0.5 * (1.0 + tf.tanh(</span><br><span class="line">      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))</span><br><span class="line">  return x * cdf</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>这个激活函数很有特色，其实这个公式就是$x \times \Phi(x)$,后一项是正态函数,也就是说,gelu中后面那一大堆其实近似等于$\int_{-\infty}^{x}\frac{1}{\sqrt(2\pi)}e^{-\frac{x^2}{2}}dx$,至于咋来的这个近似值，还不清楚。<br>测试函数:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from scipy import stats</span><br><span class="line">import math</span><br><span class="line">a = stats.norm.cdf(2, 0, 1)</span><br><span class="line"></span><br><span class="line">def gelu(x):</span><br><span class="line">    return 0.5 * (1.0 + math.tanh((math.sqrt(2 / math.pi) * (x + 0.044715 * math.pow(x, 3)))))</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line">print(gelu(2))</span><br><span class="line">#结果:</span><br><span class="line">#0.9772498680518208</span><br><span class="line">#0.9772988470438875</span><br></pre></td></tr></table></figure></p></blockquote></blockquote><h3 id="总结一"><a href="#总结一" class="headerlink" title="总结一:"></a>总结一:</h3><p>看完模型感觉真特么简单这模型,似乎除了self-attention就啥都没有了,但是先别着急,一般情况下模型是重点，但是对于Bert而言，模型却仅仅是开始，真正的创新点还在下面.</p><h2 id="create-pretraining-data-py"><a href="#create-pretraining-data-py" class="headerlink" title="create_pretraining_data.py"></a>create_pretraining_data.py</h2><blockquote><blockquote><p>这部分代码用来生成训练样本,我们从<code>main</code>函数开始看起,首先进入<code>tokenization.py</code></p></blockquote></blockquote><h3 id="def-main"><a href="#def-main" class="headerlink" title="def main"></a>def main</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def main(_):</span><br><span class="line">    tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line">    tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)</span><br><span class="line">    input_files = []</span><br><span class="line">    for input_pattern in FLAGS.input_file.split(&quot;,&quot;):</span><br><span class="line">        input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line">    tf.logging.info(&quot;*** Reading from input files ***&quot;)</span><br><span class="line">    for input_file in input_files:</span><br><span class="line">        tf.logging.info(&quot;  %s&quot;, input_file)</span><br><span class="line">    rng = random.Random(FLAGS.random_seed)</span><br><span class="line">    instances = create_training_instances(input_files,</span><br><span class="line">                                          tokenizer,</span><br><span class="line">                                          FLAGS.max_seq_length,</span><br><span class="line">                                          FLAGS.dupe_factor,</span><br><span class="line">                                          FLAGS.short_seq_prob,</span><br><span class="line">                                          FLAGS.masked_lm_prob,</span><br><span class="line">                                          FLAGS.max_predictions_per_seq,</span><br><span class="line">                                          rng)</span><br><span class="line"></span><br><span class="line">    output_files = FLAGS.output_file.split(&quot;,&quot;)</span><br><span class="line">    tf.logging.info(&quot;*** Writing to output files ***&quot;)</span><br><span class="line">    for output_file in output_files:</span><br><span class="line">        tf.logging.info(&quot;  %s&quot;, output_file)</span><br><span class="line">    write_instance_to_example_files(instances,</span><br><span class="line">                                    tokenizer, </span><br><span class="line">                                    FLAGS.max_seq_length,</span><br><span class="line">                                    FLAGS.max_predictions_per_seq, </span><br><span class="line">                                    output_files)</span><br></pre></td></tr></table></figure><h3 id="class-TrainingInstance"><a href="#class-TrainingInstance" class="headerlink" title="class TrainingInstance"></a>class TrainingInstance</h3><blockquote><blockquote><p>单个训练样本类,看<code>__init__</code>就能看出来，没什么其他东西</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class TrainingInstance(object):</span><br><span class="line">    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,</span><br><span class="line">                 is_random_next):</span><br><span class="line">        self.tokens = tokens</span><br><span class="line">        self.segment_ids = segment_ids</span><br><span class="line">        self.is_random_next = is_random_next</span><br><span class="line">        self.masked_lm_positions = masked_lm_positions</span><br><span class="line">        self.masked_lm_labels = masked_lm_labels</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        s = &quot;&quot;</span><br><span class="line">        s += &quot;tokens: %s\n&quot; % (&quot; &quot;.join(</span><br><span class="line">            [tokenization.printable_text(x) for x in self.tokens]))</span><br><span class="line">        s += &quot;segment_ids: %s\n&quot; % (&quot; &quot;.join([str(x) for x in self.segment_ids]))</span><br><span class="line">        s += &quot;is_random_next: %s\n&quot; % self.is_random_next</span><br><span class="line">        s += &quot;masked_lm_positions: %s\n&quot; % (&quot; &quot;.join(</span><br><span class="line">            [str(x) for x in self.masked_lm_positions]))</span><br><span class="line">        s += &quot;masked_lm_labels: %s\n&quot; % (&quot; &quot;.join(</span><br><span class="line">            [tokenization.printable_text(x) for x in self.masked_lm_labels]))</span><br><span class="line">        s += &quot;\n&quot;</span><br><span class="line">        return s</span><br></pre></td></tr></table></figure><h3 id="def-create-training-instances"><a href="#def-create-training-instances" class="headerlink" title="def create_training_instances"></a>def create_training_instances</h3><blockquote><blockquote><p>这个函数是重中之重，用来生成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def create_training_instances(input_files, tokenizer, max_seq_length,</span><br><span class="line">                              dupe_factor, short_seq_prob, masked_lm_prob,</span><br><span class="line">                              max_predictions_per_seq, rng):</span><br><span class="line">    all_documents = [[]]#外层是文档，内层是文档中的每个句子</span><br><span class="line">    for input_file in input_files:</span><br><span class="line">        with tf.gfile.GFile(input_file, &quot;r&quot;) as reader:</span><br><span class="line">            while True:</span><br><span class="line">                line = tokenization.convert_to_unicode(reader.readline())</span><br><span class="line">                if not line:</span><br><span class="line">                    break</span><br><span class="line">                line = line.strip()</span><br><span class="line">                if not line:# 空行表示文档分割</span><br><span class="line">                    all_documents.append([])</span><br><span class="line">                tokens = tokenizer.tokenize(line)</span><br><span class="line">                if tokens:</span><br><span class="line">                    all_documents[-1].append(tokens)</span><br><span class="line">    all_documents = [x for x in all_documents if x]</span><br><span class="line">    rng.shuffle(all_documents)</span><br><span class="line">    vocab_words = list(tokenizer.vocab.keys())</span><br><span class="line">    instances = []</span><br><span class="line">    for _ in range(dupe_factor):</span><br><span class="line">        for document_index in range(len(all_documents)):</span><br><span class="line">            instances.extend(</span><br><span class="line">                create_instances_from_document(all_documents,</span><br><span class="line">                                               document_index,</span><br><span class="line">                                               max_seq_length,</span><br><span class="line">                                               short_seq_prob,</span><br><span class="line">                                               masked_lm_prob,</span><br><span class="line">                                               max_predictions_per_seq,</span><br><span class="line">                                               vocab_words,</span><br><span class="line">                                               rng))</span><br><span class="line">    rng.shuffle(instances)</span><br><span class="line">    return instances</span><br></pre></td></tr></table></figure></p><p>参数说明:<br>dupe_factor:每一个句子用几次:因为如果一个句子只用一次的话那么mask的位置就是固定的，这样我们把每个句子在训练中都多用几次,而且没次的mask位置都不相同,就可以防止某些词永远看不到<br>short_seq_prob:长度小于“max_seq_length”的样本比例。因为在fine-tune过程里面输入的target_seq_length是可变的（小于等于max_seq_length），那么为了防止过拟合也需要在pre-train的过程当中构造一些短的样本<br>max_predictions_per_seq:一个句子里最多有多少个[MASK]标记<br>masked_lm_prob:多少比例的Token被MASK掉<br>rng:随机率</p></blockquote></blockquote><h3 id="def-create-instances-from-document"><a href="#def-create-instances-from-document" class="headerlink" title="def create_instances_from_document"></a>def create_instances_from_document</h3><blockquote><blockquote><p>一个文档中抽取训练样本,重中之重</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">def create_instances_from_document(all_documents, document_index, max_seq_length, short_seq_prob,</span><br><span class="line">                                   masked_lm_prob, max_predictions_per_seq, vocab_words, rng):</span><br><span class="line">    document = all_documents[document_index]</span><br><span class="line">    # 为[CLS], [SEP], [SEP]预留三个空位</span><br><span class="line">    max_num_tokens = max_seq_length - 3</span><br><span class="line">    target_seq_length = max_num_tokens  # 以short_seq_prob的概率随机生成（2~max_num_tokens）的长度</span><br><span class="line">    if rng.random() &lt; short_seq_prob:</span><br><span class="line">        target_seq_length = rng.randint(2, max_num_tokens)</span><br><span class="line">    instances = []</span><br><span class="line">    current_chunk = []</span><br><span class="line">    current_length = 0</span><br><span class="line">    i = 0</span><br><span class="line">    while i &lt; len(document):</span><br><span class="line">        segment = document[i]</span><br><span class="line">        current_chunk.append(segment)</span><br><span class="line">        current_length += len(segment)</span><br><span class="line">        # 将句子依次加入current_chunk中，直到加完或者达到限制的最大长度</span><br><span class="line">        if i == len(document) - 1 or current_length &gt;= target_seq_length:</span><br><span class="line">            if current_chunk:</span><br><span class="line">                # `a_end`是第一个句子A结束的下标</span><br><span class="line">                a_end = 1</span><br><span class="line">                if len(current_chunk) &gt;= 2:</span><br><span class="line">                    a_end = rng.randint(1, len(current_chunk) - 1)</span><br><span class="line">                tokens_a = []</span><br><span class="line">                for j in range(a_end):</span><br><span class="line">                    tokens_a.extend(current_chunk[j])</span><br><span class="line">                tokens_b = []</span><br><span class="line">                is_random_next = False</span><br><span class="line">                # `a_end`是第一个句子A结束的下标</span><br><span class="line">                if len(current_chunk) == 1 or rng.random() &lt; 0.5:</span><br><span class="line">                    is_random_next = True</span><br><span class="line">                    target_b_length = target_seq_length - len(tokens_a)</span><br><span class="line">                    # 随机的挑选另外一篇文档的随机开始的句子</span><br><span class="line">                    # 但是理论上有可能随机到的文档就是当前文档，因此需要一个while循环</span><br><span class="line">                    # 这里只while循环10次，理论上还是有重复的可能性，但是我们忽略</span><br><span class="line">                    for _ in range(10):</span><br><span class="line">                        random_document_index = rng.randint(0, len(all_documents) - 1)</span><br><span class="line">                        if random_document_index != document_index:</span><br><span class="line">                            break</span><br><span class="line">                    random_document = all_documents[random_document_index]</span><br><span class="line">                    random_start = rng.randint(0, len(random_document) - 1)</span><br><span class="line">                    for j in range(random_start, len(random_document)):</span><br><span class="line">                        tokens_b.extend(random_document[j])</span><br><span class="line">                        if len(tokens_b) &gt;= target_b_length:</span><br><span class="line">                            break</span><br><span class="line">                    # 对于上述构建的随机下一句，我们并没有真正地使用它们</span><br><span class="line">                    # 所以为了避免数据浪费，我们将其“放回”</span><br><span class="line">                    num_unused_segments = len(current_chunk) - a_end</span><br><span class="line">                    i -= num_unused_segments</span><br><span class="line">                else:</span><br><span class="line">                    is_random_next = False</span><br><span class="line">                    for j in range(a_end, len(current_chunk)):</span><br><span class="line">                        tokens_b.extend(current_chunk[j])</span><br><span class="line">                # 如果太多了，随机去掉一些</span><br><span class="line">                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)</span><br><span class="line">                assert len(tokens_a) &gt;= 1</span><br><span class="line">                assert len(tokens_b) &gt;= 1</span><br><span class="line">                tokens = []</span><br><span class="line">                segment_ids = []</span><br><span class="line">                # 处理句子A</span><br><span class="line">                tokens.append(&quot;[CLS]&quot;)</span><br><span class="line">                segment_ids.append(0)</span><br><span class="line">                for token in tokens_a:</span><br><span class="line">                    tokens.append(token)</span><br><span class="line">                    segment_ids.append(0)</span><br><span class="line">                # 句子A结束，加上【SEP】</span><br><span class="line">                tokens.append(&quot;[SEP]&quot;)</span><br><span class="line">                segment_ids.append(0)</span><br><span class="line">                # 处理句子B</span><br><span class="line">                for token in tokens_b:</span><br><span class="line">                    tokens.append(token)</span><br><span class="line">                    segment_ids.append(1)</span><br><span class="line">                # 句子B结束，加上【SEP】</span><br><span class="line">                tokens.append(&quot;[SEP]&quot;)</span><br><span class="line">                segment_ids.append(1)</span><br><span class="line">                # 调用 create_masked_lm_predictions来随机对某些Token进行mask</span><br><span class="line">                (tokens, masked_lm_positions,</span><br><span class="line">                 masked_lm_labels) = create_masked_lm_predictions(tokens,</span><br><span class="line">                                                                  masked_lm_prob,</span><br><span class="line">                                                                  max_predictions_per_seq,</span><br><span class="line">                                                                  vocab_words, rng)</span><br><span class="line">                instance = TrainingInstance(tokens=tokens,</span><br><span class="line">                                            segment_ids=segment_ids,</span><br><span class="line">                                            is_random_next=is_random_next,</span><br><span class="line">                                            masked_lm_positions=masked_lm_positions,</span><br><span class="line">                                            masked_lm_labels=masked_lm_labels)</span><br><span class="line">                instances.append(instance)</span><br><span class="line">            current_chunk = []</span><br><span class="line">            current_length = 0</span><br><span class="line">        i += 1</span><br><span class="line">    return instances</span><br></pre></td></tr></table></figure><h3 id="def-create-masked-lm-predictions"><a href="#def-create-masked-lm-predictions" class="headerlink" title="def create_masked_lm_predictions"></a>def create_masked_lm_predictions</h3><blockquote><blockquote><p>真正的mask在这里实现</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng):</span><br><span class="line">    cand_indexes = [] # [CLS]和[SEP]不能用于MASK</span><br><span class="line">    for (i, token) in enumerate(tokens):</span><br><span class="line">        if token == &quot;[CLS]&quot; or token == &quot;[SEP]&quot;:</span><br><span class="line">            continue</span><br><span class="line">        if (FLAGS.do_whole_word_mask and len(cand_indexes) &gt;= 1 and</span><br><span class="line">                token.startswith(&quot;##&quot;)):</span><br><span class="line">            cand_indexes[-1].append(i)</span><br><span class="line">        else:</span><br><span class="line">            cand_indexes.append([i])</span><br><span class="line">    rng.shuffle(cand_indexes)</span><br><span class="line">    output_tokens = list(tokens)</span><br><span class="line">    num_to_predict = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob))))</span><br><span class="line">    masked_lms = []</span><br><span class="line">    covered_indexes = set()</span><br><span class="line">    for index_set in cand_indexes:</span><br><span class="line">        if len(masked_lms) &gt;= num_to_predict:</span><br><span class="line">            break</span><br><span class="line">        if len(masked_lms) + len(index_set) &gt; num_to_predict:</span><br><span class="line">            continue</span><br><span class="line">        is_any_index_covered = False</span><br><span class="line">        for index in index_set:</span><br><span class="line">            if index in covered_indexes:</span><br><span class="line">                is_any_index_covered = True</span><br><span class="line">                break</span><br><span class="line">        if is_any_index_covered:</span><br><span class="line">            continue</span><br><span class="line">        for index in index_set:</span><br><span class="line">            covered_indexes.add(index)</span><br><span class="line">            masked_token = None</span><br><span class="line">            # 80% of the time, replace with [MASK]</span><br><span class="line">            if rng.random() &lt; 0.8:</span><br><span class="line">                masked_token = &quot;[MASK]&quot;</span><br><span class="line">            else:</span><br><span class="line">            # 10% of the time, keep original</span><br><span class="line">                if rng.random() &lt; 0.5:</span><br><span class="line">                    masked_token = tokens[index]</span><br><span class="line">            # 10% of the time, replace with random word</span><br><span class="line">                else:</span><br><span class="line">                    masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]</span><br><span class="line">            output_tokens[index] = masked_token</span><br><span class="line">            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))</span><br><span class="line">    assert len(masked_lms) &lt;= num_to_predict</span><br><span class="line">     # 按照下标重排，保证是原来句子中出现的顺序</span><br><span class="line">    masked_lms = sorted(masked_lms, key=lambda x: x.index)</span><br><span class="line">    masked_lm_positions = []</span><br><span class="line">    masked_lm_labels = []</span><br><span class="line">    for p in masked_lms:</span><br><span class="line">        masked_lm_positions.append(p.index)</span><br><span class="line">        masked_lm_labels.append(p.label)</span><br><span class="line">    return (output_tokens, masked_lm_positions, masked_lm_labels)</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>代码流程是这样的:首先嫁给你一个句子随机打乱,并确定一个句子的15%是多少个token，设num_to_predict,然后对于[0,是多少个token，设num_to_predict]token，以80%的概率替换为[mask],10%的概率替换，10%的概率保持,这样就做到了对于15%的toke80% [mask],10%替换,10%保持。而预测的不是那15%的80（标注问题），而是全部15%。为什么要mask呢？你想啊，我们的目的是得到这样一个模型:输入一个句子，输出一个能够尽可能表示该句子的向量(用最容易理解的语言就是我们不知道输入的是什么玩意，但是我们需要知道输出的向量是什么),如果不mask直接训练那不就相当于用1来推导1？而如果我们mask一部分就意味着并不知道输入(至少不知道全部),至于为什么要把15不全部mask，我觉得这个解释很不错，但是过于专业化:</p><ul><li>如果把 100% 的输入替换为 [MASK]：模型会偏向为 [MASK] 输入建模，而不会学习到 non-masked 输入的表征。</li><li>如果把 90% 的输入替换为 [MASK]、10% 的输入替换为随机 token：模型会偏向认为 non-masked 输入是错的。</li><li>如果把 90% 的输入替换为 [MASK]、维持 10% 的输入不变：模型会偏向直接复制 non-masked 输入的上下文无关表征。<br>所以，为了使模型可以学习到相对有效的上下文相关表征，需要以 1:1 的比例使用两种策略处理 non-masked 输入。论文提及，随机替换的输入只占整体的 1.5%，似乎不会对最终效果有影响（模型有足够的容错余量）。<br>通俗点说就是全部mask的话就意味着用mask来预测真正的单词,学习的仅仅是mask(而且mask的每个词都不一样，学到的mask表示也不一样，很显然不合理)，加入10%的替换就意味着用错的词预测对的词，而10%保持不变意味着用1来推导1，因此后两个10%的作用其实是为了学到没有mask的部分。<br>或者还有一种解释方式: 因为每次都是要学习这15%的token，其他的学不到(认识到这一点很重要)倘若某一个词在训练模型的时候被mask了，而微调的时候出现了咋办？因此不管怎样，都必须让模型好歹”认识一下”这个词.<h2 id="tokenization-py"><a href="#tokenization-py" class="headerlink" title="tokenization.py"></a>tokenization.py</h2>按照<code>create_pretraining_data.py</code>中<code>main</code>的调用顺序，先看<code>FullTokenizer</code>类</li></ul></blockquote></blockquote><h3 id="FullTokenizer"><a href="#FullTokenizer" class="headerlink" title="FullTokenizer"></a>FullTokenizer</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class FullTokenizer(object):</span><br><span class="line">    def __init__(self, vocab_file, do_lower_case=True):</span><br><span class="line">        self.vocab = load_vocab(vocab_file)</span><br><span class="line">        self.inv_vocab = &#123;v: k for k, v in self.vocab.items()&#125;</span><br><span class="line">        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)</span><br><span class="line">        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)</span><br><span class="line"></span><br><span class="line">    def tokenize(self, text):</span><br><span class="line">        split_tokens = []</span><br><span class="line">        for token in self.basic_tokenizer.tokenize(text):</span><br><span class="line">            for sub_token in self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">                split_tokens.append(sub_token)</span><br><span class="line">        return split_tokens</span><br><span class="line"></span><br><span class="line">    def convert_tokens_to_ids(self, tokens):</span><br><span class="line">        return convert_by_vocab(self.vocab, tokens)</span><br><span class="line"></span><br><span class="line">    def convert_ids_to_tokens(self, ids):</span><br><span class="line">        return convert_by_vocab(self.inv_vocab, ids)</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>在<code>__init__</code>中可以看到，又得先分析<code>BasicTokenizer</code>类和<code>WordpieceTokenizer</code>类(哎呀真烦，最后在回来做超链接吧),除此之外就是调用了几个小函数,<code>load_vocab</code>它的输入参数是bert模型的词典,返回的是一个<code>OrdereDict</code>:{词:词号}.其他的不说了，没啥意思。</p></blockquote></blockquote><h3 id="class-BasicTokenizer"><a href="#class-BasicTokenizer" class="headerlink" title="class BasicTokenizer"></a>class BasicTokenizer</h3><blockquote><blockquote><p>目的是根据空格，标点进行普通的分词，最后返回的是关于词的列表，对于中文而言是关于字的列表。</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">class BasicTokenizer(object):</span><br><span class="line">  def __init__(self, do_lower_case=True):</span><br><span class="line">    self.do_lower_case = do_lower_case</span><br><span class="line"></span><br><span class="line">  def tokenize(self, text):</span><br><span class="line">  ##其实就是把字符串转为了list，分英文单词和中文单词处理</span><br><span class="line">  ##eg:Mr. Cassius crossed the highway, and stopped suddenly.转为[&apos;mr&apos;, &apos;.&apos;, &apos;cassius&apos;, &apos;crossed&apos;, &apos;the&apos;, &apos;highway&apos;, &apos;,&apos;, &apos;and&apos;, &apos;stopped&apos;, &apos;suddenly&apos;, &apos;.&apos;]</span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    text = self._clean_text(text)</span><br><span class="line">    text = self._tokenize_chinese_chars(text)</span><br><span class="line">    orig_tokens = whitespace_tokenize(text)#无需细说，就是把string按照空格切分为list</span><br><span class="line">    split_tokens = []</span><br><span class="line">    for token in orig_tokens:</span><br><span class="line">      if self.do_lower_case:</span><br><span class="line">        token = token.lower()</span><br><span class="line">        token = self._run_strip_accents(token)#这个函数干了什么我也没看明白,但是对正题流程不重要,略过吧</span><br><span class="line">      split_tokens.extend(self._run_split_on_punc(token))</span><br><span class="line">    output_tokens = whitespace_tokenize(&quot; &quot;.join(split_tokens))</span><br><span class="line">    return output_tokens</span><br><span class="line"></span><br><span class="line">  def _run_strip_accents(self, text):</span><br><span class="line">    text = unicodedata.normalize(&quot;NFD&quot;, text)</span><br><span class="line">    output = []</span><br><span class="line">    for char in text:</span><br><span class="line">      cat = unicodedata.category(char)</span><br><span class="line">      if cat == &quot;Mn&quot;:</span><br><span class="line">        continue</span><br><span class="line">      output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br><span class="line"></span><br><span class="line">  def _run_split_on_punc(self, text):</span><br><span class="line">    chars = list(text)</span><br><span class="line">    i = 0</span><br><span class="line">    start_new_word = True</span><br><span class="line">    output = []</span><br><span class="line">    while i &lt; len(chars):</span><br><span class="line">      char = chars[i]</span><br><span class="line">      if _is_punctuation(char):</span><br><span class="line">        output.append([char])</span><br><span class="line">        start_new_word = True</span><br><span class="line">      else:</span><br><span class="line">        if start_new_word:</span><br><span class="line">          output.append([])</span><br><span class="line">        start_new_word = False</span><br><span class="line">        output[-1].append(char)</span><br><span class="line">      i += 1</span><br><span class="line">    return [&quot;&quot;.join(x) for x in output]</span><br><span class="line"></span><br><span class="line">  def _tokenize_chinese_chars(self, text):</span><br><span class="line">    # 按字切分中文，其实就是英文单词不变,中文在字两侧添加空格</span><br><span class="line">    output = []</span><br><span class="line">    for char in text:</span><br><span class="line">      cp = ord(char)</span><br><span class="line">      if self._is_chinese_char(cp):</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">        output.append(char)</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">      else:</span><br><span class="line">        output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br><span class="line"></span><br><span class="line">  def _is_chinese_char(self, cp):</span><br><span class="line">    # 判断是否是汉字,这个函数很有意义，值得借鉴</span><br><span class="line">    # refer：https://www.cnblogs.com/straybirds/p/6392306.html</span><br><span class="line">    if ((cp &gt;= 0x4E00 and cp &lt;= 0x9FFF) or  #</span><br><span class="line">        (cp &gt;= 0x3400 and cp &lt;= 0x4DBF) or  #</span><br><span class="line">        (cp &gt;= 0x20000 and cp &lt;= 0x2A6DF) or  #</span><br><span class="line">        (cp &gt;= 0x2A700 and cp &lt;= 0x2B73F) or  #</span><br><span class="line">        (cp &gt;= 0x2B740 and cp &lt;= 0x2B81F) or  #</span><br><span class="line">        (cp &gt;= 0x2B820 and cp &lt;= 0x2CEAF) or</span><br><span class="line">        (cp &gt;= 0xF900 and cp &lt;= 0xFAFF) or  #</span><br><span class="line">        (cp &gt;= 0x2F800 and cp &lt;= 0x2FA1F)):  #</span><br><span class="line">      return True</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">  def _clean_text(self, text): # 去除无意义字符以及空格</span><br><span class="line">    output = []</span><br><span class="line">    for char in text:</span><br><span class="line">      cp = ord(char)</span><br><span class="line">      if cp == 0 or cp == 0xfffd or _is_control(char):</span><br><span class="line">        continue</span><br><span class="line">      if _is_whitespace(char):</span><br><span class="line">        output.append(&quot; &quot;)</span><br><span class="line">      else:</span><br><span class="line">        output.append(char)</span><br><span class="line">    return &quot;&quot;.join(output)</span><br></pre></td></tr></table></figure><h3 id="class-WordpieceTokenizer"><a href="#class-WordpieceTokenizer" class="headerlink" title="class WordpieceTokenizer"></a>class WordpieceTokenizer</h3><blockquote><blockquote><p>这个才是重点,跑test的时候出现的那些##都是从这里拿来的，其实就是把未登录词在词表中匹配相应的前缀.</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">class WordpieceTokenizer(object):</span><br><span class="line">  def __init__(self, vocab, unk_token=&quot;[UNK]&quot;, max_input_chars_per_word=200):</span><br><span class="line">    self.vocab = vocab</span><br><span class="line">    self.unk_token = unk_token</span><br><span class="line">    self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">  def tokenize(self, text):</span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    output_tokens = []</span><br><span class="line">    for token in whitespace_tokenize(text):</span><br><span class="line">      chars = list(token)</span><br><span class="line">      if len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">        continue</span><br><span class="line">      is_bad = False</span><br><span class="line">      start = 0</span><br><span class="line">      sub_tokens = []</span><br><span class="line">      while start &lt; len(chars):</span><br><span class="line">        end = len(chars)</span><br><span class="line">        cur_substr = None</span><br><span class="line">        while start &lt; end:</span><br><span class="line">          substr = &quot;&quot;.join(chars[start:end])</span><br><span class="line">          if start &gt; 0:</span><br><span class="line">            substr = &quot;##&quot; + substr</span><br><span class="line">          if substr in self.vocab:</span><br><span class="line">            cur_substr = substr</span><br><span class="line">            break</span><br><span class="line">          end -= 1</span><br><span class="line">        if cur_substr is None:</span><br><span class="line">          is_bad = True</span><br><span class="line">          break</span><br><span class="line">        sub_tokens.append(cur_substr)</span><br><span class="line">        start = end</span><br><span class="line">      if is_bad:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">      else:</span><br><span class="line">        output_tokens.extend(sub_tokens)</span><br><span class="line">    return output_tokens</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>tokenize说明: 使用贪心的最大正向匹配算法<br>  eg:input = “unaffable” output = [“un”, “##aff”, “##able”],首先看”unaffable”在不在词表中，在的话就当做一个词，也就是WordPiece，不在的话在看”unaffabl”在不在，也就是<code>while</code>中的<code>end-=1</code>,最终发现”un”在词表中,算是一个WordPiece,然后start=2,也就是代码中的<code>start=end</code>,看”##affable”在不在词表中,在看”##affabl”(##表示接着前面)，最终返回[“un”, “##aff”, “##able”].注意，这样切分是可逆的，也就是可以根据词表重载”攒回”原词，以此便解决了oov问题.</p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文不讲Bert原理,拟进行Bert源码分析和应用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Bert源码分析&quot;&gt;&lt;a href=&quot;#Bert源码分析&quot; class=&quot;headerlink&quot; title=&quot;Bert源码分析&quot;&gt;&lt;/a&gt;Be
      
    
    </summary>
    
      <category term="NLP" scheme="https://lingyixia.github.io/categories/NLP/"/>
    
    
      <category term="论文" scheme="https://lingyixia.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>dataAugmentation</title>
    <link href="https://lingyixia.github.io/2019/07/16/dataAugmentation/"/>
    <id>https://lingyixia.github.io/2019/07/16/dataAugmentation/</id>
    <published>2019-07-16T12:34:44.000Z</published>
    <updated>2021-09-19T18:01:25.958Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>记录一下常用的NLP数据增加方式,数据增强常用于样本不够或者样本严重不均衡的情况下</p></blockquote></blockquote><h1 id="随机drop和shuffle"><a href="#随机drop和shuffle" class="headerlink" title="随机drop和shuffle"></a>随机drop和shuffle</h1><p>也就是把一个样本随机打乱词语顺序或者扔掉一些词语,当做新的样本,但是不能做过多的drop和shuffle，防止更改了原义</p><h1 id="同义词替换"><a href="#同义词替换" class="headerlink" title="同义词替换"></a>同义词替换</h1><h1 id="回译"><a href="#回译" class="headerlink" title="回译"></a>回译</h1><p>这个很有技巧性，就是吧样本翻译成其他语言，然后在翻译回来，当做新的样本</p><h1 id="生成对抗网络"><a href="#生成对抗网络" class="headerlink" title="生成对抗网络"></a>生成对抗网络</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;记录一下常用的NLP数据增加方式,数据增强常用于样本不够或者样本严重不均衡的情况下&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;随机drop和shuffle&quot;&gt;&lt;a href=&quot;#随机d
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>stackAndqueue</title>
    <link href="https://lingyixia.github.io/2019/07/13/stackAndqueue/"/>
    <id>https://lingyixia.github.io/2019/07/13/stackAndqueue/</id>
    <published>2019-07-13T09:04:58.000Z</published>
    <updated>2021-09-19T18:01:25.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="弹栈压栈"><a href="#弹栈压栈" class="headerlink" title="弹栈压栈"></a>弹栈压栈</h1><blockquote><blockquote><p>输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）<br>算法描述: 只需要按照顺序走一遍即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">bool IsPopOrder(vector&lt;int&gt; pushV,vector&lt;int&gt; popV) &#123;</span><br><span class="line">       stack&lt;int&gt; s;</span><br><span class="line">       int index=0;</span><br><span class="line">       for(int i =0;i&lt;pushV.size();i++)</span><br><span class="line">       &#123;</span><br><span class="line">           s.push(pushV[i]);</span><br><span class="line">           while(!s.empty() &amp;&amp; s.top()==popV[index])</span><br><span class="line">           &#123;</span><br><span class="line">               s.pop();</span><br><span class="line">               index++;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       return s.empty();</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p></blockquote></blockquote><p>Tip: 对于一个入栈顺序的弹栈序列必然有这么一个特征:<br>出栈序列中每个数后面的比它小的数必然按照降序排列<br>比如入栈顺序是:1,2,3,4</p><ol><li>4,1,2,3不可能是出栈顺序,因为4后面比4小的数1,2,3不是降序排列</li><li>3,1,4,2也不合法,3后面比3小的数1,2不是降序排列</li><li>1,2,3,4合法,当前每个数后面没有比它小的</li></ol><h1 id="删除相邻重复字符串"><a href="#删除相邻重复字符串" class="headerlink" title="删除相邻重复字符串"></a><a href="https://leetcode.com/problems/remove-all-adjacent-duplicates-in-string/" target="_blank" rel="noopener">删除相邻重复字符串</a></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">string removeDuplicates(string S)</span><br><span class="line">&#123;</span><br><span class="line">    stack&lt;char&gt; s;</span><br><span class="line">    for (auto ch:S)</span><br><span class="line">    &#123;</span><br><span class="line">        if (s.empty() || ch != s.top())</span><br><span class="line">        &#123;</span><br><span class="line">            s.push(ch);</span><br><span class="line">        &#125; else</span><br><span class="line">        &#123;</span><br><span class="line">            s.pop();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    string result = &quot;&quot;;</span><br><span class="line">    while (!s.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        result = s.top()+result;</span><br><span class="line">        s.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="两个队列实现栈"><a href="#两个队列实现栈" class="headerlink" title="两个队列实现栈"></a>两个队列实现栈</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">class MyStack</span><br><span class="line">&#123;</span><br><span class="line">private:</span><br><span class="line">    queue&lt;int&gt; *q1;</span><br><span class="line">    queue&lt;int&gt; *q2;</span><br><span class="line">public:</span><br><span class="line">    MyStack()</span><br><span class="line">    &#123;</span><br><span class="line">        q1 = new queue&lt;int&gt;();</span><br><span class="line">        q2 = new queue&lt;int&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    void push(int x)</span><br><span class="line">    &#123;</span><br><span class="line">        queue&lt;int&gt; *currentQ = q1-&gt;empty() ? q2 : q1;</span><br><span class="line">        currentQ-&gt;push(x);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int pop()</span><br><span class="line">    &#123;</span><br><span class="line">        queue&lt;int&gt; *currentQ = q1-&gt;empty() ? q2 : q1;</span><br><span class="line">        queue&lt;int&gt; *emptyQ = q1-&gt;empty() ? q1 : q2;</span><br><span class="line">        int current;</span><br><span class="line">        while (!currentQ-&gt;empty())</span><br><span class="line">        &#123;</span><br><span class="line">            current = currentQ-&gt;front();</span><br><span class="line">            currentQ-&gt;pop();</span><br><span class="line">            if (currentQ-&gt;empty()) break;</span><br><span class="line">            emptyQ-&gt;push(current);</span><br><span class="line">        &#125;</span><br><span class="line">        return current;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int top()</span><br><span class="line">    &#123;</span><br><span class="line">        queue&lt;int&gt; *currentQ = q1-&gt;empty() ? q2 : q1;</span><br><span class="line">        queue&lt;int&gt; *emptyQ = q1-&gt;empty() ? q1 : q2;</span><br><span class="line">        int current;</span><br><span class="line">        while (!currentQ-&gt;empty())</span><br><span class="line">        &#123;</span><br><span class="line">            current = currentQ-&gt;front();</span><br><span class="line">            currentQ-&gt;pop();</span><br><span class="line">            emptyQ-&gt;push(current);</span><br><span class="line">        &#125;</span><br><span class="line">        return current;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bool empty()</span><br><span class="line">    &#123;</span><br><span class="line">        return q1-&gt;empty() &amp;&amp; q2-&gt;empty();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="两个栈实现队列"><a href="#两个栈实现队列" class="headerlink" title="两个栈实现队列"></a>两个栈实现队列</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class MyQueue</span><br><span class="line">&#123;</span><br><span class="line">public:</span><br><span class="line">    void push(int node) &#123;</span><br><span class="line">        stack1.push(node);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int pop() &#123;</span><br><span class="line">        int result=0;</span><br><span class="line">        int temp=0;</span><br><span class="line">        if(stack2.empty())</span><br><span class="line">        &#123;</span><br><span class="line">            while(!stack1.empty())</span><br><span class="line">            &#123;</span><br><span class="line">                temp = stack1.top();</span><br><span class="line">                stack1.pop();</span><br><span class="line">                stack2.push(temp);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        result = stack2.top();</span><br><span class="line">        stack2.pop();</span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">private:</span><br><span class="line">    stack&lt;int&gt; stack1;</span><br><span class="line">    stack&lt;int&gt; stack2;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;弹栈压栈&quot;&gt;&lt;a href=&quot;#弹栈压栈&quot; class=&quot;headerlink&quot; title=&quot;弹栈压栈&quot;&gt;&lt;/a&gt;弹栈压栈&lt;/h1&gt;&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>请叫我调参工程师</title>
    <link href="https://lingyixia.github.io/2019/07/11/parameters/"/>
    <id>https://lingyixia.github.io/2019/07/11/parameters/</id>
    <published>2019-07-11T11:50:42.000Z</published>
    <updated>2021-09-19T18:01:25.962Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>这篇博客会一直更新.<br>对于初学者而言，兴趣往往都在模型构成上面,但是，只有真正成为调参工程师才能深刻体会到调参对于模型的重要性，虽然我也只能算入门级,但已经被坑了好几次了。。。。。说多了都是泪啊。<br>总的来说,训练神经网络模型的超参数一般可以分为两类:</p><ul><li>训练参数:学习率,正则项系数,epoch数,batchsize</li><li>模型参数:模型层数,隐藏层参数</li></ul></blockquote></blockquote><h1 id="调参技巧"><a href="#调参技巧" class="headerlink" title="调参技巧"></a>调参技巧</h1><ol><li><p>当你想实现一个模型的时候,一定要用最精简的结构去实现它,尽量不要在任何地方做其他tip,任何tip都要尽量在保证模型无误的条件下进行，否则你永远不知道你所谓的一个小tip对你的模型有多大影响(血的教训,当初在做一个文本生成模型的时候所有激活函数都用的relu,这个bug让我一顿好找。。。)</p></li><li><p>保证每次随机种子不变,才能让实验更有效的对比</p></li><li><h1 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h1><h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2>作为一个调参工程师，私以为,学习率是<strong>最重要</strong>的参数,对于初学者来说，往往最容易忽视学习率的作用.举一个我最开始被坑的例子,当初是做了一个NER模型,写完之后各项指标增长速度特别特别慢,慢到令人发指(一小时从1%到2%),但问题是确实是不断增长，由于刚刚接触也不知道增长速度应该是怎样就一直等着，凉了一天，发现还是那个速度,等不及了开始检查模型问题,由于刚刚接触TF，对自己没把握就一直找一直找，几乎找了一个星期愣是没改出来.不得已，要不调调参数把,还好第一个改的就是学习率,其实原来是0.01我以为已经够小了,改成了0.001,刚一运行，200个step后precision直接到了20%,我他娘的就。。。。原来问题在这。所以以后我在设置学习率的时候都会从一个特别小的数开始，比如0.0001，看看指标的变化，在增大一点学习率比如0.001，再看看变化，确定模型没问题，然后在开始。当然，学习率的设置还有很多方式,比如模拟退火方式,Transform中的学习率代码就使用了这种:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):</span><br><span class="line">    def __init__(self, d_model, warmup_steps=4000):</span><br><span class="line">        super(CustomSchedule, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_model = tf.cast(self.d_model, tf.float32)</span><br><span class="line">        self.warmup_steps = warmup_steps</span><br><span class="line"></span><br><span class="line">    def __call__(self, step):</span><br><span class="line">        arg1 = tf.math.rsqrt(step)</span><br><span class="line">        arg2 = step * (self.warmup_steps ** -1.5)</span><br><span class="line">        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)</span><br></pre></td></tr></table></figure></li></ol><p>也就是让学习率先快诉增大，当step达到warmup_steps后后在慢慢减小<br>bert中的学习率是这样设置的:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = tf.train.polynomial_decay(</span><br><span class="line">      learning_rate,</span><br><span class="line">      global_step,</span><br><span class="line">      num_train_steps,</span><br><span class="line">      end_learning_rate=0.0,</span><br><span class="line">      power=1.0,</span><br><span class="line">      cycle=False)</span><br><span class="line">  if num_warmup_steps:</span><br><span class="line">    global_steps_int = tf.cast(global_step, tf.int32)</span><br><span class="line">    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)</span><br><span class="line">    global_steps_float = tf.cast(global_steps_int, tf.float32)</span><br><span class="line">    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)</span><br><span class="line">    warmup_percent_done = global_steps_float / warmup_steps_float</span><br><span class="line">    warmup_learning_rate = init_lr * warmup_percent_done</span><br><span class="line">    is_warmup = tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32)</span><br><span class="line">    learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)</span><br></pre></td></tr></table></figure></p><p>它也是先让学习率快速增大，当step达到warmup_steps时，在安装多项式方式衰减 顺便提一下，<code>learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)</code>这行代码值得学习</p><h2 id="batchsize"><a href="#batchsize" class="headerlink" title="batchsize"></a>batchsize</h2><ol><li>batch 和 batch 之间差别太大,训练难以收敛,形成震荡</li><li>batchsize 增大会使梯度优化方向更准</li><li>随着 batch_size 增大，处理相同数据量的速度越快。</li><li>随着 batch_size 增大，达到相同精度所需要的 epoch 数量越来越多。</li><li>由于上述两种因素的矛盾，batch_size 增大到某个时候，达到时间上的最优。</li><li>增大 batchsize 能够有效的利用GPU并行能力</li><li>GPU对batchsize为2的整数次幂效果更好</li></ol><h1 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h1><p>首先需要知道的是，对于隐藏层和层数刚开始的设置要紧盯训练样本的量，要保证模型的参数量不高于样本量的一半，有权威称$\frac{1}{10}$最好.反正你不要写完模型后参数太大,你想想用一千条数据去训练好几百万的参数能学到点啥？下面给出个统计模型参数的tf代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def __get_parametres(self):</span><br><span class="line">       total_parameters = 0</span><br><span class="line">       for variable in tf.trainable_variables():</span><br><span class="line">           shape = variable.get_shape()</span><br><span class="line">           variable_parameters = 1</span><br><span class="line">           for dim in shape:</span><br><span class="line">               variable_parameters *= dim.value</span><br><span class="line">           total_parameters += variable_parameters</span><br><span class="line">       tf.logging.info(&quot;总参数量为:&#123;total_parameters&#125;&quot;.format(total_parameters=total_parameters))</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;这篇博客会一直更新.&lt;br&gt;对于初学者而言，兴趣往往都在模型构成上面,但是，只有真正成为调参工程师才能深刻体会到调参对于模型的重要性，虽然我也只能算入门级,但已经被坑了好几次了。。。。。说多了都是泪啊。&lt;br&gt;总的来说,训
      
    
    </summary>
    
      <category term="深度学习" scheme="https://lingyixia.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>总有一个算法让你惊艳</title>
    <link href="https://lingyixia.github.io/2019/07/06/interestAlgorighm/"/>
    <id>https://lingyixia.github.io/2019/07/06/interestAlgorighm/</id>
    <published>2019-07-06T05:34:05.000Z</published>
    <updated>2021-09-20T03:42:25.852Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>总有一个算法让你惊艳</p></blockquote></blockquote><h1 id="随机洗牌"><a href="#随机洗牌" class="headerlink" title="随机洗牌"></a>随机洗牌</h1><p>问:一副牌54张，如何洗牌才能让最公平。<br>什么叫公平?就是每张牌在每个位置的概率都一样就是公平, Knuth 老爷子给出来这样的算法:</p><pre><code>1. 初始化任意顺序2. 从最后一张牌开始,设为第K张,然后从[1,K]张中任选一张与其交换3. 从[1,K-1]张牌中任选一张和第K-1张交换... ...</code></pre><p>伪代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for(int i =n;i&gt;=1;i--)</span><br><span class="line">&#123;</span><br><span class="line">    swap(array[i],random(1,i))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>是不是贼简单？那为啥这个算法能做到呢？下面证明一下:<br>假设现在有5张牌，初始顺序为$1,2,3,4,5$<br>首先，从1~5(这是下标)中任选一张比如选到了2,那么用2和5交换得到$1,5,3,4,2$,也就是说，第一次交换2在最后一个位置的概率是1/5(也可以说任意一个数字在最后一个位置的概率是1/5),那么我们进行第二次交换,从1~4(这是下标)中任选一个和第4个交换,比如我们选到了1,在此之前要保证1没有在之前的步骤被选中也就是4/5,现在选中1的概率是1/4,那么两者相乘得到1/5,也就是1在第4个位置的概率是1/5也可以说任意一个数字在最后一个位置的概率是1/5),后面的不用多说了吧。<br>这题和<a href="https://lingyixia.github.io/2019/04/14/PoolSampling/">蓄水池采样算法</a>由异曲同工之妙。<br>用这个算法还可以在中途随意停止,比如有54张牌,我们要找任意10张牌进行公平洗牌,那么只需要上诉步骤执行10次就可以了.</p><h1 id="轮盘赌随机算法"><a href="#轮盘赌随机算法" class="headerlink" title="轮盘赌随机算法"></a>轮盘赌随机算法</h1><p>$\qquad$俄罗斯轮盘赌（Russian roulette）是一种残忍的赌博游戏。与其他使用扑克、色子等赌具的赌博不同的是，俄罗斯轮盘赌的赌具是左轮手枪和人的性命。俄罗斯轮盘赌的规则很简单：在左轮手枪的六个弹槽中放入一颗或多颗子弹，任意旋转转轮之后，关上转轮。游戏的参加者轮流把手枪对着自己的头，扣动板机；中枪的当然是自动退出，怯场的也为输，坚持到最后的就是胜者。旁观的赌博者，则对参加者的性命压赌注。<br>$\qquad$现在我们把问题抽象化，一个饼图，上面有一个指针，我们我们如何通过一个算法来确定到底每次拨动指针后会指向哪一个？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;ctime&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line">#include &lt;cstdlib&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">#define N  999 //精度为小数点后面3位</span><br><span class="line"></span><br><span class="line">int RouletteWheelSelection()</span><br><span class="line">&#123;</span><br><span class="line">    srand((unsigned) time(NULL));</span><br><span class="line">    float m = rand() % (N + 1) / (float)(N + 1);</span><br><span class="line">    cout &lt;&lt; m &lt;&lt; endl;</span><br><span class="line">    float Probability_Total = 0;</span><br><span class="line">    int Selection = 0;</span><br><span class="line">    vector&lt;float&gt; Probabilities = &#123;0.14, 0.35, 0.2, 0.07, 0.23, 0.01&#125;;</span><br><span class="line">    for(int i = 0; i &lt; Probabilities.size(); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        Probability_Total += Probabilities[i];</span><br><span class="line">        if(Probability_Total &gt;= m)</span><br><span class="line">        &#123;</span><br><span class="line">            Selection = i;</span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return Selection;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    cout &lt;&lt; RouletteWheelSelection();</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>其实只要意识到需要最终各部分的概率与占比相同即可。</p></blockquote></blockquote><h1 id="切分句子"><a href="#切分句子" class="headerlink" title="切分句子"></a>切分句子</h1><p>在NLP中经常会有这样的需求,对于训练数据有少部分会特别长,远远超出平均长度,那么我们就需要对句子进行拆分,但是不能直接安长度切，这样很可能会切断关键词,切分方法一般是在无用的地方切分,比如标点符号,现在给出一个算法实现这个功能:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def data_cut(sentence, cut_chars, cut_length, min_length):</span><br><span class="line">    if len(sentence) &lt;= cut_length:</span><br><span class="line">        return [sentence]</span><br><span class="line">    else:</span><br><span class="line">        for char in cut_chars:</span><br><span class="line">            start = min_length  # 防止直接从头几个就找到了，这样切的太短</span><br><span class="line">            end = len(sentence) - (min_length - 1)  # 防止从最后几个找到了,这样切的也太短</span><br><span class="line">            if char in sentence[start:end]:</span><br><span class="line">                index = sentence[start:end].index(char)</span><br><span class="line">                return data_cut(sentence[:start + index], cut_chars, cut_length, min_length) + \</span><br><span class="line">                       data_cut(sentence[start + index:], cut_chars, cut_length, min_length)</span><br><span class="line">    return [sentence]  # 如果没有找到切分点那就不管句子长度直接返回</span><br></pre></td></tr></table></figure></p><blockquote><p>&gt;<br>参数说明:sentence是一个list,内容是sentence每个字符<br>       cut_chars是一个list,内容是切分字符，按优先级排序<br>       cut_length是int类型，表示切分多长的句子,这个长度以及以下的直接返回<br>       min_length:切分后子句子的最短长度<br>return:二维list,注意，每个切分后的句子不一定全小于cut_length,有些另类的句子可能没有切分点,这样的需要手动处理<br>eg:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def data_cut(sentence, cut_chars, cut_length, min_length):</span><br><span class="line">    if len(sentence) &lt;= cut_length:</span><br><span class="line">        return [sentence]</span><br><span class="line">    else:</span><br><span class="line">        for char in cut_chars:</span><br><span class="line">            start = min_length</span><br><span class="line">            end = len(sentence) - (min_length - 1)</span><br><span class="line">            if char in sentence[start:end]:</span><br><span class="line">                index = sentence[start:end].index(char)</span><br><span class="line">                return data_cut(sentence[:start + index], cut_chars, cut_length, min_length) + data_cut(</span><br><span class="line">                    sentence[start + index:], cut_chars, cut_length, min_length)</span><br><span class="line">    return [sentence]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    sentence = &quot;三种上下文特征：单词、n-gram 和字符在词嵌入文献中很常用。大多数词表征方法本质上利用了词-词的共现统计，即使用词作为上下文特征（词特征）。受语言建模问题的启发，开发者将 n-gram 特征引入了上下文中。词到词和词到 n-gram 的共现统计都被用于训练 n-gram 特征。对于中文而言，字符（即汉字）通常表达了很强的语义。为此，开发者考虑使用词-词和词-字符的共现统计来学习词向量。字符级的 n-gram 的长度范围是从 1 到 4（个字符特征）。&quot;</span><br><span class="line">    l = data_cut(list(sentence), [&apos;，&apos;, &apos;。&apos;], 150, 5)</span><br><span class="line">    print(l)</span><br><span class="line">#自己运行看看吧，不好写</span><br></pre></td></tr></table></figure></p></blockquote><h1 id="约瑟夫环"><a href="#约瑟夫环" class="headerlink" title="约瑟夫环"></a>约瑟夫环</h1><blockquote><blockquote><p>n个人拉成圈，按照1~n标号,从1开始报数,报到m的人出局，每一次有人出局后重新排号，1号是重新排号前m的下一个人.</p></blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int JosephRing(int n, int m)</span><br><span class="line">&#123;</span><br><span class="line">    if (n == 1) return n;</span><br><span class="line">    return (JosephRing(n - 1, m) + m - 1) % n + 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>解释:对于任何一个状态而言，每个位置设为$old_i$,如果现在把该状态的第$m$个剔除掉，则重新排号后每个位置设为$new_i$,两者的关系</p><script type="math/tex; mode=display">old_i = (new +m-1)%n+1</script><p>$JosephRing(n,m)$表示有$n$个人,每次删除$m$号人最终剩下的号码，每次递归回来其实都是再算</p></blockquote></blockquote><h1 id="点和线"><a href="#点和线" class="headerlink" title="点和线"></a>点和线</h1><h2 id="一个知识点"><a href="#一个知识点" class="headerlink" title="一个知识点"></a>一个知识点</h2><p>定义: 平面上的三点$A(x1,y1),B(x2,y2),C(x3,y3)$的面积量:</p><script type="math/tex; mode=display">S(A,B,C)=\frac{1}{2}  \left|\begin{array}{}    x_1 &    y_1    & 1 \\     x_2 &    y_2   & 1\\     x_3 & y_3 & 1 \end{array}\right|</script><p>其中: 当A、B、C逆时针时S为正的,反之S为负的。<br>证明如图:<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/dotandline.png" alt title>                </div>                <div class="image-caption"></div>            </figure><br>也就是说，平面三点一定能写成一个直角梯形减两个直角三角形的形式.<br>即:</p><script type="math/tex; mode=display">S(A,B,C)=\frac{(x_1y_2+x_3y_1+x_2y_3-x_1y_3-x_2y_1-x_3y_2)}{2}</script><p>正好是上诉行列式.</p><h2 id="一个应用"><a href="#一个应用" class="headerlink" title="一个应用"></a>一个应用</h2><p>令矢量的起点为A，终点为B，判断的点为C，<br>如果S(A，B，C)为正数，则C在矢量AB的左侧；<br>如果S(A，B，C)为负数，则C在矢量AB的右侧；<br>如果S(A，B，C)为0，则C在直线AB上</p><h1 id="点和矩形"><a href="#点和矩形" class="headerlink" title="点和矩形"></a>点和矩形</h1><blockquote><blockquote><p>判断坐标系内某电点是否在某个矩形内部</p></blockquote></blockquote><p>只需要将点与四个角连接，计算形成的四个三角形面积(海伦公式)和是否等于矩形面积,等于则在内部，否则在外部。</p><h1 id="判断平面内两线段相交"><a href="#判断平面内两线段相交" class="headerlink" title="判断平面内两线段相交"></a>判断平面内两线段相交</h1><ol><li>计算两线段所在的直线的的交点(如果有),然后看该交点是否在两条线段上即可</li><li>若两直线相交,则如图:,,,算了不弄图了，也简单，假设有线段AB 和 CD 若相交则必定C 和 D 在 AB 的两侧，则有$\vec{AB} \times \vec{AC} $和$\vec{AB} \times \vec{AD} $必定异号 $\vec{AB} \times \vec{AC} $和$\vec{CA} \times \vec{CD} $必定异号，判断这个即可</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;总有一个算法让你惊艳&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;随机洗牌&quot;&gt;&lt;a href=&quot;#随机洗牌&quot; class=&quot;headerlink&quot; title=&quot;随机洗牌&quot;&gt;&lt;/a&gt;随机
      
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>二进制相关</title>
    <link href="https://lingyixia.github.io/2019/06/23/Binary/"/>
    <id>https://lingyixia.github.io/2019/06/23/Binary/</id>
    <published>2019-06-23T14:49:15.000Z</published>
    <updated>2021-09-19T18:01:25.944Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数组与-2-k"><a href="#数组与-2-k" class="headerlink" title="数组与$2^k$"></a>数组与$2^k$</h1><blockquote><blockquote><p>一个整形数组，每次取的元素和必须是$2^k$,问至少多少次能取完?假设一定能完成这个任务。</p></blockquote></blockquote><p>答:只需要把所有的数字加起来，然后换为二进制，里面有几个1就有几次.(找不到标准答案，感觉应该对)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数组与-2-k&quot;&gt;&lt;a href=&quot;#数组与-2-k&quot; class=&quot;headerlink&quot; title=&quot;数组与$2^k$&quot;&gt;&lt;/a&gt;数组与$2^k$&lt;/h1&gt;&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;一个整形数组，每次取的元素和必须是$2^k$
      
    
    </summary>
    
      <category term="算法" scheme="https://lingyixia.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
</feed>
