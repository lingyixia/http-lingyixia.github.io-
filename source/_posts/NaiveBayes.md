---
layout: post
title: 朴素贝叶斯
category: 机器学习
date: 2018-12-20 15:36:51
tags: [分类,无监督]
description: 朴素贝叶斯简介
---

>朴素贝叶斯简单推导

### 问题定义
设输入空间$\chi\subseteq R^n$，为n维空间的集合，输出空间为类标记集合$\gamma={c_1,c_2,...,c_K}$，输入为特征向量$x\subset\chi$，输出为类标记$y\subset\gamma$，$X$是输入控件$\chi$上的随机向量,$Y$是定义在输出空间$\gamma$,$P(X,Y)$是$X$和$Y$的联合分布，训练数据集:

$$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$$

由$P(X,Y)$独立同分布产生(谨记：$(x_1,y_1)$中的$x_1\subset\chi$，即$x_1$是n维)。
现在需要用T训练贝叶斯模型，并判断$x'$的标签。
### 问题探究
显然，$x'$的标签可以$y_1$~$y_N$中任何一个，若一定要判断标签也等同于哪个标签的概率最高，即计算:

$$y=arg\max_{c_k}P(Y=c_k|X=x) \qquad k=1,2,3,...,K \tag{1}$$

已知:

$$P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\Sigma_kP(X=x|Y=c_k)P(Y=c_k)} \quad k=1,2,3,...K \tag{2}$$

其中，分母对于每个$c_k$都是相同的，略去不计算，只需计算:
$P(X=x|Y=c_k)P(Y=c_k)$
，即:

$$y=arg\max_{c_k}P(X=x|Y=c_k)P(Y=c_k) \qquad  k=1,2,3,...K \tag{3}$$

对于$P(Y=c_k)$只需要在给出的$T$中数出来即可，因此现在只需要计算$P(X=x|Y=c_k)$，这也是整个朴素贝叶斯过程最困难的一步，为什么说困难呢？
$P(X=x|y=c_k)$完整表示为$P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},X^{(3)}=x^{(3)},....,X^{(N)}=x^{(N)}|Y=c_k)$
若和$P(Y=c_k)$一样只有一个维度，很容易数出来，但这里需要同时考虑N个维度，数的过程是相当复杂的，因此，朴素贝叶斯在此处做了条件**独立性假设**：

$$P(X=x|y=c_k)=\prod_{i=1}^n(X^{(i)}=x^{(i)}|Y=c_k) \qquad k=1,2,3,...K \tag{4}$$

这样就只需要同时考虑一个维度，简单了许多，最后将$(4)$带入$(3)$中得:

$$y=arg\max_{c_k}\prod_{i=1}^n(X^{(i)}=x^{(i)}|Y=c_k)P(Y=c_k) \qquad k=1,2,3,...K \tag{5}$$

至此朴素贝叶斯过程结束。
### 名词谨记

1. 先验概率:
$P(Y=c_k)$，即不管$X$，直接在$T$中数出来的概率

2. 后验概率:
$arg\max_{c_k}P(Y=c_k|X=x)$，即考虑$X$的约束下，数出来的概率（注意：两个概率都是针对$Y$的概率）

### 原理
其实前面已经可以说是整个朴素贝叶斯的过程，但是还要继续说明一个问题：
$(1)$式是整个朴素贝叶斯要计算的基本公式，我们可以看出，其实该公式就是后验概率，也就是说，朴素贝叶斯是一个**后验概率最大化**的过程。
####后验概率最大化的含义
先说结论：**后验概率最大化**的含义就是选择$0-1$损失函数时的**期望风险最小化**，下面证明这个结论：
首先$0-1$损失函数：

$$
L(Y,f(X))= \begin{cases}
1 & Y ≠ f(X)\\
0 & Y=f(X) \\
\end{cases}
$$

其中$Y$和$f(X)$分别是$X$的实际标签和计算所的标签，则期望风险函数为：

$$
\begin{align}
R_{exp}(f(X)) &= E[L(Y,f(X))] \\
              &=\sum_{x=1}^N\sum_{k=1}^K[L(Y=c_k,f(X)]P(Y=c_k,X=x_i)\\
              &=E_X(\sum_{k=1}^K[L(Y=c_k,f(X)]P(Y=c_k|X))
\end{align}
$$

意思就是先对确定$X=x$求**条件期望**,其实就是两层而来来算。
这样，针对确定的$X=x$而言：

$$
\begin{align}
f(x) &= arg\min_{y\subset\chi}\sum_{k=1}^KL(Y=c_k,f(X)=y)P(Y=c_k,|X=x) \\
     &= arg\min_{y\subset\chi}\sum_{k=1}^KP(Y≠c_k,|X=x) \\\\
     &= arg\min_{y\subset\chi}\sum_{k=1}^K(1-P(Y=c_k,|X=x)) \\
     &= arg\max_{y\subset\chi}\sum_{k=1}^KP(Y=c_k,|X=x)
\end{align}
$$

由此，我们从**期望风险最小化**入手，最终得到了**后验概率最小化**，即朴素贝叶斯的原理。