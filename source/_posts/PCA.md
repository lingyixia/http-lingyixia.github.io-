---
title: PCA
date: 2019-04-09 11:27:13
category: 机器学习
tags: [线性代数]
---
[参考1](http://blog.codinglabs.org/articles/pca-tutorial.html)
[参考2](https://applenob.github.io/pca.html)
[参考3](https://blog.csdn.net/hjq376247328/article/details/80640544)
原理就是一句话,把原来的A个维度映射到(B<A)个维度。
<!--more-->

现在我尝试用最简答的方式回答PCA的原理,假设原始数据为$X$,$shape$=$n \times m$,它代表了$n$维,$m$个数据。
假设有一个由正交基安行组成的矩阵$P$,则令$Y=PX$,意义也就是将$X$的每一列映射到以$P$的每一行代表的基(坐标轴)上.如果现在$YY^T$是对角矩阵,那么也就是说$Y$数据的协方差为0,则正好是我们所需要的矩阵(不同维度不相关),现在我们就要找到$P$使得$YY^T$是对角矩阵:
$$
\land = YY^T=PX(PX)^T=PXX^TP^T=P(XX^T)P^T
$$
也就是说我们要找到$XX^T$的特征向量！！！
还要注意的是特征向量安行组成$P$,平常计算都是安列,这里是安行。
对于$\land$而言,对角线一定是$XX^T$的特征值,而此时的特征值一定是$Y$的方差,现在捋一捋。
$PX$表示对$X$更换坐标轴,坐标轴是$P$的每一行,而$PX$每一行的方差正好又是特征值,那么有结论:
**特征值对应的特征向量就是理想中想取得正确的坐标轴，而特征值就等于数据在旋转之后的坐标上对应维度上的方差。**
所以我们按照从大到小排序特征值.取的时候取方差大的特征值对应的特征向量。这样比如要求维度为1,那么我们取$P$第一行,这是方差最大的坐标,用它乘以$X$得到的就是使方差最大的数据变换。

本来还想几句话了事，结果还是白话了半天。。。
