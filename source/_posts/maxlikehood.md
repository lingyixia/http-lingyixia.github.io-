---
title: 顿悟最大熵
date: 2019-07-28 15:27:59
category: 数学
tags: [机器学习]
---
>>虽然以前也了解最大熵模型，甚至以为自己完全理解了最大熵,知道今天才发现我以前都回了点啥。。。。今天发现自己顿悟了一下，特此记录，希望能给看到的人一点帮助，再次膜拜孔圣人的远见"温故而知新，可以为师矣."(未完待续)

#从最大熵思想开始
其实在我看来，所谓的最大熵思想就是对已知的条件**充分考虑**,对未知的条件**不做任何假设**,这就是最大熵的真谛.
#这就是最大熵模型
5个海贼团$A、B、C、D、E$去争夺罗杰的宝藏,宝藏只有一份,问谁能得到? 
我们给他建立的模型是**均匀模型**$P(X=A)=\frac{1}{6},P(X=B)=\frac{1}{6},P(X=C)=\frac{1}{6},P(X=D)=\frac{1}{6},P(X=E)=\frac{1}{6},P(X=6)=\frac{1}{6}$
我们为毛会建立这样的模型呢?因为均匀模型的**熵最大**,我们对这个5个海贼团一无所知,因此只能创建**均匀模型**来保证**熵最大**，这就是**最大熵**的一个最简单应用

5个海贼团$A、B、C、D、E$去争夺罗杰的宝藏,我们已知:$A$得到宝藏的概率和$B$一样,都是$\frac{1}{10}$,$C$和$D$一样,都是$\frac{3}{10}$,而$E$就比较牛逼了,他的概率是$\frac{5}{10},因为他叫路飞$,宝藏只有一份,问谁能得到? 
现在我们有了条件，就不能简单的创建**均匀模型**了,因为此时我们要找的模型不是$P(X,\theta)$,而是$P(Y|X,\theta)$,我们需要的是$P(Y|X,\theta)$的熵最大，这便是**最大熵模型**

#看看公式
已知:$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}$
目标:运用最大熵原理找最大熵模型
由上诉**最大熵模型**的由来我们很容易得出公式,不是要找一个模型,在$X$的条件下这个模型的$P(Y|X,\theta)$的熵最大么,很明显这是个条件熵呀,那么这个模型在这份数据上的条件熵的公式:
$$
H(Y|X) = \sum_{x \in X} \widetilde{P}(x)H(Y|X=x)) \tag{1}
$$
很容易就得到了(其实想想交叉熵的公式很容易明白)
现在我们的目标确定了,我们要从$$P(Y|X,\theta_1)，P(Y|X,\theta_2)，P(Y|X,\theta_3)，P(Y|X,\theta_4),,,$$很多的模型中找到一个模型,这个模型**充分考虑**了已知数据$T$,而且对未知**不做任何假设**，**不做任何假设**解决了,我们让公式$(1)$要尽量大即可,那么**充分考虑**已知数据要怎样表示呢?说到这有人可能会有些思绪了,其实就是从$T$中找一个用来表示**充分考虑**已知条件的约束,让公式$(1)$在这个约数下尽量大.找到这个约束,一个最优化问题就出现了,现在我们的目的就是找这个约束。
#充分考虑
现在我们想想，什么是**充分考虑**了已知数据呢?比如我们找到了一个模型$P(X|Y，\theta)$,要考量它是对已知数据的考虑程度,把这句话数学化就是考量这个模型对原始数据**特征**的考虑程度,怎样表示原始数据的特征呢?**特征函数**出现了！！！
我们一般用这样一个**特征函数**来描述**特征**:
$$
f(x,y)=\begin{cases}
1 这个x和y满足某一事实\\
0 这个x和y不满足某一事实\\
\end{cases}
$$
那么这分数据特征的总值是多少呢?找期望呗
$$
E_{\widetilde p}(f)=\sum_{x,y}\widetilde P(x,y)f(x,y) \tag{2}
$$
谨记:$E_{\widetilde p}(f)$是原始数据的特征总值.
对于我们找的的模型$P(X|Y，\theta)$在这份数据的特征总值是多少呢?
$$
E_{p}(f)=\sum_{x,y}\widetilde P(x)P(y|x,\theta)f(x,y) \tag{3}
$$
要想让$P(Y|X,\theta)$**充分考虑**原始数据咋做呢?两个特征总值相等呗
$$
E_{\widetilde p}(f)=E_{p}(f) \tag{4}
$$
这就是约束条件了.
#最大熵模型
现在我们的目标确定了,我们要让公式$(1)$在约束条件为公式$(4)$的条件下越大越好,现在我们把公式$(1)$展开:
$$
\begin{align}
H(Y|X) &= \sum_{x \in X} \widetilde{P}(x)H(Y|X=x)) \\
&=-\sum_{x \in X}\widetilde P(x)\sum_{y \in Y}P(y|X=x) \log P(Y|X=x)\\
&= -\sum_{x \in X}\sum_{y \in Y}\widetilde P(x)P(y|X=x) \log P(Y|X=x)\\
&=-\sum_{x \in X y\in Y}\widetilde{P}(x,y)\log(y|x)
\end{align}
$$
所以最终公式为:
$$
\max \quad \quad \quad -\sum_{x \in X y\in Y}\widetilde{P}(x,y)\log(y|x) \\
s.t. \quad \quad \quad \quad \quad \quad \quad E_{\widetilde p}(f)=E_{p}(f)  \\
\quad \quad \quad \quad \sum_{y}P(y|x)=1(隐含条件)
$$
完毕散花!!!!卧槽写完后发现写的好清楚...不信你看了还不明白！
至于后面的求解，，,歇歇在写吧.
#从最大熵模型到逻辑回归
二元逻辑回归的似然函数为:
$$
L(\theta)=\prod_{i}^n[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}
$$
在这里，其实$\pi(x_i)$就是$P(Y=1|X=x_i)$而且$y_i$完全可以写成数据中$Y=1$的数量你说对也不对。。其实在细想还可以写成数据中X=x_i,Y=1$的数量.所以可以写成:

$$
L(\theta)=P(Y=1|X=x_i)^{N_{1x_i}}P(Y=0|X=x_i)^{N_{0x_i}}
$$

在开个N(N是数据总量)次方得:
$$
L(\theta)=P(Y=1|X=x_i)^\frac{N_{1x_i}}{N}P(Y=0|X=x_i)^\frac{N_{0x_i}}{N}
$$
想想$\frac{N_{0x_i}}{N}$是啥,,,这不就是$\widetilde{P}(x,y)$么,,,而且开个N次方对优化也没有影响,拿这个公式和$公式(3)$去掉log,即$P(y|x)^{\widetilde{p}(x,y)}$对比一下，这不一样么.哈哈哈，有没有恍然大明白的感觉。

#再到最大似然估计

>>未完待续，先乱写一通最后整理
最大熵思想的核心思想其实是均匀分布,而均匀分布的熵最大,所以是最大熵思想

>>今天重新梳理一下最大自然估计，本来以为很简单的东西发现内容好多都忘了，还明白了不少以前很模糊的知识点,特此记录.

#从定义看开始
>>设$X_1,X_2,...X_n$是来自总体的样本，$x_i,x_2,...x_n$是相对的观察值,则**似然函数**为:
$$
L(\theta)= L(x_1,x_2,...x_n)=\begin{cases}
\prod_{i=1}^np(x_i,\theta) 离散型(p是概率值)\\
\prod_{i=1}^nf(x_i,\theta) 连续型(f是密度函数) \\
\end{cases}
$$
也就是把所有的概率连乘,而**最大似然估计**就是想求一个参数$\theta$,使得$L(\theta)$最大,这种情况下我们认为求得的模型是最好的.
已知:模型+样本+样本观察值(带入模型得概率)
求:模型参数